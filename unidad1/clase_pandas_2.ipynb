{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulación de datos con [*pandas*](https://pandas.pydata.org/)\n",
    "\n",
    "En este clase veremos \n",
    "- como crear dataframes a partir de datos de distintas fuentes \n",
    "- funciones avanzadas de manipulación de dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(\"Versión de pandas \"+pd.__version__)\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de archivos CSV (Comma-Separated Values)\n",
    "\n",
    "Un archivo CSV es una tabla en formato texto plano cuyas columnas están separadas por comas\n",
    "\n",
    "Descarguemos la base de datos \"Dow Jones Index\" del repositorio UCI\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Dow+Jones+Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -c https://archive.ics.uci.edu/ml/machine-learning-databases/00312/dow_jones_index.zip\n",
    "unzip -o dow_jones_index.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizemos el archivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head dow_jones_index.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provee la función [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) para importar tablas en formato texto plano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dow_jones_index.data\", sep=',', header=0, index_col='stock')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas columnas se han guardado como strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[\"date\"][0],\n",
    "        df[\"open\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing: Selección manual de formato de dato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos dejado que pandas asigne los tipos de dato de manera automática\n",
    "\n",
    "La función `read_csv` tiene un argumento `converters` que recibe un diccionar de funciones\n",
    "\n",
    "Esto puede usarse para *parsear* manualmente las columnas que no se importaron automaticamente como deseabamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = dict.fromkeys(['open', 'close', 'high', 'low', \n",
    "                      'next_weeks_open', 'next_weeks_close'], lambda x: float(x.strip(\"$\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas fue diseñado para analizar series de tiempo e incorpora la función [`to_datetime()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) que toma un string y retorna un `Timestamp`\n",
    "\n",
    "Por defecto el formato se infiere, pero puede forzarce usando el argumento `format`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.to_datetime(\"1/5/2018\"),\n",
    "        pd.to_datetime(\"1st of May of 2018\"),\n",
    "        pd.to_datetime(\"May/1/2018\"),\n",
    "        pd.to_datetime(\"2018\"),\n",
    "        pd.to_datetime(\"14:45\"),\n",
    "        pd.to_datetime(\"May/1/2018 14:45\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitarnos la vida `read_csv` tiene un argumento `parse_dates` que recibe una lista de enteros especificando las columnas que queremos convertir a fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dow_jones_index.data\", sep=',', header=0, index_col='stock', \n",
    "                 converters=conv, parse_dates=[2])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tipos de la nueva tabla son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que los tiempos tiene formato *timestamp* podemos usarlos como índice\n",
    "\n",
    "Esto nos permite recuperar rapidamente todos los eventos dentro de un intervalo de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dow_jones_index.data\", sep=',', header=0, index_col='date', \n",
    "                 converters=conv, parse_dates=[2])\n",
    "\n",
    "df[df[\"stock\"] == \"AA\"].loc[\"2011-02-01\":\"2011-03-12\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib tiene funciones para parsear datos temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as md\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "sub_df = df[df[\"stock\"] == \"AA\"]\n",
    "for x, o, c in zip(sub_df.index.values, sub_df['open'].values, sub_df['close'].values):\n",
    "    ax.arrow(x=md.date2num(x), y=o, dx=0, dy=c-o, head_width=3, head_length=0.1, fc='k', ec='k')\n",
    "ax.fill_between(sub_df.index.values, sub_df['low'].values, sub_df['high'].values, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de archivos excel\n",
    "\n",
    "- Pandas provee la función [`read_excel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html)\n",
    "\n",
    "- Requisito adicional: [python-xlrd](https://github.com/python-excel/xlrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget -c http://www.censo2017.cl/wp-content/uploads/2017/12/Cantidad-de-Viviendas-por-Tipo.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Cantidad-de-Viviendas-por-Tipo.xlsx\", sheet_name=1, \n",
    "                   usecols=list(range(1, 20)), header=1, index_col='ORDEN')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.drop(0, inplace=True)\n",
    "display(df.head())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podriamos querer obtener los valores totales de la Provincia de Valdivia: **reducción suma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mask = df.columns[4:-1]\n",
    "display(col_mask)\n",
    "row_mask = df[\"NOMBRE PROVINCIA\"] == \"VALDIVIA\"\n",
    "display(df.loc[row_mask].head())\n",
    "df.loc[row_mask, col_mask].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambio de índice\n",
    "\n",
    "Podemos usar las funciones [reset_index](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html) y [set_index](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html#pandas.DataFrame.set_index) para modificar el índice del dataframe a nuestra conveniencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df = df.set_index(\"NOMBRE PROVINCIA\")\n",
    "display(df.head())\n",
    "df.loc[\"VALDIVIA\", col_mask].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df = df.set_index(\"ORDEN\")\n",
    "%timeit -n20 df.loc[df[\"NOMBRE PROVINCIA\"] == \"VALDIVIA\", col_mask].sum()\n",
    "df = df.reset_index()\n",
    "df = df.set_index(\"NOMBRE PROVINCIA\")\n",
    "%timeit -n20 df.loc[\"VALDIVIA\", col_mask].sum()\n",
    "df = df.reset_index()\n",
    "df = df.set_index(\"ORDEN\")"
   ]
  },
  {
   "attachments": {
    "groupby.svg": {
     "image/svg+xml": [
      "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"
     ]
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby: Reducciones condicionales\n",
    "\n",
    "En el caso anterior podemos reducir de forma separada para cada región o provincia sin cambiar índices\n",
    "\n",
    "La función [`groupby`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) permite hacer una reducción condicional a una etiqueta\n",
    "\n",
    "Podemos imaginar que la función `groupby` es una [secuencia](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) como la siguiente:\n",
    "![groupby.svg](attachment:groupby.svg)\n",
    "\n",
    "Donde\n",
    "- *Split*: divide los datos según una **llave**\n",
    "- *Apply*: Realiza una función sobre cada grupo: reducción, transformación, filtrado\n",
    "- *Combine*: Mezcla el resultado en un nuevo dataframe donde la **llave** se convierte en el índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redu = df.groupby(\"NOMBRE REGIÓN\").sum()\n",
    "display(df_redu.head())\n",
    "fig, ax = plt.subplots(figsize=(6, 7), tight_layout=True)\n",
    "df_redu.plot(ax=ax, y=0, kind='bar', logy=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`groupby` puede usarse como iterador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (region, sub_df) in df.groupby('NOMBRE REGIÓN'):\n",
    "    display(region, sub_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que no estamos limitados a las reducciones definidas\n",
    "\n",
    "Podemos usar los atributos de `groupby` para obtener más funcionalidad\n",
    "- `aggregate` : Operación de reducción\n",
    "- `filter` : Operación de eliminación de filas (drop)\n",
    "- `transform` : Operación de modificación columna a columna\n",
    "- `apply`: Aplica una función arbitraria "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las funciones de reducción deben recibir un arreglo y retornar un valor\n",
    "def mi_reduccion(x):\n",
    "    return np.sum(x**np.log10(x+1e-10))\n",
    "\n",
    "# Argumento lista de funciones: Cada función se aplica a todas las columnas\n",
    "display(df.groupby(\"NOMBRE REGIÓN\").aggregate([np.median, mi_reduccion]).head())\n",
    "# Argumento diccionario de funciones: una función distinta por columna\n",
    "display(df.groupby(\"NOMBRE REGIÓN\").aggregate({'Viviendas Particulares Ocupadas con Moradores Presentes': np.median, \n",
    "                                               'Viviendas Particulares Ocupadas con Moradores Ausentes': mi_reduccion}).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_filtro(x):\n",
    "    return x[\"Viviendas Particulares Ocupadas con Moradores Presentes\"].sum() > 500000\n",
    "\n",
    "# El filtro debe establecer una condición sobre el grupo completo\n",
    "# En este caso retorna df menos las comunas de las regiones con menos de 500.000 viviendas\n",
    "\n",
    "df.groupby(\"NOMBRE REGIÓN\").filter(mi_filtro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_transformación(x):\n",
    "    if x.dtype == np.float:\n",
    "        return (x - x.mean())/x.std()\n",
    "    else:\n",
    "        return x \n",
    "\n",
    "# La transformación opera columna por columna\n",
    "# La transformación debe retornar un resultado que es del mismo tamaño de la entrada\n",
    "\n",
    "df.groupby(\"NOMBRE REGIÓN\").transform(mi_transformación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_funcion(x):\n",
    "    col = 'Viviendas Particulares Ocupadas con Moradores Presentes'\n",
    "    x[col] -= x[col].mean()\n",
    "    return x\n",
    "\n",
    "\n",
    "df.groupby(\"NOMBRE REGIÓN\").apply(mi_funcion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting\n",
    "\n",
    "Podemos usar más de una etiqueta para hacer el splitting de `groupby`\n",
    "\n",
    "Podemos también usar una función, una lista, diccionario o dataframe\n",
    "\n",
    "Refierase a la [documentación](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) para más detalles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"NOMBRE REGIÓN\", \"NOMBRE PROVINCIA\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi índices\n",
    "\n",
    "Hemos notado que la tabla tiene estructura jerárquica: REGION, PROVINCIA, COMUNA\n",
    "\n",
    "Podemos usar un multi-índice en pandas para manipular mejor esta tabla\n",
    "\n",
    "Podemos crear un multi-índice usando la modulo `MultiIndex` y sus funciones `from_array`, `from_frame` y `from_tuple`\n",
    "\n",
    "Luego podemos aplicarlo con la función `set_index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set_index acepta una lista con nombres de columna:\n",
    "df.reset_index()\n",
    "df = df.set_index([\"NOMBRE REGIÓN\", \"NOMBRE PROVINCIA\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexando y haciendo slicing con dataframes multi-indexados\n",
    "\n",
    "Usamos una tupla para especificar los índices primario y secundario\n",
    "\n",
    "Se usa el objeto [`IndexSlice`](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.IndexSlice.html) para generar slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "\n",
    "display(df.loc[\"LOS RÍOS\"],\n",
    "        df.loc[(\"LOS LAGOS\", \"OSORNO\")],\n",
    "        df.loc[idx[:, \"VALDIVIA\"], :],\n",
    "        df.loc[(\"LOS RÍOS\", \"RANCO\"), \"Viviendas Particulares Ocupadas con Moradores Presentes\":])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La función `eval` y la función `query` \n",
    "\n",
    "[`eval`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.eval.html) y [`query`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html) son atributos de DataFrame que permiten evaluar una expresión arbitraria o hacer consultas (filtro) sobre las columnas del dataframe, respectivamente\n",
    "\n",
    "Están basadas en [`numexpr`](https://github.com/pydata/numexpr) que es un evaluador de expresiones numéricas acelerado para ndarray (rendimiento casi C)\n",
    "\n",
    "`numexpr` acepta un string con una expresión estilo numpy, la evalua y returna el resultado\n",
    "\n",
    "Si los arreglos son grandes ganamos en velocidad y en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numexpr as ne\n",
    "\n",
    "A = np.arange(100000).reshape(1000, 100)\n",
    "\n",
    "# NumPy es más lento ya que evalua y guarda cada paso\n",
    "%timeit -n10 np.tanh(-A**2) > np.exp(np.cos(A)/2)\n",
    "%timeit -n10 ne.evaluate(\"tanh(-A**2) > exp(cos(A)/2)\")\n",
    "\n",
    "b1 = np.tanh(-A**2) > np.exp(np.cos(A)/2)\n",
    "b2 = ne.evaluate(\"tanh(-A**2) > exp(cos(A)/2)\")\n",
    "np.allclose(b1, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OJO:** Nuestras columnas no pueden tener espacios en sus nombres para poder usar `query`/`eval`\n",
    "\n",
    "### Renombrando las columnas\n",
    "\n",
    "Podemos \n",
    "\n",
    "- Usar el atributo `rename` y especificar los nuevos nombres uno a uno \n",
    "- Aplicar operaciones de string al atributo `columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Cantidad-de-Viviendas-por-Tipo.xlsx\", sheet_name=1, \n",
    "                   usecols=list(range(1, 20)), header=1, index_col='ORDEN')\n",
    "df.dropna(inplace=True)\n",
    "df.drop(0, inplace=True)\n",
    "# df.rename(columns={'NOMBRE PROVINCIA': 'NOMBRE_PROVINCIA'}, inplace=True)\n",
    "df.columns = df.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de eval y query\n",
    "\n",
    "Hacemos operaciones o consultas sobre las columnas usando su etiqueta\n",
    "\n",
    "Para `query` podemos juntar varias consultas con `and` y `or`\n",
    "\n",
    "Podemos llamar variables externas anteponiendo un `@`\n",
    "\n",
    "Al igual que con numexpr las operaciones intermedias no se guardan en memoria\n",
    "\n",
    "- Opinión objetiva: Si el dataframe es grande ganamos en velocidad en uso de memoria\n",
    "- Opinion subjetiva: En general ganamos en legibilidad\n",
    "\n",
    "Forma tradicional versus `eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['Viviendas_Particulares_Ocupadas_con_Moradores_Presentes']/df['TOTAL_VIVIENDAS']\n",
    "b = df.eval('Viviendas_Particulares_Ocupadas_con_Moradores_Presentes/TOTAL_VIVIENDAS')\n",
    "np.allclose(a, b)\n",
    "display(b.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos evaluar y guardar el resultado directamente en el dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.eval('Proporcion_encuestas_vs_total = Viviendas_Particulares_Ocupadas_con_Moradores_Presentes/TOTAL_VIVIENDAS', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrando con `query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 0.75\n",
    "df.query('NOMBRE_PROVINCIA == \"VALDIVIA\" \\\n",
    "and Viviendas_Particulares_Ocupadas_con_Moradores_Presentes/TOTAL_VIVIENDAS > @percentage').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de bases de datos SQL\n",
    "\n",
    "Pandas puede usarse para leer y hacer consultas a una base de datos en formato SQL\n",
    "\n",
    "Primero vamos a crear una base de datos e insertar un dataframe como tabla\n",
    "\n",
    "Usaremos [sqlite3](https://docs.python.org/3/library/sqlite3.html) que es parte de la librería estándar de Python\n",
    "\n",
    "- sqlite permite conectar a una base de datos local: RAM, disco, o disco externo montado\n",
    "- sqlite no está diseñado para soportar múltiples usuarios conectados a una misma base de datos\n",
    "- Alternativas: [SQL Alchemy](https://www.sqlalchemy.org/), [PostgreSQL+Python](http://initd.org/psycopg/), [Peewee](http://docs.peewee-orm.com/en/latest/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "# Abrimos una conexión\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "# La palabra clave :memory: corresponde a una base de datos en memoria RAM\n",
    "df.to_sql(\"censo_viviendas\", conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar [`read_sql_query()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql_query.html) para hacer una consulta y retornar un dataframe\n",
    "\n",
    "Por ejemplo si queremos toda la tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_string = \"SELECT * FROM censo_viviendas limit 10\"\n",
    "print(sql_string)\n",
    "pd.read_sql_query(sql_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O si quieremos un subconjunto específico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_string = \"SELECT [{0}], [{1}] FROM censo_viviendas WHERE [{2}] = 'VALDIVIA' limit 5\".format(\"Viviendas_Particulares_Ocupadas_con_Moradores_Presentes\", \n",
    "                                                                                                \"NOMBRE_COMUNA\",\n",
    "                                                                                                \"NOMBRE_PROVINCIA\")\n",
    "print(sql_string)\n",
    "pd.read_sql_query(sql_string, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cerramos la conexión a la base de datos\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar y leer una tabla en formato JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar el atributo [`to_json`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_json.html) para convertir un dataframe a este formato\n",
    "\n",
    "El keyword `orient` nos permite seleccionar como organizar el scheme del json\n",
    "\n",
    "Las opciones son `columns` (por defecto), `table`, `values`, `index`, `split` y `records`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"pandas.json\", orient='table')\n",
    "\n",
    "!head -c 200 pandas.json\n",
    "print(\"\")\n",
    "!ls -lah pandas.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenemos que usar la misma orientación con la que lo guardamos\n",
    "pd.read_json(\"pandas.json\", orient='table').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar y leer una tabla en formato HDF5\n",
    "\n",
    "Podemos usar el atributo `to_hdf` para convertir nuestra tabla a formato HDF5\n",
    "\n",
    "Para acceso de más bajo nivel podemos usar la clase `HDFStore`\n",
    "\n",
    "Para lectura podemos usar la función `read_hdf`\n",
    "\n",
    "NO FUNCIONA CON NUMPY MAS RECIENTE: https://github.com/PyTables/PyTables/issues/719"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf(\"pandas_hdf.h5\", key='asd', mode='w')\n",
    "\n",
    "mi_tabla_recuperada = pd.read_hdf(\"pandas_hdf.h5\", key='/asd', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
