{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje no supervisado\n",
    "\n",
    "En esta lección se estudian dos algoritmos de aprendizaje no supervisado clásicos y ampliamente utilizados: PCA como representante de los algoritmos de reducción de dimensionalidad y KMeans como representando de los algoritmos de agrupamiento/clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "A continuación se presentan los fundamentos teóricos tras el algoritmo PCA. En seguida veremos como utilizar la implementación de PCA de la librería `sklearn` para hacer reducción de dimensionalidad en base de datos con atributos continuos\n",
    "\n",
    "### Formalismo matemático\n",
    "\n",
    "PCA es un procedimiento estadístico que busca una **transformación ortogonal** que **maximice la varianza** de los datos \n",
    "\n",
    "Matemáticamente, podemos escribir un conjunto de datos $\\{x_i\\}$ con $i=1,2,\\ldots, N$ y $x_i \\in \\mathbb{R}^D$, como una matriz $X \\in \\mathbb{R}^{N\\times D}$\n",
    "\n",
    "La transformación está representada por una matriz $W \\in \\mathbb{R}^{D\\times D}$ y los datos transformados se obtienen mediante una multiplicación matricial\n",
    "\n",
    "$$\n",
    "X' = X W\n",
    "$$\n",
    "\n",
    "Una transformación es ortogonal si cumple $W^T W = I$ es decir que la transformación traspuesta es equivalente a su inversa. Para encontrar una transformación que maximice la varianza de los datos debemos obtener primero la matriz de correlación de $X$ definida por\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{N} X^T X\n",
    "$$\n",
    "\n",
    "donde $C \\in \\mathbb{R}^{D\\times D}$ y donde asumimos que la media de $X$ es cero. Notemos que la matriz de correlación de los datos transformados es $\\frac{1}{N} X'^T X' = \\frac{1}{N} W^T X^T X W = W^T C W$ con lo que podemos escribir la siguiente función objetivo\n",
    "\n",
    "$$\n",
    "\\max_W W^T C W \\text{ sujeto a } W^T W = I\n",
    "$$\n",
    "\n",
    "Usando *multiplicadores de Lagrange* para incluir la restricción en el objetivo y derivando e igualando a cero obtemos lo siguiente\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dW} W^T C W + \\Lambda(I- W^T W) &= 0 \\nonumber \\\\ \n",
    "(C - \\Lambda) W &= 0 \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde $\\Lambda = \\lambda I$ y $\\lambda = (\\lambda_1, \\lambda_2, \\ldots, \\lambda_D)$\n",
    "\n",
    "Esto se conoce como el problema de los valores $\\lambda$ y vectores propios $W$ de la matriz $C$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Sean los siguientes datos bidimensionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.multivariate_normal([0, 0], [[0.5, -0.7], [-0.7, 1]], size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos PCA para encontrar los ejes coordenados de máxima varianza y graficarlos \n",
    "\n",
    "Para resolver el problema de valores propios usaremos `np.linalg.eigh` que recibe una matriz cuadrada y retorna sus valores y vectores propios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg\n",
    "# Restamos la media\n",
    "X_ = X - np.mean(X, axis=0, keepdims=True)\n",
    "# Calculamos la covarianza\n",
    "C = np.dot(X_.T, X_)/len(X_)\n",
    "# Calculamos los valores y vectores propios de la covarianza\n",
    "L, W = scipy.linalg.eigh(C)\n",
    "# Proyectamos\n",
    "U = np.dot(X, W)\n",
    "\n",
    "# Visualización de datos y proyección\n",
    "arrow_args = {'width': 0.05, 'length_includes_head': True, 'alpha': 0.5}\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3), dpi=120, \n",
    "                       tight_layout=True, sharex=True, sharey=True)\n",
    "ax[0].scatter(X[:, 0], X[:, 1], s=10);\n",
    "for i, c in enumerate(['g', 'r']):\n",
    "    ax[0].arrow(0, 0, W[i, 0], W[i, 1], color=c, **arrow_args)\n",
    "ax[0].set_aspect('equal'); \n",
    "ax[1].set_aspect('equal');\n",
    "ax[0].set_xlim([-3.5, 3.5])\n",
    "ax[1].scatter(U[:, 0], U[:, 1])\n",
    "ax[1].spines['bottom'].set_color('g')\n",
    "ax[0].set_ylabel('Atributo 1')\n",
    "ax[0].set_xlabel('Atributo 2')\n",
    "ax[1].set_ylabel('Componente principal 1')\n",
    "ax[1].set_xlabel('Componente principal 2')\n",
    "ax[1].spines['left'].set_color('r')\n",
    "ax[0].set_title('Datos originales')\n",
    "ax[1].set_title('Datos proyectados');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discusión:** \n",
    "\n",
    "- El eje rojo acumula un 99.5% de la varianza\n",
    "- El eje verde es ortogonal al rojo\n",
    "- Los nuevos ejes están decorrelacionados c/r a los originales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducción de dimensionalidad con PCA\n",
    "\n",
    "Una aplicación típica de PCA es la reducción de dimensionalidad\n",
    "\n",
    "Recordemos\n",
    "\n",
    "- La matriz $W$ tiene las mismas dimensiones que $C$\n",
    "- Las columnas de $W$ son los vector propios\n",
    "- Cada vector propio tiene un valor propio asociado\n",
    "\n",
    "Considerar que \n",
    "\n",
    "> El valor propio $\\lambda_i$ asociado a la columna $i$ de $W$ corresponde a la \"cantidad de varianza\" de dicha columna\n",
    "\n",
    "Para obtener una proyección que disminuya la dimensionalidad necesitamos una matriz $\\widehat W \\in \\mathbb{R}^{D\\times \\hat D}$. Podemos obtener $\\widehat W$ uniendo un subconjunto de las columnas de $W$. En particular nos interesa proyectar a los vectores propios de mayor varianza. \n",
    "\n",
    "- Para tareas de visualización de datos podemos retener los 2 o 3 componentes principales de mayor varianza\n",
    "- Para tareas más generales de reducción de dimensionalidad o decorrelación de atributos debemos encontrar una dimensión $\\hat D < D$ apropiada\n",
    "- Un criterio típico es ordenar los vectores propios de mayor a menor y retener aquellos que acumulen un 90% de la varianza\n",
    "- **Importante:** Si proyectamos con $\\widehat W$ la transformación no es invertible porque estamos \"descartando\" información. Es una compresión de tipo *lossy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA con `scikit-learn`\n",
    "\n",
    "PCA está incluido en el módulo [`sklearn.decomposition`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition)\n",
    "\n",
    "El constructor y sus argumentos más importantes son\n",
    "\n",
    "```python\n",
    "sklearn.decomposition.PCA(n_components=None, # Cantidad (int) o porcentaje de varianza (float) a retener\n",
    "                          copy=True, # Si es falso, la data transformada reemplaza la original\n",
    "                          whiten=False, # Reescala los datos para que tengan igual dispersión\n",
    "                          ...\n",
    "                         )\n",
    "```\n",
    "\n",
    "Los métodos más importantes de la clase `PCA` son\n",
    "\n",
    "- `fit(X)`: Calcula la matriz de correlación y los vectores y valores propios\n",
    "- `transform(X)`: Retorna la proyección de $X$\n",
    "- `fit_transform(X)`: `fit` y `transform` en un solo paso\n",
    "- `inverse_transform(hatX)`: Retorna los datos al espacio original (con pérdidas si $\\hat D < D$)\n",
    "- `get_covariance()`: Retorna la matriz de covarianza\n",
    "\n",
    "Los atributos más importantes son\n",
    "\n",
    "- `components_`: Los componentes principales (vectores propios)\n",
    "- `singular_values_`: Los valores propios\n",
    "- `explained_variance_ratio_`: El porcentaje de varianza asociado a cada vector propio \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Base de datos con **cuatro atributos numéricos** asociados a las características de un conjunto de 150 flores del género Iris separadas en 3 clases\n",
    "\n",
    "<img src=\"https://www.math.umd.edu/~petersd/666/html/iris_with_labels.jpg\">\n",
    "\n",
    "A continuación se presenta un gráfico de dispersión para visualizar las relaciones entre los atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "iris_set = sklearn.datasets.load_iris()\n",
    "X = iris_set.data\n",
    "Y = iris_set.target\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(8, 5), tight_layout=True, sharex=True, sharey=True)\n",
    "for i in range(3):\n",
    "    for j in range(i, 3):\n",
    "        for y in range(3):\n",
    "            ax[i, j].scatter(X[Y==y, i], X[Y==y, j+1], s=10, alpha=0.5)            \n",
    "\n",
    "ax[0 ,0].set_ylabel('Sepal length')\n",
    "ax[1 ,0].set_ylabel('Sepal width')\n",
    "ax[2 ,0].set_ylabel('Petal length')\n",
    "ax[2 ,0].set_xlabel('Sepal width')\n",
    "ax[2 ,1].set_xlabel('Petal length')\n",
    "ax[2 ,2].set_xlabel('Petal width');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer una visualización más concisa podemos usar PCA para reducir la dimensión de los datos de 4 a 2\n",
    "\n",
    "Además podemos estudiar la contribución de cada atributo a los nuevos ejes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "hatX = pca.fit_transform(X)\n",
    "hatW = pca.components_\n",
    "\n",
    "display(\"Vectores propios de los componentes más relevantes\", hatW)\n",
    "display(\"Valores propios\", pca.singular_values_)\n",
    "display(\"Porcentaje de varianza de cada vector propio\", pca.explained_variance_ratio_)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), dpi=120, tight_layout=True)\n",
    "for y, name in enumerate(iris_set.target_names):\n",
    "    ax[0].scatter(hatX[Y==y, 0], hatX[Y==y, 1], s=10, label=name)\n",
    "ax[0].legend();\n",
    "for ax_ in ax:\n",
    "    ax_.set_ylabel('Vector propio 2'); ax_.set_xlabel('Vector propio 1')\n",
    "ax[1].plot([0, 0], [-1 ,1], 'k--', alpha=0.5)\n",
    "ax[1].plot([-0.5, 2], [0, 0],  'k--', alpha=0.5)\n",
    "for i, name in enumerate(iris_set.feature_names):\n",
    "    ax[1].arrow(0, 0, hatW[0, i], hatW[1, i], color='b', **arrow_args)\n",
    "    ax[1].text(hatW[0, i], hatW[1, i], name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis**\n",
    "\n",
    "La figura de la izquierda son los datos de iris proyectados en los componentes principales más importantes. la figura de la derecha muestra la contribución de los atributos originales a cada uno de los vectores propios\n",
    "\n",
    "- Podemos notar que el tipo de flor, es decir las clases de Iris, pueden separarse en el eje del vector propio 1\n",
    "- Podemos ver también que las variables que tienen que ver con el sépalo están alineadas con el vector propio 2 (ángulo menor)\n",
    "- En cambio las variables que tienen que ver con el pétalo están alineadas con el vector propio 1\n",
    "- La clase de la flor tiene mayor relación con el pétalo que con el sépalo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering con algoritmo K-means\n",
    "\n",
    "### Formalismo matemático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means con `scikit-learn`\n",
    "\n",
    "K-means está incluido en el módulo [`sklearn.cluster`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster)\n",
    "\n",
    "El constructor y sus argumentos más importantes son\n",
    "\n",
    "```python\n",
    "sklearn.cluster.KMeans(n_clusters=8, # El número de grupos/clusters\n",
    "                       init='k-means++', #El algoritmo de inicialización puede ser random o k-means++\n",
    "                       n_init=10, # El número de condiciones iniciales que se prueban\n",
    "                       max_iter=300, # El número máximo de iteraciones\n",
    "                       tol=0.0001, # Si la diferencia entre los centroides en distintas iteraciones es menor que este valor, el algoritmo se detiene\n",
    "                       ...\n",
    "                      )\n",
    "```\n",
    "\n",
    "Los métodos más importantes de la clase `KMeans` son\n",
    "\n",
    "- `fit(X)`: Entrena y obtiene los centroides\n",
    "- `predict(X)`: Retorna el índice del cluster más cercano para cada dato\n",
    "- `fit_predict(X)`: `fit` y `predict` en un solo paso\n",
    "\n",
    "Los atributos más importantes son\n",
    "\n",
    "- `cluster_centers_`: Los centroides de los clusters\n",
    "- `inertia_`: La suma de errores cuadrados\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "A modo de ejemplo se realiza un clustering con KMeans sobre el dataset Iris. ¿Cuál es el número de clusters \n",
    "\n",
    "Para escoger el mejor número de clusters se utiliza el coeficiente de silueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics \n",
    "import sklearn.cluster\n",
    "\n",
    "iris_set = sklearn.datasets.load_iris()\n",
    "X = iris_set.data\n",
    "\n",
    "fig, ax = plt.subplots(4, 2, figsize=(5, 6), dpi=120, tight_layout=True)\n",
    "\n",
    "for k, n_clusters in enumerate(range(2, 6)):\n",
    "    # Clustering con kmeans\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=n_clusters)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    # Score de silueta promedio y por ejemplo\n",
    "    score_promedio = sklearn.metrics.silhouette_score(X, labels)\n",
    "    score_ejemplos = sklearn.metrics.silhouette_samples(X, labels)\n",
    "    y_lower = 10\n",
    "    for cluster in np.unique(labels):  \n",
    "        ax[k, 1].scatter(hatX[labels==cluster, 0], hatX[labels==cluster, 1], s=10)\n",
    "        \n",
    "        scores_cluster_sorted = np.sort(score_ejemplos[labels==cluster])\n",
    "        y_upper = y_lower + len(scores_cluster_sorted)\n",
    "        ax[k, 0].fill_betweenx(np.arange(y_lower, y_upper), 0, scores_cluster_sorted, alpha=0.7)\n",
    "        ax[k, 0].axvline(score_promedio, ls='--', c='k')\n",
    "        ax[k, 0].set_title(f'K: {n_clusters}')\n",
    "        ax[k, 0].set_xlim([0, 1])\n",
    "        y_lower = y_upper + 10\n",
    "    ax[-1, 0].set_xlabel('Coeficiente de silueta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis**\n",
    "\n",
    "La figura de la izquierda muestra los coeficientes de silueta de los ejemplos asociados a cada uno de los clusters. La linea punteada negra corresponde al coeficiente de silueta promedio. \n",
    "\n",
    "La figura de la derecha muestra un gráfico de dispersión donde el color corresponde a los clusters detecteados por KMeans\n",
    "\n",
    "Tanto el caso de $K=4$ como $K=5$ presentan algunos clusters cuyos coeficientes de silueta son menores que el promedio. Por otro lado el caso $K=2$ presenta alta disparidad den el tamaño de los clusters. En el caso $K=3$ los clusters son de un tamaño más uniforme y todos los clusters tienen ejemplos que superan el coeficiente promedio.\n",
    "\n",
    "Otra forma de guiar la selección del número de clusters es visualizar el decaimiento de la función de costo (suma de errores cuadráticos) en función del número de clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "n_clusters_test = range(2, 10)\n",
    "for n_clusters in n_clusters_test:\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(X)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(5, 3), dpi=120, tight_layout=True)\n",
    "ax.plot(n_clusters_test, sse)\n",
    "ax.set_ylabel('Suma de errores\\ncuadráticos')\n",
    "ax.set_xlabel('K');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso el mayor decaimiento en error cuadrático ocurre cuando pasamos de $K=2$ a $K=3$, lo cual indica que $K=3$ es una buena elección."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué aprendimos en esta lección?\n",
    "\n",
    "- A proyectar datos usando PCA\n",
    "- A agrupar datos usando KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
