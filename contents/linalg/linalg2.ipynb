{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from functools import partial\n",
    "import sklearn.datasets\n",
    "import scipy.linalg\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices como transformaciónes lineales\n",
    "\n",
    "Sea una base de datos (tabla) con atributos continuos $V \\in \\mathbb{R}^{M\\times D}$\n",
    "- Cada ejemplo tiene $D$ atributos\n",
    "- Si asumimos que $0^D$ es el origen entonces cada ejemplo es un **vector** $D$-dimensional \n",
    "- Existen $M$ vectores $v_i \\in \\mathbb{R}^D$  en $V$ (filas)\n",
    "\n",
    "Consideremos el caso $D=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_set = sklearn.datasets.load_iris()\n",
    "V = iris_set.data[-5:, 2:]\n",
    "display(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "arrow_args = {'width': 0.05, 'length_includes_head': True, 'alpha': 0.5}\n",
    "ax1.set_aspect('equal')\n",
    "ax1.set_xlim([0, 6]); ax1.set_ylim([0, 4]); \n",
    "ax1.set_xlabel(\"Largo del pétalo [cm]\")\n",
    "ax1.set_ylabel(\"Ancho del pétalo [cm]\")\n",
    "\n",
    "for v in V:\n",
    "    ax1.arrow(0, 0, v[0], v[1], color='k', **arrow_args)\n",
    "    ax1.scatter(v[0], v[1], cmap=plt.cm.Set3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos encontrar el vector promedio (rojo) y visualizarlo geometrícamente\n",
    "- Si lo restamos podemos mover el origen de nuestros ejes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vmean = np.mean(V, axis=0)\n",
    "ax1.arrow(0, 0, Vmean[0], Vmean[1], color='r', **arrow_args)\n",
    "ax1.scatter(Vmean[0], Vmean[1], color='k');\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "arrow_args = {'width': 0.01, 'length_includes_head': True, 'alpha': 0.5}\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim([-0.5, 0.5]); ax.set_ylim([-0.5, 0.5]); \n",
    "ax.set_xlabel(\"Largo del pétalo [cm] - %0.2f\" %(Vmean[0]))\n",
    "ax.set_ylabel(\"Ancho del pétalo [cm] - %0.2f\" %(Vmean[1]))\n",
    "\n",
    "V_ = V - Vmean \n",
    "for v in V_:\n",
    "    ax.arrow(0, 0, v[0], v[1], color='k', **arrow_args)\n",
    "    ax.scatter(v[0], v[1], cmap=plt.cm.Set3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos una matriz cuadrada $A = \\begin{pmatrix} \\alpha & \\beta \\\\ \\beta & \\alpha \\end{pmatrix}$\n",
    "\n",
    "¿Qué le ocurre a nuestros datos centrados si los multiplicamos por $A$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax2 = plt.subplots(1, figsize=(6, 4), tight_layout=True)\n",
    "\n",
    "def update_plot(alpha, beta):\n",
    "    A = [[alpha, beta], [beta, alpha]]    \n",
    "    display(A)\n",
    "    ax2.cla(); ax2.set_aspect('equal')\n",
    "    ax2.set_xlim([-0.5, 0.5]); ax2.set_ylim([-0.5, 0.5]);   \n",
    "    V_ = V - Vmean \n",
    "    for v in V_:\n",
    "        ax2.arrow(0, 0, v[0], v[1], color='k', **arrow_args)\n",
    "        ax2.scatter(v[0], v[1], cmap=plt.cm.Set3);\n",
    "    ax2.set_prop_cycle(None)    \n",
    "    Vn = np.dot(V_, A)\n",
    "    for v in Vn:\n",
    "        ax2.arrow(0, 0, v[0], v[1], color='b', **arrow_args)\n",
    "        ax2.scatter(v[0], v[1], cmap=plt.cm.Set3);    \n",
    "    \n",
    "widgets.interact(update_plot, \n",
    "                 alpha=FloatSlider_nice(min=-2, max=2, value=1.), \n",
    "                 beta=FloatSlider_nice(min=-2, max=2, value=0.));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación afín\n",
    "\n",
    "- Sumando un vector podemos trasladar nuestros datos\n",
    "- Multiplicando por una matriz cuadrada podemos rotar y escalar nuestros datos\n",
    "- La combinación de estas operaciones se conoce como [**transformación afín**](https://en.wikipedia.org/wiki/Affine_transformation#Image_transformation)\n",
    "\n",
    "$$\n",
    "v = A v' + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bases y transformaciones\n",
    "\n",
    "> La base de un espacio es un conjunto de vectores tal que todos los elementos del espacio se pueden escribir como una combinación de esos vectores\n",
    "\n",
    "En el caso bidimensional la base estándar es $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "y un vector cualquiera puede representarse como\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} x \\\\ y \\end{pmatrix} = x \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + y \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Geometricamente podemos interpretar la base como los **ejes coordenados** de nuestro sistema\n",
    "\n",
    "### Cambio de coordenadas\n",
    "\n",
    "Una multiplicación por una matriz podría verse entonces como un \"cambio de ejes coordenados\"\n",
    "\n",
    "¿Qué le ocurre a la base estándar si la multiplicamos por $A = \\begin{pmatrix} a_1 & a_2 \\\\ a_3 & a_4 \\end{pmatrix}$\n",
    "\n",
    "¿Cómo se ven los datos en el nuevo espacio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax3 = plt.subplots(1, 2, figsize=(6, 4), tight_layout=True)\n",
    "world_axis = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "def update_plot(a1, a2, a3, a4):\n",
    "    A = [[a1, a2], [a3, a4]]\n",
    "    new_world_axis = np.dot(A, world_axis)\n",
    "    print(new_world_axis)\n",
    "    for ax_ in ax3:\n",
    "        ax_.cla(); ax_.set_aspect('equal')\n",
    "        ax_.set_xlim([-1, 1]); ax_.set_ylim([-1, 1]);  \n",
    "        ax_.spines['right'].set_color('none')\n",
    "        ax_.spines['top'].set_color('none')\n",
    "        ax_.spines['bottom'].set_position(('data', 0))\n",
    "        ax_.spines['left'].set_position(('data', 0))        \n",
    "        ax_.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1))\n",
    "        ax_.yaxis.set_major_locator(mpl.ticker.MultipleLocator(1))\n",
    "    ax3[1].spines['bottom'].set_color('blue') \n",
    "    ax3[1].spines['left'].set_color('green') \n",
    "    ax3[0].set_title('Espacio original')\n",
    "    ax3[1].set_title('Proyección en los nuevos ejes')\n",
    "    for j, c in enumerate(['b', 'g']):\n",
    "        ax3[0].arrow(0, 0, new_world_axis[0, j], new_world_axis[1, j], color=c, **arrow_args)\n",
    "    V_ = V - Vmean \n",
    "    for v in V_:\n",
    "        ax3[0].scatter(v[0], v[1], cmap=plt.cm.Set3);\n",
    "    Vn = scipy.linalg.solve(A, V_.T).T\n",
    "    for v in Vn:\n",
    "        ax3[1].scatter(v[0], v[1], cmap=plt.cm.Set3);  \n",
    "    \n",
    "widgets.interact(update_plot, \n",
    "                 a1=FloatSlider_nice(min=-1, max=1, value=1.), \n",
    "                 a2=FloatSlider_nice(min=-1, max=1, value=0.), \n",
    "                 a3=FloatSlider_nice(min=-1, max=1, value=0),\n",
    "                 a4=FloatSlider_nice(min=-1, max=1, value=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los ejes de la base estándar se escalan y rotan obteniendo un nuevo par de ejes \n",
    "\n",
    "La proyección de los vectores originales $\\vec v$ a los nuevos ejes es\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{pmatrix} x \\\\ y \\end{pmatrix} &=  A \\begin{pmatrix} x' \\\\ y' \\end{pmatrix} \\\\\n",
    "A^{-1}\\begin{pmatrix} x \\\\ y \\end{pmatrix} &=   \\begin{pmatrix} x' \\\\ y' \\end{pmatrix} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Por ejemplo si $A = \\begin{pmatrix} 0.5 & 0 \\\\ 0 & 1 \\end{pmatrix}$ tendríamos una nueva base donde el eje horizontal es \"más corto\" y la proyección de $(x, y)$ sería $(2x, y)$\n",
    "\n",
    "\n",
    "Consideremos ahora una transformación en particular  $A = \\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{pmatrix}$ y un vector cualquiera $v = \\rho \\begin{pmatrix} \\sin(\\theta) \\\\ \\cos(\\theta) \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation \n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(6, 4), tight_layout=True)\n",
    "world_axis = np.array([[1, 0], [0, 1]])\n",
    "A = [[1, 0.5], [0.5, 1]]\n",
    "new_world_axis = np.dot(A, world_axis)\n",
    "theta = np.linspace(-np.pi, np.pi, num=100)\n",
    "rho = 0.75\n",
    "\n",
    "def update_plot(k):    \n",
    "    v = rho*np.array([np.sin(theta[k]), np.cos(theta[k])])\n",
    "    ax.cla(); ax.set_aspect('equal')\n",
    "    ax.set_xlim([-1.2, 1.2]); ax.set_ylim([-1.2, 1.2]);  \n",
    "    ax.axis('off')\n",
    "    ax.set_title(r\"$\\theta$ = %0.4f\" %(theta[k]))\n",
    "    for j in range(2):\n",
    "        ax.arrow(0, 0, world_axis[0, j], world_axis[1, j], \n",
    "                 linestyle='--', color='k', **arrow_args)\n",
    "        ax.arrow(0, 0, new_world_axis[0, j], new_world_axis[1, j], \n",
    "                 linestyle='--', color='b', **arrow_args)\n",
    "    for v_,c  in zip([v, np.dot(A, v)], ['k', 'b']):\n",
    "        ax.scatter(v_[0], v_[1], color=c);\n",
    "        ax.arrow(0, 0, v_[0], v_[1], color=c, **arrow_args)\n",
    "    \n",
    "animation.FuncAnimation(fig, update_plot, frames=len(theta), interval=100, \n",
    "                        repeat=True, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Existen algunos vectores que al ser transformados por $A$ mantienen su orientación original\n",
    "\n",
    "En el ejemplo anterior esto ocurre para $\\theta = \\pm \\pi/4 \\approx \\pm 0.7854$, es decir $v = \\frac{\\rho}{\\sqrt{2}} \\begin{pmatrix}  \\pm 1 \\\\ 1\\end{pmatrix}$\n",
    "\n",
    "Los vectores que tienen ese ángulo **son afectados sólo en su escala**\n",
    "\n",
    "> Esos vectores se conocen como los *vectores propios* de $A$\n",
    "\n",
    "¿Cómo podemos encontrar los vectores propios para una matriz $A$ cualquiera?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema de los valores/vectores propios\n",
    "\n",
    "Sea una matriz cuadrada $A \\in \\mathbb{R}^{D\\times D}$\n",
    "\n",
    "El siguiente sistema de ecuaciones de $D$ ecuaciones\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "A \\vec v &= \\lambda I \\vec v \\\\\n",
    "(A - \\lambda I) \\vec v &= 0,\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "tiene como resultado \n",
    "- $\\lambda$, los valores propios de $A$\n",
    "- $\\vec v$ los vectores propios de $A$\n",
    "\n",
    "La solución no trivial de este problema ($\\vec v \\neq 0$) se obtiene si $(A - \\lambda I)$ es singular, es decir\n",
    "\n",
    "$$\n",
    "|A - \\lambda I | = 0\n",
    "$$\n",
    "\n",
    "que resulta en un polinomio de grado $D$ cuyas raices son $\\{\\lambda_i\\}$, $i=1,2,\\ldots, D$\n",
    "\n",
    "Una vez determinado $\\lambda_i$ se pueden usar para despejar $\\vec v_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Para la matriz $A$ del ejemplo, si igualamos su determinante a cero tenemos\n",
    "\n",
    "$$\n",
    "(1 - \\lambda)^2 - 1/4 = 3/4 - 2\\lambda + \\lambda^2 = 0\n",
    "$$\n",
    "\n",
    "osea $\\lambda_1 = 1.5$ y $\\lambda_2 = 0.5$. Luego para el primer vector propio tenemos un sistema de ecuaciones\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "-0.5v_{11} +0.5v_{12} &= 0 \\\\\n",
    "0.5 v_{11} -0.5v_{12} &= 0\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "osea $v_{11} = v_{12}$ con esto podemos construir un vector normalizado genérico $v_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "De forma equivalente para $v_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "### Ahora usemos Python\n",
    "\n",
    "Podemos usar las funciones de [`scipy.linalg`](https://docs.scipy.org/doc/scipy/reference/linalg.html#eigenvalue-problems) `eig()` o `eigvals()` (y sus variantes) para resolver el sistema de ecuaciones\n",
    "\n",
    "`eig` retorna una tupla con los valores y vectores propios\n",
    "\n",
    "Para el ejemplo anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg\n",
    "\n",
    "A = np.array([[1., 0.5], [0.5, 1]])\n",
    "evals, evecs = scipy.linalg.eig(A)\n",
    "display(evals, evecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descomposicion en valores propios\n",
    "\n",
    "Existen [múltiples sistemas en física](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Applications) que [ocurren naturalmente](https://hubpages.com/education/What-the-Heck-are-Eigenvalues-and-Eigenvectors) como un problema de valores/vectores propios\n",
    "\n",
    "Sin embargo la aplicación más amplia para este elemento matemático es la **descomposición en vectores propios**\n",
    "\n",
    "> Descomponer: Expresar un elemento como una suma de partes de más simples\n",
    "\n",
    "La descomposición que veremos a continuación usa los **vectores propios** como \"las partes simples\" (base)\n",
    "\n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Es un procedimiento estadístico que busca una **transformación ortogonal** para los datos que logre **maximizar su varianza**\n",
    "\n",
    "Un conjunto de datos $\\{x_i\\}$ con $i=1,2,\\ldots, M$ y $x_i \\in \\mathbb{R}^D$\n",
    "\n",
    "Podemos escribirlo como una matriz $X \\in \\mathbb{R}^{M\\times D}$\n",
    "\n",
    "Asumiendo que los datos **tienen media cero**, podemos calcular su matriz de correlación como \n",
    "$$\n",
    "C = \\frac{1}{M} X^T X\n",
    "$$\n",
    "\n",
    "donde $C \\in \\mathbb{R}^{D\\times D}$ y $\\bar X$ es la media del conjunto\n",
    "\n",
    "Llamemos $W \\in \\mathbb{R}^{D\\times D}$ a la matriz de proyección y $X' = X W$ los datos proyectados\n",
    "\n",
    "- Varianza de $X'$: $\\frac{1}{M} X'^T X' = \\frac{1}{M} W^T X^T X W = W^T C W$\n",
    "- Ortogonalidad: $W^T W = I$\n",
    "\n",
    "Luego el problema de PCA se puede escribir como\n",
    "\n",
    "$$\n",
    "\\max_W W^T C W \\text{ sujeto a } W^T W = I\n",
    "$$\n",
    "\n",
    "Si usamos *multiplicadores de Lagrange* para incluir la restricción. Luego derivamos e igualamos a cero\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dW} W^T C W + \\Lambda(I- W^T W) &= 0 \\nonumber \\\\ \n",
    "CW - \\Lambda W &= 0 \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde $\\Lambda = \\lambda I$ y $\\lambda = (\\lambda_1, \\lambda_2, \\ldots, \\lambda_D)$\n",
    "\n",
    "> La transformación de PCA consiste en encontrar los valores y vectores propios de $C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Sean los siguientes datos bidimensionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.multivariate_normal([0, 0], [[0.5, -0.7], [-0.7, 1]], size=1000)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 4), tight_layout=True, sharex=True, sharey=True)\n",
    "ax[0].scatter(X[:, 0], X[:, 1], s=10);\n",
    "ax[0].set_aspect('equal'); ax[1].set_aspect('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos PCA para encontrar los ejes coordenados de máxima varianza y graficarlos \n",
    "\n",
    "Nota de Python: Conviene usar `eigh` que es una versión de `eig` para matrices simétricas\n",
    "\n",
    "**Resultado:** \n",
    "- El eje rojo acumula un 99.5% de la varianza\n",
    "- El eje verde es ortogonal al rojo\n",
    "- Los nuevos ejes están decorrelacionados c/r a los originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resto la media\n",
    "X_ = X - np.mean(X, axis=0, keepdims=True)\n",
    "# Calculo la covarianza\n",
    "C = np.dot(X_.T, X_)/len(X_)\n",
    "# Calculo los valores y vectores propios de la covarianza\n",
    "L, W = scipy.linalg.eigh(C)\n",
    "for i, c in enumerate(['g', 'r']):\n",
    "    ax[0].arrow(0, 0, W[i, 0], W[i, 1], color=c, **arrow_args)\n",
    "    ax[0].text(W[i, 0], W[i, 1], \"%0.3f\" %(L[i]))\n",
    "# Los datos proyectados\n",
    "U = np.dot(X, W)\n",
    "ax[1].scatter(U[:, 0], U[:, 1])\n",
    "ax[1].spines['bottom'].set_color('g')\n",
    "ax[1].spines['left'].set_color('r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de dimensionalidad con PCA\n",
    "\n",
    "Una aplicación típica de PCA es la reducción de dimensionalidad\n",
    "\n",
    "Recordemos\n",
    "- La matriz $W$ tiene las mismas dimensiones que $C$\n",
    "- Las columnas de $W$ son los vector propios\n",
    "- Cada vector propio tiene un valor propio asociado\n",
    "\n",
    "> El valor propio $\\lambda_i$ asociado a la columna $i$ de $W$ corresponde a la \"cantidad de varianza\" de dicha columna\n",
    "\n",
    "Podemos disminuir la dimensión de nuestros datos si proyectamos usando $\\widehat W$ en lugar de $W$ \n",
    "\n",
    "$\\widehat W$ se obtiene uniendo un subconjunto de las columnas de $W$\n",
    "\n",
    "En particular nos interesa proyectar a los vectores propios de mayor varianza \n",
    "\n",
    "- Si queremos visualizar podemos retener los 2 o 3 componentes principales de mayor varianza\n",
    "- Si sólo queremos reducir dimensión o decorrelacionar debemos encontrar una dimensión $\\hat D < D$ apropiada\n",
    "    - Un criterio típico es retener los vectores propios que acumulen un 90% de la varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Iris\n",
    "\n",
    "Base de datos con **cuatro atributos numéricos** asociados a las características de un conjunto de 150 flores del género Iris separadas en 3 clases\n",
    "\n",
    "<img src=\"https://www.math.umd.edu/~petersd/666/html/iris_with_labels.jpg\">\n",
    "\n",
    "Podemos visualizar las relaciones entre las cuatro características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_set = sklearn.datasets.load_iris()\n",
    "X = iris_set.data\n",
    "Y = iris_set.target\n",
    "X -= np.mean(X, axis=0)\n",
    "display(X.shape)\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(6, 4), tight_layout=True, sharex=True, sharey=True)\n",
    "for i in range(3):\n",
    "    for j in range(i, 3):\n",
    "        for y in range(3):\n",
    "            ax[i, j].scatter(X[Y==y, i], X[Y==y, j+1], s=10, alpha=0.5)\n",
    "ax[0 ,0].set_ylabel('Sepal length')\n",
    "ax[1 ,0].set_ylabel('Sepal width')\n",
    "ax[2 ,0].set_ylabel('Petal length')\n",
    "ax[2 ,0].set_xlabel('Sepal width')\n",
    "ax[2 ,1].set_xlabel('Petal length')\n",
    "ax[2 ,2].set_xlabel('Petal width');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer una visualización más concisa podemos \n",
    "\n",
    "> usar PCA para reducir la dimensión de los datos de 4 a 2\n",
    "\n",
    "Además podemos estudiar la contribución de cada atributo a los nuevos ejes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.dot(X.T, X)/len(X)\n",
    "display(iris_set.feature_names)\n",
    "L, W = scipy.linalg.eigh(C)\n",
    "display(\"Matriz W\")\n",
    "display(W)\n",
    "display(\"Valores propios\")\n",
    "display(L)\n",
    "display(\"Porcentage de varianza de cada vector propio\")\n",
    "display(100*L/np.sum(L))\n",
    "\n",
    "P = np.dot(X, W[:, [3, 2]])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "for y, name in enumerate(iris_set.target_names):\n",
    "    ax[0].scatter(P[Y==y, 0], P[Y==y, 1], s=10, label=name)\n",
    "ax[0].legend(); ax[1].set_aspect('equal')\n",
    "for ax_ in ax:\n",
    "    ax_.set_ylabel('PC2'); ax_.set_xlabel('PC3')\n",
    "ax[1].plot([0, 0], [-1 ,1], 'k--', alpha=0.5)\n",
    "ax[1].plot([-0.5, 2], [0, 0],  'k--', alpha=0.5)\n",
    "for i, name in enumerate(iris_set.feature_names):\n",
    "    ax[1].arrow(0, 0, W[i, 3], W[i, 2], color='b', **arrow_args)\n",
    "    ax[1].text(W[i, 3], W[i, 2], name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El vector propio con mayor varianza es PC3 (cuarta columna de W)\n",
    "\n",
    "En este caso podemos notar como las distintas clases de Iris aparecen en distintos rangos de PC3\n",
    "\n",
    "### Interpretación de los componentes principales\n",
    "\n",
    "Podemos estudiar como se conforman los componentes principales para entenderlos mejor\n",
    "\n",
    "En el ejemplo anterior PC3 es influenciado principalmente por el atributo \"largo del pétalo\". Luego de forma equivalente y en menor medida por \"ancho del pétalo\" y \"largo del sépalo\"\n",
    "\n",
    "PC2 en cambio está comformado principalmente de los atributos de los sépalos\n",
    "\n",
    "Este ejercicio es útil para poder darle una interpretación a los componentes principales en nuestro análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio: *Eigen-faces* o Rostros principales\n",
    "\n",
    "Para el set de imágenes de 50x37 píxeles de rostros famosos LFW\n",
    "- ¿Cúales son los rostros principales (vectores propios)?\n",
    "- ¿Cúanto podemos reducir en dimensionalidad sin afectar la calidad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar la data, esto demora un poco\n",
    "import sklearn.datasets\n",
    "lfw_people = sklearn.datasets.fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "X = lfw_people['data'].astype('float64')\n",
    "display(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando los 21 primeros ejemplos\n",
    "fig, ax = plt.subplots(3, 7, figsize=(7, 4))\n",
    "for i, ax_ in enumerate(ax.ravel()):\n",
    "    ax_.axis('off')\n",
    "    ax_.imshow(X[i, :].reshape(50, 37), cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando el rostro promedio\n",
    "fig, ax = plt.subplots(1, figsize=(5, 2))\n",
    "ax.axis('off')\n",
    "ax.imshow(np.mean(X, axis=0).reshape(50, 37), cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "50*37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restamos la media\n",
    "X_mean = np.mean(X, axis=0, keepdims=True)\n",
    "X_center = X - X_mean\n",
    "# Calculamos C\n",
    "C = np.dot(X_center.T, X_center)/len(X_center)\n",
    "display(C.shape)\n",
    "# Obtenemos los valores y vectores propios\n",
    "L, W = scipy.linalg.eigh(C)\n",
    "# Ordenar de L más grande a más pequeño\n",
    "idx = np.argsort(L)[::-1] \n",
    "L = L[idx]\n",
    "W = W[:, idx]\n",
    "plt.figure(figsize=(5, 3), tight_layout=True)\n",
    "plt.plot(np.cumsum(100*L/np.sum(L)))\n",
    "plt.ylabel('Varianza acumulada');\n",
    "plt.xlabel('Cantidad de componentes principales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rostro principales\n",
    "\n",
    "Los 21 componentes principales de mayor varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 7, figsize=(7, 4.5), tight_layout=True)\n",
    "for i, ax_ in enumerate(ax.ravel()):\n",
    "    ax_.axis('off')\n",
    "    ax_.set_title(\"{0:0.1e}\".format(L[i]))\n",
    "    ax_.imshow(W[:, i].reshape(50, 37), cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstrucción de rostros a partir de los componentes principales\n",
    "\n",
    "Aumente el valor de $k$ para incluir progresivamente más rostros principales a la reconstrucción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3))\n",
    "ax[0].axis('off'); ax[1].axis('off'); ax[2].axis('off')\n",
    "ax[0].set_title('Original')\n",
    "ax[1].set_title('Reconstrucción')\n",
    "ax[2].set_title('Error')\n",
    "\n",
    "d = 0\n",
    "ax[0].imshow(X[d, :].reshape(50, 37), cmap=plt.cm.Greys_r)\n",
    "# Esto calcula los coeficientes de la imagen d\n",
    "P = np.dot(X_center[d, :], W[:2])\n",
    "# Esto regenera la imagen cero a partir de sus coeficientes\n",
    "# Xhat = X_mean + np.dot(W, P)\n",
    "# np.allclose(X[d, :], Xhat[0, :], rtol=1e-3)\n",
    "\n",
    "var_acum = 100*np.cumsum(L)/np.sum(L)\n",
    "# ¿Cuantos coeficientes se necesitan para que X se parezca a Xhat?\n",
    "def update_plot(k):\n",
    "    Xhat = X_mean + np.dot(W[:, :k], P[:k])\n",
    "    ax[1].imshow(Xhat.reshape(50, 37), cmap=plt.cm.Greys_r)\n",
    "    ax[2].imshow(X[d, :].reshape(50, 37) - Xhat.reshape(50, 37), cmap=plt.cm.RdBu_r, vmin=-255, vmax=255)\n",
    "    display(\"Porcentaje de varianza:\", 100*np.sum(L[:k])/np.sum(L))\n",
    "widgets.interact(update_plot, k=SelSlider_nice(options=[1, 2, 5, 10, 20, 50, 100, 250, 500, \n",
    "                                                        1000, 1500, 1849]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "Tema para otra iteración\n",
    "\n",
    "https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "Tema para otra iteración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
