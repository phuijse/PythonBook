{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T22:07:27.240767Z",
     "start_time": "2020-07-29T22:07:26.389604Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnóstico de cancer usando un regresor logístico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considere el dataset de [diagnóstico de cancer de mama de la Universidad de Wisconsin](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T22:07:27.289000Z",
     "start_time": "2020-07-29T22:07:27.243067Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('cancer.csv', index_col=0)\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta actividad formativa es entrenar un regresor logístico para clasificar las muestras como benignas y malignas (columna *diagnosis*) en base a las demás columnas disponibles. Las columnas corresponden al valor medio (1), desviación estándar (2) y \"peor caso\" (3) de 10 descriptores de la forma de los nucleos celulares presentes en la biopsia del paciente. Cada fila en la tabla corresponde a un paciente distinto.\n",
    "\n",
    "Revisando este ejercicio aprenderas a\n",
    "\n",
    "1. formular una regresión logística \n",
    "1. estimar los mejores parámetros de un regresor logístico en base a datos\n",
    "1. implementar un regresor logístico con `numpy` y `scipy`\n",
    "1. analizar los resultados del regresor logístico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de regresión logística\n",
    "\n",
    "El regresor logístico es la extensión del regresor lineal para problemas de clasificación binaria, es decir donde los datos se agrupan en dos clases\n",
    "\n",
    "Sean $N$ tuplas $\\{\\vec x_i, y_i\\}$ donde $\\vec x_i \\in \\mathbb{R}^D$ y $y_i \\in \\{0, 1\\}$. El modelo de **regresión logística** para estos datos busca ajustar\n",
    "\n",
    "$$\n",
    "y_i \\approx \\mathcal{S} \\left(\\theta_0 + \\sum_{j=1}^D \\theta_j x_{ij}\\right),\n",
    "$$\n",
    "\n",
    "donde $\\mathcal{S}(z) = \\frac{1}{1+\\exp(-z)}$ se conoce como función logística o sigmoide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(-10, 10, step=0.1)\n",
    "sigmoide = lambda z : 1/(1+np.exp(-z))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.plot(z, sigmoide(z));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de costo para el regresor logístico\n",
    "\n",
    "Definamos primero el regresor lineal como \n",
    "\n",
    "$$\n",
    "f_\\theta(x_i) = \\theta_0 + \\sum_{j=1}^D \\theta_j x_{ij}\n",
    "$$\n",
    "\n",
    "en base a esta definición el modelo de regresión logística es entonces $\\mathcal{S} (f_\\theta(x_i))$\n",
    "\n",
    "Para optimizarlo usaremos como función de costo el **logaritmo de la verosimilitud** (ver lección estadística inferencial para más detalles)\n",
    "\n",
    "$$\n",
    "\\hat \\theta = \\text{arg} \\max_\\theta \\log \\mathcal{L}(\\theta)\n",
    "$$\n",
    "\n",
    "Asumiremos que las observaciones $y_i$ son independientes y que siguen una [distribución de Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution) con parámetro $p=\\mathcal{S} (f_\\theta(x_i))$. Luego podemos escribir el logaritmo de la verosimilitud como\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log \\mathcal{L}(\\theta) &= \\sum_{i=1}^N \\log p(y_i | \\mathcal{S} (f_\\theta(x_i)) ) \\nonumber \\\\\n",
    "&= \\sum_{i=1}^N \\log \\mathcal{S} (f_\\theta(x_i))^{y_i} (1-\\mathcal{S} (f_\\theta(x_i)) )^{1-y_i} \\nonumber \\\\\n",
    "&= \\sum_{i=1}^N y_i \\log (\\mathcal{S} (f_\\theta(x_i)) ) + (1-y_i) \\log(1-\\mathcal{S} (f_\\theta(x_i)) ) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde la primera igualdad viene de reemplazar la función de masa de la distribución de Bernoulli y la segundo de aplicar el logaritmo\n",
    "\n",
    "Este modelo no admite una solución analítica cerrada, por lo tanto usaremos un método de optimización iterativo que tome el logaritmo de la verosimilitud como función de costo. En particular usaremos un algoritmo basado en gradiente descendente por lo que necesitamos calcular la derivada de la función de costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivada de la log verosimilitud con respecto a $\\theta_j$ es\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\theta_j} \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^N \\left ( \\frac{y_i}{\\mathcal{S} (f_\\theta(x_i))} - \\frac{1-y_i}{1-\\mathcal{S} (f_\\theta(x_i))} \\right) \\frac{d \\mathcal{S} (f(x_i, \\theta))}{d f_\\theta(x_i)} \\frac{d f_\\theta(x_i)}{d \\theta_j} \n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$\n",
    "\\frac{d \\mathcal{S}(z)}{dz} =  \\mathcal{S}(z) ( 1 - \\mathcal{S}(z))\n",
    "$$\n",
    "\n",
    "entonces\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{d\\theta_j} \\log \\mathcal{L}(\\theta) &= \\sum_{i=1}^N \\left[ y_i - y_i\\mathcal{S} (f_\\theta(x_i)) - \\mathcal{S} (f_\\theta(x_i))  + y_i \\mathcal{S} (f_\\theta(x_i)) \\right] \\frac{d f_\\theta(x_i)}{d \\theta_j} \\nonumber \\\\\n",
    "&= \\sum_{i=1}^N \\left[ y_i  - \\mathcal{S} (f_\\theta(x_i)) \\right] \\frac{d f_\\theta(x_i)}{d \\theta_j} \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "La derivada con respecto a los parámetros es \n",
    "\n",
    "$$\n",
    "\\frac{d f(x_i, \\theta)}{d \\theta_0} =  1\n",
    "$$\n",
    "\n",
    "y para $j>0$\n",
    "$$\n",
    "\\frac{d f(x_i, \\theta)}{d \\theta_j} =  x_{ij}\n",
    "$$\n",
    "\n",
    "Con esto tenemos todo lo necesario para implementar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación y optimización del modelo \n",
    "\n",
    "Primero implementamos el logaritmo de la verosimilitud y su gradiente\n",
    "\n",
    "Lo hacemos de tal forma que sean compatibles con `scipy.optimize.minimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T22:07:27.322849Z",
     "start_time": "2020-07-29T22:07:27.291008Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modelo\n",
    "def modelo(theta, X):\n",
    "    f = theta[0] + np.sum(theta[1:]*X, axis=1)     \n",
    "    return sigmoide(f)\n",
    "\n",
    "# Función de costo \n",
    "def neglogverosimilitud(theta, *args):\n",
    "    X, Y = args\n",
    "    S = modelo(theta, X)\n",
    "    # Le agregamos un signo menos ya que quiero minimizar en lugar de maximizar\n",
    "    return -np.sum(Y*np.log(S+1e-10) + (1-Y)*np.log(1-S+1e-10), axis=0)\n",
    "    # La función logaddexp es más estable numericamente:\n",
    "    #return -np.sum(-np.logaddexp(0, -f) - (1-y)*f, axis=0)\n",
    "    \n",
    "# Gradiente \n",
    "def grad_neglogverosimilitud(theta, *args):\n",
    "    X, Y = args\n",
    "    N = len(Y)\n",
    "    S = modelo(theta, X)\n",
    "    X1 = np.concatenate((np.ones(shape=(N, 1)), X), axis=1)\n",
    "    e = (Y - S)\n",
    "    return -np.sum(e[:, np.newaxis]*X1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ajuste del modelo se muestra a continuación. \n",
    "\n",
    "Para facilitar la convergencia del modelo estándarizamos los atributos restándole su media y dividiendo por su desviación estándar\n",
    "\n",
    "Aprovechando que tenemos la información del gradiente usaremos BFGS para optimizar. BFGS es un método quasi-Newton que usa información de las primeras derivadas\n",
    "\n",
    "Para validar que el modelo usaremos un conjunto *hold-out*. Otras técnicas para validar son *k-fold* y *bootstrap*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T22:07:27.436894Z",
     "start_time": "2020-07-29T22:07:27.324632Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obtener base de datos\n",
    "Y = df[\"diagnosis\"].values\n",
    "X = df.drop(columns=[\"diagnosis\"]).values\n",
    "# Estandarizar\n",
    "X_std = (X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "N = len(Y)\n",
    "D = X_std.shape[1]\n",
    "train_proportion = 0.75\n",
    "idx = np.random.permutation(len(X))\n",
    "train_idx, test_idx = idx[:int(N*train_proportion)], idx[int(N*train_proportion):]\n",
    "\n",
    "# Solución inicial\n",
    "theta_init = 0.1*np.random.randn(1+D)\n",
    "# Usaremos un callback para guardar el mejor modelo de validación\n",
    "best_theta = np.zeros_like(theta_init)\n",
    "best_logl = np.inf\n",
    "def eval_model(theta):  \n",
    "    global best_theta, best_logl\n",
    "    logltrain = neglogverosimilitud(theta, *(X_std[train_idx, :], Y[train_idx]))/len(train_idx)\n",
    "    logltest = neglogverosimilitud(theta, *(X_std[test_idx, :], Y[test_idx]))/len(test_idx)\n",
    "    print(\"Train: %0.4f, Test: %0.4f\" %(logltrain, logltest))   \n",
    "    if logltest < best_logl: # Guardar el mejor modelo de test\n",
    "        best_theta = theta\n",
    "        best_logl = logltest\n",
    "\n",
    "# Mejor valor de theta\n",
    "res = scipy.optimize.minimize(fun=neglogverosimilitud, x0=theta_init, \n",
    "                              method='BFGS', jac=grad_neglogverosimilitud, \n",
    "                              args=(X_std[train_idx, :], Y[train_idx]),\n",
    "                              callback=eval_model, tol=1e-1)\n",
    "\n",
    "print(res.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del modelo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida de este clasificador es un número en el rango $[0, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = modelo(best_theta, X_std)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.hist(p[Y==1], label='Ejemplos con etiqueta maligno', bins=10, range=(0, 1), alpha=0.5, color='r')\n",
    "ax.hist(p[Y==0], label='Ejemplos con etiqueta benigno', bins=10, range=(0, 1), alpha=0.5, color='b')\n",
    "ax.set_xlabel('Predicción del modelo')\n",
    "#ax.set_yscale('log')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tomar un decisión se debe seleccionar un umbral $\\mathcal{T} \\in [0,1]$ tal que\n",
    "\n",
    "$$\n",
    "d_i = \n",
    "\\begin{cases} \n",
    "\\text{maligno/1}, & \\text{si } p(y_i|\\theta, \\vec x_i) < \\mathcal{T} \\\\ \n",
    "\\text{benigno/0}, & \\text{si } p(y_i|\\theta, \\vec x_i) \\geq \\mathcal{T}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Una vez seleccionado el umbral se puede contar la cantidad de \n",
    "- True positives (TP): Tumores malignos clasificados como malignos\n",
    "- True negative (TN): Tumores benignos clasificados como benignos\n",
    "- False positives (FP): Tumores benignos clasificados como malignos\n",
    "- False negative (FN): Tumores malignos clasificados como benignos \n",
    "\n",
    "Estas métricas son la base para construir una \"tabla o matriz de confusión\" para el clasificador\n",
    "\n",
    "|Clasificado como/En realidad era|Positivo/1:|Negativo/0:|\n",
    "|---|---|---|\n",
    "|Positivo/1:|TP | FP |\n",
    "|Negativo/0:| FN | TN |\n",
    "\n",
    "Por ejemplo si usamos $\\mathcal{T} = 0.5$ tendríamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matriz_confusion(Y_real, Y_pred):\n",
    "    TP = sum(Y_real & Y_pred)\n",
    "    FP = sum(~Y_real & Y_pred)\n",
    "    FN = sum(Y_real & ~Y_pred)\n",
    "    TN = sum(~Y_real & ~Y_pred)\n",
    "    return np.array([[TP, FP], [FN, TN]])\n",
    "\n",
    "\n",
    "C = matriz_confusion(Y[test_idx], modelo(res.x, X_std[test_idx]) > 0.5)\n",
    "display(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la próxima unidad veremos formas más sofisticadas para evaluar modelos de clasifición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
