{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T22:07:27.240767Z",
     "start_time": "2020-07-29T22:07:26.389604Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "from sklearn import metrics, model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnóstico de cancer usando Regresor Logístico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considere el dataset de diagnóstico de cancer de mama de la Universidad de Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T22:07:27.289000Z",
     "start_time": "2020-07-29T22:07:27.243067Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cancer.csv', index_col=0)\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 0, 'B': 1})\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta actividad es entrenar un regresor logístico para clasificar las muestras como benignas y malignas en base a 30 atributos\n",
    "\n",
    "1. Primero se describe el modelo de forma teórica y se muestra el estimador de máxima verosimilutd\n",
    "1. Luego se implementa el modelo usando `numpy` y se optimize usando `scipy.optimize`\n",
    "1. El desempeño del modelo se mide usando `sklearn.metrics`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de regresión logística\n",
    "\n",
    "Sea un problema de clasificación de $M$ observaciones $\\{\\vec x_i, y_i\\}$ donde $\\vec x_i \\in \\mathbb{R}^D$ (D atributos) y $y_i \\in \\{0, 1\\}$ (clasificación binaria)\n",
    "\n",
    "Se propone el siguiente modelo conocido como **regresión logística** con $D+1$ parámetros\n",
    "\n",
    "$$\n",
    "y_i \\approx \\mathcal{S} \\left(\\theta_0 + \\sum_{j=1}^D \\theta_j x_{ij}\\right),\n",
    "$$\n",
    "\n",
    "donde $\\mathcal{S}(z) = \\frac{1}{1+\\exp(-z)}$\n",
    "\n",
    "> En lo que sigue asumiremos que las observaciones son iid y que $y_i$ sigue una distribución de Bernoulli \n",
    "\n",
    "\n",
    "Actividades:\n",
    "1. Obtenga una expresión simplificada para la función de costo: máximo logaritmo de la verosimilitud \n",
    "1. Obtenga una expresión simplificada para el gradiente de la función de costo con respecto a $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verosimilitud\n",
    "\n",
    "En lo que sigue llamaremos\n",
    "\n",
    "$$\n",
    "f_\\theta(x_i) = \\theta_0 + \\sum_{j=1}^D \\theta_j x_{ij}\n",
    "$$\n",
    "\n",
    "\n",
    "Con el supuesto iid tenemos que la verosimilitud conjunta es igual a la multiplicación de las marginales\n",
    "\n",
    "Si además aplicamos logaritmo tenemos\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\theta) = \\sum_{i=1}^M \\log p(y_i | \\mathcal{S} (f_\\theta(x_i)) )\n",
    "$$\n",
    "\n",
    "Luego si asumimos que la probabilidad de $y_i$ dado el modelo $\\mathcal{S} (f(x_i, \\theta))$ es Bernoulli entonces\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(y_i | \\mathcal{S} (f_\\theta(x_i) ) &= \\log \\mathcal{S} (f_\\theta(x_i))^{y_i} (1-\\mathcal{S} (f_\\theta(x_i)) )^{1-y_i} \\nonumber \\\\\n",
    "&= y_i \\log (\\mathcal{S} (f_\\theta(x_i)) ) + (1-y_i) \\log(1-\\mathcal{S} (f_\\theta(x_i)) ) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "y todo junto\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\theta) = \\sum_{i=1}^M y_i \\log (\\mathcal{S} (f_\\theta(x_i)) ) + (1-y_i) \\log(1-\\mathcal{S} (f_\\theta(x_i)) )\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivada de la verosimilitud\n",
    "\n",
    "La derivada de la verosimilitud con respecto a $\\theta_j$ es\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\theta_j} \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^M \\left ( \\frac{y_i}{\\mathcal{S} (f_\\theta(x_i))} - \\frac{1-y_i}{1-\\mathcal{S} (f_\\theta(x_i))} \\right) \\frac{d \\mathcal{S} (f(x_i, \\theta))}{d f_\\theta(x_i)} \\frac{d f_\\theta(x_i)}{d \\theta_j} \n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$\n",
    "\\frac{d \\mathcal{S}(z)}{dz} =  \\mathcal{S}(z) ( 1 - \\mathcal{S}(z))\n",
    "$$\n",
    "\n",
    "entonces\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{d\\theta_j} \\log \\mathcal{L}(\\theta) &= \\sum_{i=1}^M \\left[ y_i - y_i\\mathcal{S} (f_\\theta(x_i)) - \\mathcal{S} (f_\\theta(x_i))  + y_i \\mathcal{S} (f_\\theta(x_i)) \\right] \\frac{d f_\\theta(x_i)}{d \\theta_j} \\nonumber \\\\\n",
    "&= \\sum_{i=1}^M \\left[ y_i  - \\mathcal{S} (f_\\theta(x_i)) \\right] \\frac{d f_\\theta(x_i)}{d \\theta_j} \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "La derivada con respecto a los parámetros es \n",
    "\n",
    "$$\n",
    "\\frac{d f(x_i, \\theta)}{d \\theta_0} =  1\n",
    "$$\n",
    "\n",
    "y para $j>0$\n",
    "$$\n",
    "\\frac{d f(x_i, \\theta)}{d \\theta_j} =  x_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación del modelo en `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T22:07:27.322849Z",
     "start_time": "2020-07-29T22:07:27.291008Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modelo y optimización\n",
    "def sigmoide(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def modelo(theta, X):\n",
    "    f = theta[0] + np.sum(theta[1:]*X, axis=1)     \n",
    "    return sigmoide(f), f\n",
    "\n",
    "# Funciones que usaremos con scipy.optimize.minimize\n",
    "\n",
    "def neglogverosimilitud(theta, *args):\n",
    "    X, y = args\n",
    "    s, f = modelo(theta, X)\n",
    "    # Le agregamos un signo menos ya que quiero minimizar en lugar de maximizar\n",
    "    return -np.sum(-np.logaddexp(0, -f) - (1-y)*f, axis=0)\n",
    "    # La linea superior es una versión simplificada de:\n",
    "    # La función logaddexp es más estable numericamente\n",
    "    #return -np.sum(y*np.log(s+1e-10) + (1-y)*np.log(1-s+1e-10) )\n",
    "\n",
    "def grad_neglogverosimilitud(theta, *args):\n",
    "    X, y = args\n",
    "    N = len(y)\n",
    "    s, f = modelo(theta, X)\n",
    "    X1 = np.concatenate((np.ones(shape=(N, 1)), X), axis=1)\n",
    "    e = (y - s)\n",
    "    return -np.sum(e[:, np.newaxis]*X1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo\n",
    "\n",
    "En primer lugar separaremos la data en conjuntos de entrenamiento y prueba usando `sklearn.model_selection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T22:07:27.436894Z",
     "start_time": "2020-07-29T22:07:27.324632Z"
    }
   },
   "outputs": [],
   "source": [
    "# Crear base de datos\n",
    "y = df[\"diagnosis\"].values\n",
    "X = df.drop(columns=[\"diagnosis\"]).values\n",
    "\n",
    "# Estandarizar\n",
    "X = (X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "# Partición estratificada\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=1, train_size=0.75)\n",
    "train_idx, test_idx = next(sss.split(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar el modelo usaremos la función `minimize` de `scipy.optimize` \n",
    "\n",
    "Usaremos `BFGS`, un método quasi-Newton que usa información de las primeras derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T22:07:27.572712Z",
     "start_time": "2020-07-29T22:07:27.439574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usaremos un callback para guardar el mejor modelo de validación\n",
    "def eval_model(theta):  \n",
    "    global best_theta, best_logl\n",
    "    logltrain = neglogverosimilitud(theta, *(X[train_idx, :], y[train_idx]))\n",
    "    logltest = neglogverosimilitud(theta, *(X[test_idx, :], y[test_idx]))\n",
    "    print(\"Train: %0.4f, Test: %0.4f\" %(logltrain, logltest))   \n",
    "    if logltest < best_logl: # Guardar el mejor modelo de test\n",
    "        best_theta = theta\n",
    "        best_logl = logltest\n",
    "\n",
    "# Valor inicial de theta\n",
    "theta = np.random.randn(1+X.shape[1])\n",
    "# Mejor valor de theta\n",
    "best_theta = np.zeros(1+X.shape[1])\n",
    "# Mejor valor de la verosimilitud\n",
    "best_logl = np.inf\n",
    "res = scipy.optimize.minimize(fun=neglogverosimilitud, x0=theta, \n",
    "                              method='BFGS', jac=grad_neglogverosimilitud, \n",
    "                              args=(X[train_idx, :], y[train_idx]),\n",
    "                              callback=eval_model, tol=1e-1)\n",
    "\n",
    "print(res.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de modelo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida de este clasificador es un número en el rango $[0, 1]$\n",
    "\n",
    "Para tomar un decisión binaria se debe seleccionar un umbral $\\mathcal{T}$ tal que\n",
    "\n",
    "$$\n",
    "d_i = \n",
    "\\begin{cases} \n",
    "\\text{maligno} (0), & \\text{si } p(y_i|\\theta, \\vec x_i) < \\mathcal{T} \\\\ \n",
    "\\text{benigno} (1), & \\text{si } p(y_i|\\theta, \\vec x_i) \\geq \\mathcal{T}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Una vez seleccionado el umbral se puede contar la cantidad de \n",
    "- True positives (TP): Tumores benignos clasificados como benignos\n",
    "- True negative (TN): Tumores malignos clasificados como malignos\n",
    "- False positives (FP): Tumores malignos clasificados como benignos\n",
    "- False negative (FN): Tumores  benignos clasificados como malignos\n",
    "\n",
    "Estas métricas son la base para construir una \"tabla de confusión\" para el clasificador\n",
    "\n",
    "|Clasificado como/En realidad era|Positivo|Negativo|\n",
    "|---|---|---|\n",
    "|Positivo:|TP | FP |\n",
    "|Negativo:| FN | TN |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T22:07:27.864382Z",
     "start_time": "2020-07-29T22:07:27.574463Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(\"Parametros\", best_theta)\n",
    "haty_train, _ = modelo(best_theta, X[train_idx, :])\n",
    "haty_test, _ = modelo(best_theta, X[test_idx, :])\n",
    "\n",
    "# Encontrando el umbral\n",
    "T = np.linspace(0, 1, num=100)\n",
    "f1 = np.array([metrics.f1_score(y[test_idx], haty_test > t) for t in T])\n",
    "\n",
    "# Resultados de clasificación\n",
    "display(\"Los resultados de clasificación en entrenamiento\")\n",
    "print(metrics.classification_report(y[train_idx], haty_train > T[np.argmax(f1)]))\n",
    "display(\"Los resultados de clasificación en validación\")\n",
    "print(metrics.classification_report(y[test_idx], haty_test > T[np.argmax(f1)]))\n",
    "\n",
    "display(\"Matriz de confusión\")\n",
    "cm = metrics.confusion_matrix(y[test_idx], haty_test > T[np.argmax(f1)])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        print(\"Era {0} y predije {1}: {2}\".format(i, j, cm[i, j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
