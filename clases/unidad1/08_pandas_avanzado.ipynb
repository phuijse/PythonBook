{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:42.030700Z",
     "start_time": "2020-06-13T18:05:42.016723Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo, Markdown, SVG\n",
    "from functools import partial\n",
    "YouTubeVideo_formato = partial(YouTubeVideo, modestbranding=1, disablekb=0,\n",
    "                               width=640, height=360, autoplay=0, rel=0, showinfo=0)\n",
    "\n",
    "display(Markdown(filename='../preamble.md'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración y manipulación de datos con [*pandas*](https://pandas.pydata.org/) <a class=\"tocSkip\"></a>\n",
    "\n",
    "En la clase anterior aprendimos a operar DataFrames construidos a partir de estructuras de datos de Python\n",
    "\n",
    "> Pero el caso más general de uso de pandas es la exploración y manipulación de una base de datos tabular que podría existir como un fichero en nuestro sistema o en un servidor remoto\n",
    "\n",
    "En este clase veremos \n",
    "- como crear DataFrames a partir de distintas fuentes de datos\n",
    "- funciones más avanzadas de manipulación de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:42.621323Z",
     "start_time": "2020-06-13T18:05:42.032886Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "display(\"Versión de pandas \"+pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar datos tabulares a partir de archivos CSV \n",
    "\n",
    "Un archivo  **CSV** (Comma-Separated Values) es \n",
    "\n",
    "> una tabla en formato texto plano cuyas columnas están separadas por comas (u otro delimitador)\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Base de datos \"Dow Jones Index\" en formato CSV del [repositorio UCI](https://archive.ics.uci.edu/ml/datasets/Dow+Jones+Index)\n",
    "\n",
    "La descargamos y observamos las 5 primeras lineas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:43.833142Z",
     "start_time": "2020-06-13T18:05:42.623608Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -c https://archive.ics.uci.edu/ml/machine-learning-databases/00312/dow_jones_index.zip\n",
    "unzip -o dow_jones_index.zip\n",
    "head -5 dow_jones_index.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Dow Jones es un índice bursatil muy utilizado ya que refleja el comportamiento del mercado accionario norteamericano\n",
    "\n",
    "Podemos ver que cada fila tiene un \n",
    "- identificador textual de la acción: `AA`\n",
    "- una fecha de observación: `1/7/2011`\n",
    "- un precio de apertura, máximo, mínimo y cierre para la fecha: `$15.82, $16.72, $15.78, $16.42`\n",
    "- entre otros\n",
    "\n",
    "Podemos notar algunos aspectos típicos de los archivos CSV\n",
    "- La primera fila del archivo CSV contiene el *header*, es decir los nombres de las columnas\n",
    "- Las columnas son de tipos distintos \n",
    "\n",
    "¿Qué tipos puedes identificar en el ejemplo anterior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función `pd.read_csv` y atributo `to_csv()`\n",
    "\n",
    "Leer un archivo CSV como DataFrame es directo usando la función (sólo algunos argumentos resaltados)\n",
    "\n",
    "```python\n",
    "pd.read_csv(\n",
    "    filepath_or_buffer: Union[str, pathlib.Path, IO[~AnyStr]], # path completo al archivo CSV\n",
    "    sep=',', # String o expresión regular que se usará para delimitar las columnas\n",
    "    header='infer', # Puede ser un int (fila donde está el header) o una lista de de int's\n",
    "    names=None, # Lista de strings con nombres de columnas (útil si el CSV no tiene header)\n",
    "    index_col=None, # La columna que se usará como header\n",
    "    usecols=None, # Lista: subconjunto de columnas que se desean importar (por defecto se importan todas)\n",
    "    converters=None, # Se explica en detalle más adelante junto a otros argumentos de parsing\n",
    "    parse_dates=None, # Se explica en detalle más adelante junto a otros argumentos de fecha\n",
    "    ...\n",
    "    )\n",
    "```\n",
    "\n",
    "También podemos crear un archivo CSV desde un DataFrame usando\n",
    "\n",
    "```python\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"mis_datos.csv\")\n",
    "```\n",
    "\n",
    "- Esto  crea un archivo `mis_data.csv` en el directorio actual\n",
    "- Por defecto guardara las nombres de columna como un header y usará \",\" como delimitador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis sintático o *parsing*\n",
    "\n",
    "Los textos de un archivo CSV pueden representar \n",
    "- valores numéricos continuos\n",
    "- valores numéricos discretos\n",
    "- fechas\n",
    "- coordenadas \n",
    "- moneda\n",
    "- direcciones\n",
    "- etiquetas de texto\n",
    "- entre **muchos** otros\n",
    "\n",
    "Los programas que importan un archivo CSV deben interpretar estos valores y convertirlos al formato más adecuado, por ejemplo\n",
    "- flotante\n",
    "- entero\n",
    "- booleano\n",
    "- string\n",
    "\n",
    "> Se llama ***parser* o analizador sintático** al programa que analiza los textos y luego \n",
    "- filtra y/o completa los textos invalidos\n",
    "- convierte los datos a un formato estándar\n",
    "\n",
    "Pandas hace este proceso de forma automática y podemos hacer algunos ajustes usando ciertos argumentos disponibles en `read_csv`, por ejemplo \n",
    "\n",
    "\n",
    "```python\n",
    "pd.read_csv(\n",
    "    ...\n",
    "    dtypes=None # Diccionario donde la llave es el nombre de la columna y el valor el tipo requerido\n",
    "    na_values=None, # String o lista de strings con valores que serán reconocidos como NaN\n",
    "    decimal='.', # String que se usará para reconocer el punto decimal\n",
    "    comment=None, # String, todos las lineas que empiezen con este string serán ignoradas\n",
    "    converters=None # Se explica a continuación\n",
    "    ...\n",
    "    )\n",
    "```\n",
    "\n",
    "> Si las opciones automáticas no son suficientes se puede hacer *parsing* en base a reglas manualmente creadas\n",
    "\n",
    "El argumento `converters` recibe un diccionario con \"reglas de parseo\" como sigue\n",
    "\n",
    "```python\n",
    "    {'nombre de la columna 1': funcion_parseadora1, \n",
    "     'nombre de la columna 2': funcion_parseadora2,\n",
    "     ...\n",
    "    }\n",
    "```\n",
    "\n",
    "Donde `funcion_parseadoraX` puede ser una función explicita o anómina (lambda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**\n",
    "\n",
    "Los datos de la columna de precio de apertura (open) de \"dow_jones_index.data\" están formateados como \n",
    "\n",
    "`'$15.84'`\n",
    "\n",
    "Es decir un signo dolar seguido de un número real con punto decimal\n",
    "\n",
    "Para *parsear* este valor debemos escribir una función que \n",
    "1. Elimine el signo dolar del string\n",
    "1. Convierta el resto del string en flotante\n",
    "\n",
    "Luego creamos el diccionario que entregaremos al argumento `converters`\n",
    "\n",
    "```python\n",
    "    def remove_dollar(text):\n",
    "        # return float(x[1:]) # Elimina el primer caracter\n",
    "        return float(x.strip(\"$\")) # Elimina todos los $ del string\n",
    "        \n",
    "    parser = {'open': remove_dollar}\n",
    "```\n",
    "\n",
    "o con una función anónima\n",
    "\n",
    "```python\n",
    "    parser = {'open': lambda x: float(x.strip(\"$\"))}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación Fechas\n",
    "\n",
    "Un dato textual muy usual en datos tabulares y series de tiempo son las fechas. Pero el formato de fecha puede variar considerablemente entre distintas bases de datos\n",
    "\n",
    "> Pandas tiene un tipo denominado `Timestamp` el cual se puede construir con la función `pd.to_datetime()` a partir de un string \n",
    "\n",
    "Pandas identifica automaticamente fechas y horas en distintos formatos\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "```python\n",
    ">>> pd.to_datetime(\"1/5/2018\") # Formato norteamericano Mes/Día/Año \n",
    "Timestamp('2018-01-05 00:00:00')\n",
    "\n",
    ">>> pd.to_datetime(\"May/1/2018\") # También se acepta un string para el mes\n",
    "Timestamp('2018-05-01 00:00:00')\n",
    "\n",
    ">>> pd.to_datetime(\"1st of May of 2018\") # También se puede usar una frase \"Día del Mes del Año\"\n",
    "Timestamp('2018-05-01 00:00:00')\n",
    "\n",
    ">>> pd.to_datetime(\"2018\") # Autocompletación por defecto para fechas incompletas\n",
    "Timestamp('2018-01-01 00:00:00')\n",
    "\n",
    ">>> pd.to_datetime(\"14:45\") # Si usamos sólo la hora se usa la fecha actual\n",
    "Timestamp('2020-06-12 14:45:00')\n",
    "\n",
    ">>> pd.to_datetime(\"May/1/2018 14:45\") # Timestamp completo\n",
    "Timestamp('2018-05-01 14:45:00')\n",
    "```\n",
    "\n",
    "Podemos controlar el parseo de fechas en `read_csv` con los argumentos\n",
    "\n",
    "```python\n",
    "pd.read_csv(\n",
    "    ...\n",
    "    parse_dates=False # Booleano o lista con las columnas que deben ser interpretadas como fechas\n",
    "    infer_datetime_format=False, # Inferir una función parseadora de forma automática\n",
    "    dayfirst=False, # Formato día/mes/año o mes/día/año\n",
    "    date_parser=None, # Función provista por el usuario que toma un string y retorna TimeStamp\n",
    "    ...\n",
    "    )\n",
    "```\n",
    "\n",
    "Las fechas/tiempos en formato *timestamp* pueden usarse como índices\n",
    "\n",
    "> Esto nos permite recuperar rapidamente todos los eventos dentro de un intervalo de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:**\n",
    "\n",
    "1. Lea el archivo `dow_jones_index.data` con `pd.read_csv` con las opciones por defecto y estudie el DataFrame resultante\n",
    "1. Corrija incrementalmente:\n",
    "    1. Use un conversor para todas las columnas numéricas que empiezan con `$`\n",
    "    1. Use el argumento `parse_dates` para parsear la columna date como un `Timestamp` de pandas\n",
    "\n",
    "En cada paso verifique el tipo de las columnas con el atributo `dtypes`\n",
    "\n",
    "Con su tabla adecuadamente formateada: \n",
    "> Retorne los valores de apertura (open), cierre (close), mínimo (low) y máximo (high) para las acciones de Alcoa Corp. (AA) entre el Marzo y Junio del 2011\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:43.841550Z",
     "start_time": "2020-06-13T18:05:43.836781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Completa tu solución acá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:45.376384Z",
     "start_time": "2020-06-13T18:05:43.844935Z"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo_formato('zrBQQQZeGXw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar datos tabulares a partir de archivos excel\n",
    "\n",
    "Muchas empresas e instituciones manejan sus datos como hojas de cálculo o *spreadsheets* construidas usando software como Microsoft Excel, Openoffice/Libreoffice calc o Google spreadsheets\n",
    "\n",
    "Pandas permite importar como dataframe hojas de cálculo en formatos `xls, xlsx, xlsm, xlsb, and odf` usando  la función [`read_excel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html)\n",
    "\n",
    "   \n",
    "Muchos de los argumentos de `read_csv` están disponibles en `read_excel`, los nuevos argumentos son\n",
    "\n",
    "```python\n",
    "pd.read_excel(io, # string o path a la hoja de cálculo\n",
    "              sheet_name=0, # Entero, string o lista, especifica la(s) hoja (s) que vamos a importar\n",
    "              ...\n",
    "             )\n",
    "```\n",
    "\n",
    "**Nota** \n",
    "\n",
    "Esta función depende adicionalmente de la librería de Python [xlrd](https://xlrd.readthedocs.io/en/latest/) que se puede instalar con `conda`:\n",
    "\n",
    "    conda install xlrd\n",
    "    \n",
    "    \n",
    "**Ejemplo**\n",
    "\n",
    "Consideremos los siguientes datos del censo chileno de 2017 en formato Excel de donde importaremos datos de vivienda por comuna\n",
    "\n",
    "Esto corresponde a la segunda hoja (`sheet_name=1`) y en particular las columnas de 1 a 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:45.617288Z",
     "start_time": "2020-06-13T18:05:45.382190Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget -c http://www.censo2017.cl/wp-content/uploads/2017/12/Cantidad-de-Viviendas-por-Tipo.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:45.779664Z",
     "start_time": "2020-06-13T18:05:45.619879Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Cantidad-de-Viviendas-por-Tipo.xlsx\", \n",
    "                   sheet_name=1, # Importamos la segunda hoja (vivienda)\n",
    "                   usecols=list(range(1, 20)), # Importamos las columnas 1 a 20\n",
    "                   header=1, # El header está en la segunda fila\n",
    "                   skiprows=[2], # Eliminamos la fila 2 ya que es invalida\n",
    "                   index_col='ORDEN' # Usaremos el orden de las comunas como índice\n",
    "                  ).dropna() # Eliminamos las filas con NaN\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulando índices y multi-índices\n",
    "\n",
    "Estudiando la tabla anterior notamos que tiene una estructura jerárquica\n",
    "\n",
    "> REGION, PROVINCIA, COMUNA\n",
    "\n",
    "Podemos representar este tipo de estructuras en pandas usando `MultiIndex` \n",
    "\n",
    "Para asignar un índice a un DataFrame que ya está creado podemos usar el atributo\n",
    "\n",
    "```python\n",
    "df.set_index(keys, # Una etiqueta o una lista de etiquetas que serán los nuevos índices\n",
    "             drop=True, # Eliminar las columnas que pasarán a ser índices\n",
    "             inplace=False, # Retornar un nuevo dataframe o modificar df\n",
    "             ...\n",
    "            )\n",
    "```\n",
    "\n",
    "- Si keys es una etiqueta crearemos un índice regular\n",
    "- Si keys es una lista crearemos un `MultiIndex`\n",
    "\n",
    "```python\n",
    ">>> df.set_index([\"NOMBRE REGIÓN\", \"NOMBRE PROVINCIA\"], inplace=True) \n",
    ">>> # Se crea un MultiIndex de dos niveles de jerarquía en df\n",
    ">>> df.index\n",
    "MultiIndex([(                  'ARICA Y PARINACOTA',             'ARICA'),\n",
    "            (                  'ARICA Y PARINACOTA',             'ARICA'),\n",
    "            (                  'ARICA Y PARINACOTA',        'PARINACOTA'),\n",
    "            ...\n",
    "```\n",
    "\n",
    "Si queremos que nuestro índice o multi-índice vuelva a convertirse en columna podemos usar el atributo\n",
    "\n",
    "```python\n",
    "df.reset_index(level = None, # Permite especificar cuantos niveles de índices se removeran\n",
    "               drop: bool = False, # Si los índices se deben eliminar o agregar como columnas\n",
    "               inplace: bool = False,  # Retornar un nuevo dataframe o modificar df\n",
    "               ...\n",
    "               )\n",
    "```\n",
    "\n",
    "Para recuperar un elemento de un DataFrame con `MultiIndex` podemos indexar usando una tupla especificando cada uno de los niveles de índices\n",
    "\n",
    "```python\n",
    ">>> df.loc[(\"LOS RÍOS\", \"VALDIVIA\", \"VALDIVIA\")] # Nos retorna una fila\n",
    "```\n",
    "\n",
    "Si queremos recuperar un conjunto de elementos podemos usar\n",
    "\n",
    "```python\n",
    ">>> df.loc[(\"LOS RÍOS\", \"VALDIVIA\")] # Retorna todas las comunas de la provincia de Valdivia\n",
    ">>> df.loc[(\"LOS RÍOS\")] # Retorna todas las comunas de región de los rios\n",
    "```\n",
    "\n",
    "Si queremos usar fancy indexing o slicing lo más recomendable es usar el objeto [`IndexSlice`](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.IndexSlice.html) para generar slices\n",
    "\n",
    "```python\n",
    ">>> idx = pd.IndexSlice\n",
    ">>> df.loc[idx[:, :, [\"VALDIVIA\", \"OSORNO\"]], :] # Las comunas de Valdivia y Osorno\n",
    ">>> df.loc[idx[:, [\"LLANQUIHUE\", \"PALENA\"], : ], :] # Las comunas de las provincias de Llanquihue y Palena\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "- Asigne un MultiIndex al DataFrame de datos de vivienda del censo\n",
    "    - Use como primer nivel la etiqueta \"NOMBRE REGION\"\n",
    "    - Use como segundo nivel la etiqueta \"NOMBRE PROVINCIA\" \n",
    "    - Use como tercer nivel la etiqueta \"NOMBRE COMUNA\"\n",
    "- Use `loc` para seleccionar\n",
    "    - las comunas de la región de \"LOS RÍOS\"\n",
    "    - las comunas de las provincias de \"RANCO\" y \"OSORNO\"\n",
    "    - las comunas \"VALDIVIA\" y \"FRUTILLAR\"\n",
    "- Selecciona las comunas de la provincia de \"VALDIVIA\" y usa una reducción suma para encontrar el número de viviendas totales de cada tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:45.783752Z",
     "start_time": "2020-06-13T18:05:45.781259Z"
    }
   },
   "outputs": [],
   "source": [
    "# Completa tu solución acá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:47.549517Z",
     "start_time": "2020-06-13T18:05:45.785577Z"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo_formato('bWjB4089EbA')"
   ]
  },
  {
   "attachments": {
    "groupby.svg": {
     "image/svg+xml": [
      "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"
     ]
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupamiento en DataFrames usando Groupby\n",
    "\n",
    "Digamos que queremos obtener los totales de todos los tipos de vivienda a nivel de provincia\n",
    "\n",
    "Si asignamos \"NOMBRE PROVINCIA\" como índice podríamos usar\n",
    "\n",
    "```python\n",
    "result = []\n",
    "for provincia in df.index.unique():    \n",
    "    sub_df = df.loc[provincia, col_mask]\n",
    "    if sub_df.ndim>1:    \n",
    "        result.append(df.loc[provincia, col_mask].sum())\n",
    "    else: # No hacer reducción suma si la provincia tiene una sola comuna\n",
    "        result.append(df.loc[provincia, col_mask])\n",
    "pd.DataFrame(result, columns=col_mask, index=df.index.unique())\n",
    "```\n",
    "\n",
    "que obtiene el resultado que queremos pero es bastante engorroso\n",
    "\n",
    "> Esta operación condicionada por una llave se conoce como **agrupamiento**. A continuación veremos como hacer agrupamiento de forma simple y eficiente con pandas\n",
    "\n",
    "Los DataFrame tienen un atributo llamado [`groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) \n",
    "\n",
    "Este atributo permite implementar una [secuencia](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) de tres pasos como la que sigue\n",
    "\n",
    "![groupby.svg](attachment:groupby.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde\n",
    "- *split*: Divide los datos según una **llave**\n",
    "- *apply*: Realiza una función sobre cada grupo\n",
    "- *combine*: Mezcla el resultado en un nuevo dataframe donde la **llave** se convierte en el índice\n",
    "\n",
    "Los argumentos básicos de `groupby` son\n",
    "\n",
    "```python\n",
    "df.groupby(by=None, # Columna o lista de columnas con se hace el split\n",
    "           axis=0, # Dividir en filas (0) o en columnas (1)\n",
    "           as_index: bool = True, # Retornar las etiquetas de cada grupo como índice\n",
    "           sort: bool = True, # Retornar las etiquetas de grupo ordenadas alfabeticamente\n",
    "           ...\n",
    "          )    \n",
    "```\n",
    "\n",
    "Groupby actua como un iterador\n",
    "\n",
    "```python\n",
    "for (region, sub_df) in df.groupby('NOMBRE REGIÓN'):\n",
    "    display(region, # La etiqueta\n",
    "            sub_df  # El dataframe con las filas que comparten esa etiqueta\n",
    "           )\n",
    "```\n",
    "\n",
    "La función que se ejecuta a cada grupo en el paso *apply* es un atributo de `groupby`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupamiento y reducción\n",
    "\n",
    "El atributo se llama `aggregate` y la sintaxis básica es\n",
    "\n",
    "```python\n",
    "    df.groupby(by=llave).aggregate(funcion1) \n",
    "    # Se aplica funcion1 a todas las columnas\n",
    "    df.groupby(by=llave).aggregate([funcion1, funcion2, ...]) \n",
    "    # Se aplican todas las funciones a todos las columnas\n",
    "    df.groupby(by=llave).aggregate({columna1: funcion1, columna2: funcion2}) \n",
    "    # Se aplican funcion1 a columna1 y funcion2 a columna2, respectivamente\n",
    "```\n",
    "\n",
    "Las funciones debe entregar un número por cada columna del grupo\n",
    "\n",
    "Se usa para hacer resumenes, por ejemplo sumas, promedios o varianzas\n",
    "\n",
    "**Ejemplo: Agrupamiento con reducción**\n",
    "\n",
    "Podemos encontrar los totales por provincia en una sola linea usando\n",
    "\n",
    "```python\n",
    ">>> df.groupby(by=\"NOMBRE PROVINCIA\", sort=False).aggregate(np.sum)\n",
    ">>> # df.groupby(by=\"NOMBRE PROVINCIA\", sort=False).sum() # alternativa\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Agrupamiento y filtro\n",
    "\n",
    "El atributo se llama `filter` y la sintaxis básica es\n",
    "\n",
    "```python\n",
    "    df.groupby(by=llave).filter(funcion)\n",
    "```\n",
    "\n",
    "La función debe retornar `True` o `False`\n",
    "\n",
    "Se recupera un nuevo DataFrame con todos los grupos que pasaron el filtro\n",
    "\n",
    "Se usa para eliminar grupos de filas (drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupamiento y transformación\n",
    "\n",
    "El atributo se llama `transform` y la sintaxis básica es\n",
    "\n",
    "```python\n",
    "    df.groupby(by=llave).transform(funcion)\n",
    "```\n",
    "\n",
    "La función \n",
    "- debe retornar un dataframe con la misma dimensión y tamaño que el original\n",
    "- se aplica columna a columna\n",
    "- puede ser explicita o anónima\n",
    "\n",
    "Se puede usar para reescalar o normalizar los valores de las columnas a nivel de grupo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "Considere las columnas de \"viviendas particulares ocupadas con moradores presentes\" ($V_1$) y \"viviendas particulares ocupadas con moradores ausentes\" ($V_2$)\n",
    "\n",
    "1. Cree un MultiIndex equivalente al del ejercicio anterior\n",
    "1. Realice una reducción promedio y desviación estándar de cada región\n",
    "1. Use un filtro para encontrar las comunas \"más responsables\", es decir aquellas donde $\\frac{V_1}{V_1 + V_2} > 0.98$\n",
    "1. Use una transformación para describir las columnas $V_1$ y $V_2$ como porcentajes a nivel regional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:47.556699Z",
     "start_time": "2020-06-13T18:05:47.553178Z"
    }
   },
   "outputs": [],
   "source": [
    "# Completa tu solución acá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:48.787403Z",
     "start_time": "2020-06-13T18:05:47.559455Z"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo_formato('BopVrkZKNtw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de bases de datos SQL\n",
    "\n",
    "\n",
    "Pandas es capaz de conectar y hacer consultas en lenguaje SQL a una base de datos externa y retornar el resultado como un DataFrame usando la función [`read_sql_query`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql_query.html) \n",
    "\n",
    "```python\n",
    "pd.read_sql_query(sql, # Consulta SQL en formato string\n",
    "                  con, # dirección a la base de datos o objeto de conexión\n",
    "                  index_col=None, # Selecciona la columna que actuara como índice del DataFrame\n",
    "                  parse_dates=None, # Igual que read_csv y read_excel\n",
    "                  ...\n",
    "                 )\n",
    "```\n",
    "\n",
    "También se puede usar el atributo\n",
    "\n",
    "```python\n",
    "df.to_sql(name, # string: el nombre de la tabla\n",
    "          con, # Engine con conexión\n",
    "          if_exists: str = 'fail', # Que hacer si la tabla ya existe: fail, replace, append\n",
    "          index: bool = True, # Escribir el índice del dataframe como columna\n",
    "          ...\n",
    "         )\n",
    "```\n",
    "\n",
    "**Qué es SQL?**\n",
    "\n",
    "Structured Query Languaje (SQL) es un lenguaje estándar ampliamente usado para consultar, crear, modificar y eliminar bases de datos relacionales. \n",
    "\n",
    "**Qué es una base de datos relacional?**\n",
    "\n",
    "Es un tipo de base de datos organizada como múltiples tablas. Por ejemplo\n",
    "\n",
    "\n",
    "|id_cliente | nombre | apellido |\n",
    "|----|----|----|\n",
    "|1| Pablo | Huijse |\n",
    "|2| Luis | Alvarez |\n",
    "|3| Cristobal | Navarro |\n",
    "|  | CLIENTES |  |\n",
    "\n",
    "|id_orden | platanos | manzanas | id_cliente |\n",
    "|----|----|----| ---- |\n",
    "|1| 0 | 5 | 1 |\n",
    "|2| 2 | 2 | 3 |\n",
    "|3| 3 | 1 | 1 |\n",
    "|  | ORDENES |  | | \n",
    "\n",
    "- Las filas se llaman entidades y las columnas atributos\n",
    "- Cada tabla tiene una lalve primaria: id_orden e id_cliente\n",
    "- La tabla ORDENES **está relacionada** a la tabla CLIENTES con la llave foranea: id_clientes\n",
    "- Las tablas no pueden tener el mismo nombre \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Donde corre la base de datos relacional?**\n",
    "\n",
    "La base de datos relacional corre en un sistema de manejo \n",
    "\n",
    "Algunos ejemplos populares son MySQL, PostgreSQL y SQLite3\n",
    "\n",
    "\n",
    "**Ejemplo básico de una consulta SQL**\n",
    "\n",
    "SQL es un lenguaje de alto nivel. Algunos comandos comunes son\n",
    "\n",
    "- `SELECT`: recuperar un subconjunto de la tabla\n",
    "- `INSERT`: insertar datos en una tabla\n",
    "- `UPDATE`: actualizar datos en una tabla\n",
    "- `DELETE`: eliminar datos de la tabla\n",
    "\n",
    "La tabla que se quiere manipular se selecciona con el keyword `FROM`\n",
    "\n",
    "Se agregan condiciones usando el keyword `WHERE`\n",
    "\n",
    "Se puede usar `*` como alias para \"todas las columnas\"\n",
    "\n",
    "Por ejemplo\n",
    "```SQL\n",
    "    SELECT A, B, C FROM mi_tabla WHERE D > 1\n",
    "```\n",
    "\n",
    "Esto recupera las valores de las columnas A, B y C que tegan un valor de la columna D mayor que 1 a partir de la tabla \"mi_tabla\" \n",
    "\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "Escriba y prueba una consulta SQL que recupere un DataFrame con las columnas \"Viviendas Particulares Ocupadas con Moradores Presentes\" y \"NOMBRE COMUNA\" para la provincia de Valdivia\n",
    "\n",
    "Indicación: Cuando escriba su consulta encierre los nombres de las columnas con paréntesis cuadrados. Esto es necesario cuando los nombres contienen espacios en blanco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:49.052005Z",
     "start_time": "2020-06-13T18:05:48.790059Z"
    }
   },
   "outputs": [],
   "source": [
    "#Primero vamos a crear una base de datos e insertar un DataFrame como tabla\n",
    "\n",
    "import sqlite3  # SQLite3 es parte de la librería estándar de Python\n",
    "\n",
    "# Creamos una base de datos persistente\n",
    "with sqlite3.connect('censo.db') as conn:\n",
    "\n",
    "    df.to_sql(\"censo_viviendas\", # Insertamos una tabla llamada censo_viviendos\n",
    "              conn, # Usamos el objeto conexión que acabos de crear\n",
    "              if_exists='replace', \n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:49.061456Z",
     "start_time": "2020-06-13T18:05:49.056375Z"
    }
   },
   "outputs": [],
   "source": [
    "# Escribe tu solución aquí\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:50.499149Z",
     "start_time": "2020-06-13T18:05:49.065443Z"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo_formato('VsoZ1oh8Az8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notas sobre SQLite**\n",
    "\n",
    "- sqlite permite conectar a una base de datos local: RAM, disco, o disco externo montado\n",
    "- sqlite no está diseñado para soportar múltiples usuarios conectados a una misma base de datos\n",
    "- Otras alternativas: [SQL Alchemy](https://www.sqlalchemy.org/), [PostgreSQL+Python](http://initd.org/psycopg/), [Peewee](http://docs.peewee-orm.com/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar y leer una tabla  otros formatos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "Podemos usar el atributo [`to_json`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_json.html) para convertir un dataframe a este formato\n",
    "\n",
    "```python\n",
    "df.to_json(\n",
    "    path_or_buf = None, # Ubicación en disco\n",
    "    orient = None, # Indica el formato del string JSON\n",
    "    ...\n",
    "    )\n",
    "```\n",
    "\n",
    "Por ejemplo\n",
    "\n",
    "```python\n",
    ">>> df.to_json(\"pandas.json\", orient='table')\n",
    "```\n",
    "\n",
    "crea un string pandas.json en el directorio actual\n",
    "\n",
    "Luego la función `read_json`\n",
    "\n",
    "```python\n",
    ">>> df = pd.read_json(\"pandas.json\", orient='table')\n",
    "```\n",
    "\n",
    "regenera el DataFrame que teniamos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5\n",
    "\n",
    "Podemos usar el atributo `to_hdf` para convertir nuestra tabla a formato HDF5\n",
    "\n",
    "```python\n",
    "df.to_hdf(path_or_buf, # Path completo con nombre de archivo\n",
    "          key: str, # Llave maestra del archivo\n",
    "          mode: str = 'a', # Agrega lineas a un archivo existente (a) o crea una archivo nuevo (w)\n",
    "          ...\n",
    "         )\n",
    "```\n",
    "\n",
    "Por ejemplo\n",
    "\n",
    "```python\n",
    ">>> df.to_hdf(\"pandas_hdf.h5\", key='excel', mode='w')\n",
    "```\n",
    "\n",
    "crea un archivo pandas_hdf.h5 en el directorio actual\n",
    "\n",
    "Para lectura podemos usar la función `read_hdf`\n",
    "\n",
    "```python\n",
    ">>> mi_tabla_recuperada = pd.read_hdf(\"pandas_hdf.h5\", key='/excel', mode='r')\n",
    "```\n",
    "\n",
    "OJO: Se requiere Pytables mayor a 3.5: https://github.com/PyTables/PyTables/issues/719\n",
    "\n",
    "Podamos usar las librerías `PyTables` o `h5py` para trabajar con el archivo hdf5 en Python\n",
    "\n",
    "```python\n",
    "import h5py\n",
    "with h5py.File(\"pandas_hdf.h5\", mode=\"r\") as f:\n",
    "    print(f[\"excel\"].keys())\n",
    "    print(f[\"excel\"]['block0_items'][()])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
