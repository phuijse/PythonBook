{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración y manipulación de datos con `pandas`\n",
    "\n",
    "En la clase anterior aprendimos a operar sobre `DataFrames` construidos a partir de estructuras de datos de Python (diccionario, numpy array)\n",
    "\n",
    "También podemos usar pandas para explorar y manipular datos tabulares que existen como fichero en nuestro sistema o en un servidor remoto\n",
    "\n",
    "En este clase veremos \n",
    "\n",
    "- como crear un `DataFrame` a partir de archivos CSV o excel\n",
    "- algunas funciones más avanzadas de `pandas`: `MultiIndex` y `groupBy` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:42.621323Z",
     "start_time": "2020-06-13T18:05:42.032886Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "from functools import partial\n",
    "YouTubeVideo_formato = partial(YouTubeVideo, modestbranding=1, disablekb=0,\n",
    "                               width=640, height=360, autoplay=0, rel=0, showinfo=0)\n",
    "\n",
    "display(\"Versión de pandas \"+pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar datos tabulares a partir de archivos CSV \n",
    "\n",
    "Un archivo  **CSV** (Comma-Separated Values) es una tabla en formato texto plano cuyas columnas están separadas por comas (u otro delimitador)\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Consideremos la base de datos [\"Dow Jones Index\"](https://archive.ics.uci.edu/ml/datasets/Dow+Jones+Index) del repositorio UCI, la cual se distribuye en formato CSV\n",
    "\n",
    "```{note}\n",
    "El Dow Jones es un índice bursatil muy utilizado ya que refleja el comportamiento del mercado accionario norteamericano\n",
    "```\n",
    "\n",
    "Descarguémo la base de datos y observemos las primeras cinco lineas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:43.833142Z",
     "start_time": "2020-06-13T18:05:42.623608Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -cq https://archive.ics.uci.edu/ml/machine-learning-databases/00312/dow_jones_index.zip\n",
    "unzip -o dow_jones_index.zip\n",
    "head -5 dow_jones_index.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del archivo CSV podemos ver que cada fila tiene un \n",
    "\n",
    "- identificador textual de la acción: `AA`\n",
    "- una fecha de observación: `1/7/2011`\n",
    "- un precio de apertura, máximo, mínimo y cierre para la fecha: `$15.82, $16.72, $15.78, $16.42`\n",
    "- entre otros\n",
    "\n",
    "También podemos notar algunos aspectos típicos de los archivos CSV\n",
    "\n",
    "- Las columnas están separadas por comas\n",
    "- La primera fila del archivo CSV contiene el *header*, es decir los nombres de las columnas\n",
    "- Las columnas son de tipos distintos: ¿Qué tipos puedes identificar en el ejemplo anterior?\n",
    "\n",
    "> A continuación veremos como importar y escribir un archivo CSV usando `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función `pd.read_csv`**\n",
    "\n",
    "Leer un archivo CSV como DataFrame es directo usando la función `read_csv`\n",
    "\n",
    "A continuación se resaltan los argumentos principales\n",
    "\n",
    "```python\n",
    "pd.read_csv(\n",
    "    filepath_or_buffer: Union[str, pathlib.Path, IO[~AnyStr]], # path completo al archivo CSV\n",
    "    sep=',', # String o expresión regular que se usará para delimitar las columnas\n",
    "    header='infer', # Puede ser un int (fila donde está el header) o una lista de de int's\n",
    "    names=None, # Lista de strings con nombres de columnas (útil si el CSV no tiene header)\n",
    "    index_col=None, # La columna que se usará como header\n",
    "    usecols=None, # Lista: subconjunto de columnas que se desean importar (por defecto se importan todas)\n",
    "    converters=None, # Se explica en detalle más adelante junto a otros argumentos de parsing\n",
    "    parse_dates=None, # Se explica en detalle más adelante junto a otros argumentos de fecha\n",
    "    ...\n",
    "    )\n",
    "```\n",
    "\n",
    "Más adelante veremos más argumentos y un ejemplo de uso\n",
    "\n",
    "**Atributo `to_csv()`**\n",
    "\n",
    "Podemos crear un archivo CSV desde un DataFrame usando el atributo `to_csv` como se muestra a continuación\n",
    "\n",
    "```python\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"mis_datos.csv\")\n",
    "```\n",
    "\n",
    "- Esto  crea un archivo `mis_data.csv` en el directorio actual\n",
    "- Por defecto guardara las nombres de columna como un header y usará \",\" como delimitador\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis sintático o *parsing*\n",
    "\n",
    "En general un archivo de texto plano podría contener\n",
    "\n",
    "- valores numéricos continuos\n",
    "- valores numéricos discretos\n",
    "- fechas\n",
    "- coordenadas \n",
    "- monedas\n",
    "- direcciones\n",
    "- etiquetas de texto\n",
    "- y un largo etcétera\n",
    "\n",
    "Los programas que leen e importan archivos de texto plano como CSV deben interpretar estos valores y convertirlos al formato más adecuado, por ejemplo\n",
    "\n",
    "- flotante\n",
    "- entero\n",
    "- booleano\n",
    "- string\n",
    "\n",
    "Se llama ***parser* o analizador sintático** al programa que analiza los textos y luego \n",
    "\n",
    "- filtra y/o completa los textos invalidos\n",
    "- convierte los datos a un formato estándar\n",
    "\n",
    "Pandas hace este proceso de forma automática y podemos hacer algunos ajustes usando los argumentos disponibles en `read_csv`\n",
    "\n",
    "Por ejemplo \n",
    "\n",
    "\n",
    "```python\n",
    "pd.read_csv(\n",
    "    ...\n",
    "    dtypes=None # Diccionario donde la llave es el nombre de la columna y el valor el tipo requerido\n",
    "    na_values=None, # String o lista de strings con valores que serán reconocidos como NaN\n",
    "    decimal='.', # String que se usará para reconocer el punto decimal\n",
    "    comment=None, # String, todos las lineas que empiezen con este string serán ignoradas\n",
    "    converters=None # Se explica a continuación\n",
    "    ...\n",
    "    )\n",
    "```\n",
    "\n",
    "Si las opciones automáticas no son suficientes se puede hacer *parsing* en base a reglas manualmente creadas usando el argumento `converters`\n",
    "\n",
    "`converters` recibe un diccionario con \"reglas de parseo\" con la siguiente sintaxis\n",
    "\n",
    "```python\n",
    "    {'nombre de la columna 1': funcion_parseadora1, \n",
    "     'nombre de la columna 2': funcion_parseadora2,\n",
    "     ...\n",
    "    }\n",
    "```\n",
    "\n",
    "Notar que `funcion_parseadoraX` puede ser una función explicita o anómina (lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**\n",
    "\n",
    "Los datos de la columna de precio de apertura (open) de \"dow_jones_index.data\" están formateados como \n",
    "\n",
    "`'$15.84'`\n",
    "\n",
    "que corresponde a un signo dolar seguido de un número real con punto decimal\n",
    "\n",
    "Para *parsear* este valor debemos escribir una función que \n",
    "\n",
    "1. Elimine el signo dolar del string\n",
    "1. Convierta el resto del string en flotante\n",
    "\n",
    "Por ejemplo\n",
    "\n",
    "```python\n",
    "def remove_dollar(text):\n",
    "    # return float(x[1:]) # Elimina el primer caracter\n",
    "    return float(x.strip(\"$\")) # Elimina todos los $ del string\n",
    "```\n",
    "\n",
    "Luego agregamos esta función a un diccionario con la llave `open` y se lo entregamos al argumento `converters`, es decir \n",
    "\n",
    "```python\n",
    "parser = {'open': remove_dollar}\n",
    "```\n",
    "\n",
    "Se puede lograr lo mismo usando una función anónima, por ejemplo:\n",
    "\n",
    "```python\n",
    "parser = {'open': lambda x: float(x.strip(\"$\"))}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación/parseo de fechas\n",
    "\n",
    "Un dato textual muy usual en datos tabulares y series de tiempo son las fechas\n",
    "\n",
    "Sin embargo el formato de fecha puede variar considerablemente entre distintas bases de datos\n",
    "\n",
    "`pandas` tiene un tipo denominado `Timestamp` el cual se puede construir con la función `pd.to_datetime()` a partir de un string \n",
    "\n",
    "Pandas identifica automaticamente fechas y horas en distintos formatos\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "```python\n",
    ">>> pd.to_datetime(\"1/5/2018\") # Formato norteamericano Mes/Día/Año \n",
    "Timestamp('2018-01-05 00:00:00')\n",
    "\n",
    ">>> pd.to_datetime(\"May/1/2018\") # También se acepta un string para el mes\n",
    "Timestamp('2018-05-01 00:00:00')\n",
    "\n",
    ">>> pd.to_datetime(\"1st of May of 2018\") # También se puede usar una frase \"Día del Mes del Año\"\n",
    "Timestamp('2018-05-01 00:00:00')\n",
    "\n",
    ">>> pd.to_datetime(\"2018\") # Autocompletación por defecto para fechas incompletas\n",
    "Timestamp('2018-01-01 00:00:00')\n",
    "\n",
    ">>> pd.to_datetime(\"14:45\") # Si usamos sólo la hora se usa la fecha actual\n",
    "Timestamp('2020-06-12 14:45:00')\n",
    "\n",
    ">>> pd.to_datetime(\"May/1/2018 14:45\") # Timestamp completo\n",
    "Timestamp('2018-05-01 14:45:00')\n",
    "```\n",
    "\n",
    "Podemos controlar el parseo de fechas en `read_csv` con los argumentos\n",
    "\n",
    "```python\n",
    "pd.read_csv(\n",
    "    ...\n",
    "    parse_dates=False # Booleano o lista con las columnas que deben ser interpretadas como fechas\n",
    "    infer_datetime_format=False, # Inferir una función parseadora de forma automática\n",
    "    dayfirst=False, # Formato día/mes/año o mes/día/año\n",
    "    date_parser=None, # Función provista por el usuario que toma un string y retorna TimeStamp\n",
    "    ...\n",
    "    )\n",
    "```\n",
    "\n",
    "Las fechas/tiempos en formato `TimeStamp` pueden usarse como índices\n",
    "\n",
    "Esto nos permite recuperar rapidamente todos los eventos dentro de un intervalo de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio práctico\n",
    "\n",
    "1. Lea el archivo `dow_jones_index.data` con `pd.read_csv` con las opciones por defecto y estudie el DataFrame resultante\n",
    "1. Corrija incrementalmente:\n",
    "    1. Use un conversor para todas las columnas numéricas que empiezan con `$`\n",
    "    1. Use el argumento `parse_dates` para importar la columna *date* como un `Timestamp` de pandas\n",
    "\n",
    "En cada paso verifique el tipo de las columnas con el atributo `dtypes`\n",
    "\n",
    "Con su tabla adecuadamente formateada retorne los valores de apertura (open), cierre (close), mínimo (low) y máximo (high) para las acciones de Alcoa Corp. (AA) entre el Marzo y Junio del 2011\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución paso a paso con comentarios:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:45.376384Z",
     "start_time": "2020-06-13T18:05:43.844935Z"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo_formato('zrBQQQZeGXw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar datos tabulares a partir de archivos excel\n",
    "\n",
    "Muchas empresas e instituciones manejan sus datos como hojas de cálculo o *spreadsheets* construidas usando software como Microsoft Excel, Openoffice/Libreoffice calc o Google spreadsheets\n",
    "\n",
    "`pandas` permite importar como `DataFrame` una hoja de cálculo en formatos `xls, xlsx, xlsm, xlsb, odf` usando  la función [`read_excel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html)\n",
    "\n",
    "   \n",
    "Muchos de los argumentos de `read_csv` están disponibles en `read_excel`, los \"nuevos\" argumentos son\n",
    "\n",
    "```python\n",
    "pd.read_excel(io, # string o path a la hoja de cálculo\n",
    "              sheet_name=0, # Entero, string o lista, especifica la(s) hoja (s) que vamos a importar\n",
    "              ...\n",
    "             )\n",
    "```\n",
    "\n",
    "**Nota** \n",
    "\n",
    "Para trabajar con archivos `excel` se requieren algunas librerías adicionales las cuales puede instalarse facilmente con conda\n",
    "\n",
    "    conda install xlrd openpyxl\n",
    "    \n",
    "    \n",
    "**Ejemplo**\n",
    "\n",
    "Consideremos los siguientes datos del censo chileno de 2017 en formato Excel de donde importaremos datos de vivienda por comuna\n",
    "\n",
    "Esto corresponde a la segunda hoja (`sheet_name=1`) y en particular las columnas de 1 a 20\n",
    "\n",
    "Importemos la planilla e inspecciones sus primeras filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:45.779664Z",
     "start_time": "2020-06-13T18:05:45.619879Z"
    }
   },
   "outputs": [],
   "source": [
    "!wget -cq http://www.censo2017.cl/wp-content/uploads/2017/12/Cantidad-de-Viviendas-por-Tipo.xlsx\n",
    "df = pd.read_excel(\"Cantidad-de-Viviendas-por-Tipo.xlsx\", \n",
    "                   sheet_name=1, # Importamos la segunda hoja (vivienda)\n",
    "                   usecols=list(range(1, 20)), # Importamos las columnas 1 a 20\n",
    "                   header=1, # El header está en la segunda fila\n",
    "                   skiprows=[2], # Eliminamos la fila 2 ya que es invalida\n",
    "                   index_col='ORDEN' # Usaremos la columna orden como índice\n",
    "                  ).dropna() # Eliminamos las filas con NaN\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulando índices y multi-índices\n",
    "\n",
    "Estudiando la tabla anterior notamos que tiene una estructura jerárquica\n",
    "\n",
    "> REGION, PROVINCIA, COMUNA\n",
    "\n",
    "Podemos representar este tipo de estructuras en pandas usando un `MultiIndex` \n",
    "\n",
    "**Creación y remoción de índices**\n",
    "\n",
    "Para asignar un índice a un DataFrame que ya está creado podemos usar el atributo\n",
    "\n",
    "```python\n",
    "df.set_index(keys, # Una etiqueta o una lista de etiquetas que serán los nuevos índices\n",
    "             drop=True, # Eliminar las columnas que pasarán a ser índices\n",
    "             inplace=False, # Retornar un nuevo dataframe o modificar df\n",
    "             ...\n",
    "            )\n",
    "```\n",
    "\n",
    "- Si keys es una etiqueta crearemos un índice regular\n",
    "- Si keys es una lista crearemos un `MultiIndex`\n",
    "\n",
    "Si queremos que nuestro índice o multi-índice vuelva a convertirse en columna podemos usar el atributo\n",
    "\n",
    "```python\n",
    "df.reset_index(level = None, # Permite especificar cuantos niveles de índices se removeran\n",
    "               drop: bool = False, # Si los índices se deben eliminar o agregar como columnas\n",
    "               inplace: bool = False,  # Retornar un nuevo dataframe o modificar df\n",
    "               ...\n",
    "               )\n",
    "```\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Crearemos un `MultiIndex` para la tabla del censo. Usaremos dos niveles de jerarquía, el superior para región y el inferior para provincia. Esto se logra con\n",
    "\n",
    "```python\n",
    "df.set_index([\"NOMBRE REGIÓN\", \"NOMBRE PROVINCIA\"], inplace=True) \n",
    "```\n",
    "\n",
    "Si inspeccionamos el objeto `df.index` veremos lo siguiente:\n",
    "\n",
    "```python\n",
    "MultiIndex([(                  'ARICA Y PARINACOTA',             'ARICA'),\n",
    "            (                  'ARICA Y PARINACOTA',             'ARICA'),\n",
    "            (                  'ARICA Y PARINACOTA',        'PARINACOTA'),\n",
    "            ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplos de *Slicing* con `MultiIndex`**\n",
    "\n",
    "Para recuperar un elemento de un DataFrame con `MultiIndex` podemos indexar usando una tupla especificando cada uno de los niveles de índices\n",
    "\n",
    "Por ejemplo para recuperar una fila en particular usamos\n",
    "\n",
    "```python\n",
    "df.loc[(\"LOS RÍOS\", \"VALDIVIA\", \"VALDIVIA\")]\n",
    "```\n",
    "Lo cual retorna la comuna de Valdivia \n",
    "\n",
    "Si queremos recuperar un conjunto de elementos podemos usar\n",
    "\n",
    "```python\n",
    "df.loc[(\"LOS RÍOS\", \"VALDIVIA\")]\n",
    "```\n",
    "Lo cual retorna todas las comunas de la provincia de Valdivia\n",
    "\n",
    "También podríamos usar\n",
    "\n",
    "```python\n",
    "df.loc[(\"LOS RÍOS\")] \n",
    "```\n",
    "Lo cual retorna todas las comunas de región de los rios\n",
    "\n",
    "Para hacer *slices* o `fancy indexing`, lo más simple es usar el objeto [`IndexSlice`](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.IndexSlice.html) \n",
    "\n",
    "Por ejemplo para recuperar dos filas usamos\n",
    "\n",
    "```python\n",
    "idx = pd.IndexSlice\n",
    "df.loc[idx[:, :, [\"VALDIVIA\", \"OSORNO\"]], :] \n",
    "```\n",
    "Lo cual retorna las comunas de Valdivia y Osorno\n",
    "\n",
    "Para recuperar subconjuntos arbitrarios de filas en base al índice jerárquico podemos usar \n",
    "\n",
    "```python\n",
    "idx = pd.IndexSlice\n",
    "df.loc[idx[:, [\"LLANQUIHUE\", \"PALENA\"], : ], :]\n",
    "```\n",
    "\n",
    "Lo cual retorna las comunas pertenecientes a las provincias de Llanquihue y Palena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio práctico\n",
    "\n",
    "- Asigne un MultiIndex al DataFrame de datos de vivienda del censo\n",
    "    - Use como primer nivel la etiqueta \"NOMBRE REGION\"\n",
    "    - Use como segundo nivel la etiqueta \"NOMBRE PROVINCIA\" \n",
    "    - Use como tercer nivel la etiqueta \"NOMBRE COMUNA\"\n",
    "- Use `loc` para seleccionar\n",
    "    - las comunas de la región de \"LOS RÍOS\"\n",
    "    - las comunas de las provincias de \"RANCO\" y \"OSORNO\"\n",
    "    - las comunas \"VALDIVIA\" y \"FRUTILLAR\"\n",
    "- Selecciona las comunas de la provincia de \"VALDIVIA\" y usa una reducción suma para encontrar el número de viviendas totales de cada tipo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:45.783752Z",
     "start_time": "2020-06-13T18:05:45.781259Z"
    }
   },
   "source": [
    "**Solución paso a paso con comentarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:47.549517Z",
     "start_time": "2020-06-13T18:05:45.785577Z"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo_formato('bWjB4089EbA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patrón *Split-Apply-Combine* en DataFrames\n",
    "\n",
    "Digamos que queremos obtener los totales de todos los tipos de vivienda a nivel de provincia\n",
    "\n",
    "Si asignamos \"NOMBRE PROVINCIA\" como índice podríamos usar\n",
    "\n",
    "```python\n",
    "result = []\n",
    "for provincia in df.index.unique():    \n",
    "    sub_df = df.loc[provincia, col_mask]\n",
    "    if sub_df.ndim>1:    \n",
    "        result.append(df.loc[provincia, col_mask].sum())\n",
    "    else: # No hacer reducción suma si la provincia tiene una sola comuna\n",
    "        result.append(df.loc[provincia, col_mask])\n",
    "pd.DataFrame(result, columns=col_mask, index=df.index.unique())\n",
    "```\n",
    "que obtiene el resultado deseado, pero es ineficiente y bastante engorroso\n",
    "\n",
    "El ejemplo anterior representa un patrón de \"operaciones condicionadas por llave\" que es muy utilizado en bases de datos y se conoce como *split-apply-combine*\n",
    "\n",
    "<img src=\"img/groupby.svg\">\n",
    "\n",
    "Donde\n",
    "\n",
    "- *split*: Divide los datos según una **llave**\n",
    "- *apply*: Realiza una función sobre cada grupo\n",
    "- *combine*: Mezcla el resultado en un nuevo dataframe donde la **llave** se convierte en el índice\n",
    "\n",
    "En el ejemplo anterior \n",
    "\n",
    "- *split*: Crea subconjuntos con las comunas de cada provincia\n",
    "- *apply*: Hace una reducción suma en las columnas de viviendas\n",
    "- *combine*: Construye un nuevo dataframe con los resultados donde la llave son las provincias "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atributo `groupby`**\n",
    "\n",
    "El patrón *split-apply-combine* está implementado de forma muy eficiente en `pandas` a través del atributo [`groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) \n",
    "\n",
    "Los argumentos básicos de `groupby` son\n",
    "\n",
    "```python\n",
    "df.groupby(by=None, # Columna o lista de columnas con se hace el split\n",
    "           axis=0, # Dividir en filas (0) o en columnas (1)\n",
    "           as_index: bool = True, # Retornar las etiquetas de cada grupo como índice\n",
    "           sort: bool = True, # Retornar las etiquetas de grupo ordenadas alfabeticamente\n",
    "           ...\n",
    "          )    \n",
    "```\n",
    "\n",
    "Notemos que `groupby` actua como un iterador\n",
    "\n",
    "```python\n",
    "for (region, sub_df) in df.groupby('NOMBRE REGIÓN'):\n",
    "    display(region, # La etiqueta\n",
    "            sub_df  # El dataframe con las filas que comparten esa etiqueta\n",
    "           )\n",
    "```\n",
    "\n",
    "La función que se ejecuta a cada grupo en el paso *apply* es un atributo de `groupby`, existen cuatro atributos\n",
    "\n",
    "- `aggregate`: Para hacer reducciones\n",
    "- `filter`: Para eliminar filas\n",
    "- `transform`: Para modificar filas\n",
    "- `apply`: Función flexible que puede combinar lo que hace `aggregate` y `transform` \n",
    "\n",
    "A continuación revisaremos las primeras tres en detalle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducción con `aggregate`**\n",
    "\n",
    "La sintaxis básica de este atributo es\n",
    "\n",
    "```python\n",
    "# Para aplicar la misma función a todos las columnas\n",
    "df.groupby(by=llave).aggregate(funcion1) \n",
    "# Para aplicar varias funciones a todos las columnas\n",
    "df.groupby(by=llave).aggregate([funcion1, funcion2, ...]) \n",
    "# Para aplicar funciones específicas a columnas específicas\n",
    "df.groupby(by=llave).aggregate({columna1: funcion1, columna2: funcion2}) \n",
    "```\n",
    "Las funciones debe entregar un sólo valor por cada columna del grupo\n",
    "\n",
    "En general las reducciones se usan para hacer resumenes, por ejemplo sumas, promedios o varianzas\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Podemos encontrar los totales de vivienda por provincia en una sola linea usando\n",
    "\n",
    "```python\n",
    "df.groupby(by=\"NOMBRE PROVINCIA\", sort=False).aggregate(np.sum)\n",
    "```\n",
    "\n",
    "O usando el alias\n",
    "\n",
    "```python\n",
    "df.groupby(by=\"NOMBRE PROVINCIA\", sort=False).sum() \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtrado con `filter`**\n",
    "\n",
    "La sintaxis básica de este atributo es\n",
    "\n",
    "```python\n",
    "df.groupby(by=llave).filter(funcion)\n",
    "```\n",
    "\n",
    "La función debe retornar `True` o `False`\n",
    "\n",
    "El resultado es un nuevo DataFrame con todos los grupos que \"pasaron el filtro\"\n",
    "\n",
    "En general este atributo se usa para eliminar/descartar grupos de filas (drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modificando el `DataFrame` con `transform`**\n",
    "\n",
    "La sintaxis básica de este atributo es\n",
    "\n",
    "```python\n",
    "df.groupby(by=llave).transform(funcion)\n",
    "```\n",
    "\n",
    "La función debe retornar un dataframe con la misma dimensión y tamaño que el original y se aplica columna a columna\n",
    "\n",
    "La función puede ser explicita o anónima (lambda)\n",
    "\n",
    "Un uso típico de este atributo es el reescalamiento/normalización a nivel de grupo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio práctico\n",
    "\n",
    "Considere las columnas de \"viviendas particulares ocupadas con moradores presentes\" ($V_1$) y \"viviendas particulares ocupadas con moradores ausentes\" ($V_2$)\n",
    "\n",
    "1. Cree un MultiIndex equivalente al del ejercicio anterior\n",
    "1. Realice una reducción promedio y desviación estándar de cada región\n",
    "1. Use un filtro para encontrar las comunas \"más responsables\", es decir aquellas donde $\\frac{V_1}{V_1 + V_2} > 0.98$\n",
    "1. Use una transformación para describir las columnas $V_1$ y $V_2$ como porcentajes a nivel regional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución paso a paso con comentarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T18:05:48.787403Z",
     "start_time": "2020-06-13T18:05:47.559455Z"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo_formato('BopVrkZKNtw')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
