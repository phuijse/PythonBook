{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T18:29:11.834669Z",
     "start_time": "2020-07-23T18:29:10.986816Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import animation\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.dpi'] = 120\n",
    "from IPython.display import HTML\n",
    "from IPython.display import YouTubeVideo\n",
    "from functools import partial\n",
    "YouTubeVideo_formato = partial(YouTubeVideo, modestbranding=1, disablekb=0,\n",
    "                               width=640, height=360, autoplay=0, rel=0, showinfo=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estadística inferencial\n",
    "\n",
    "La inferencia busca\n",
    "\n",
    "> Extraer **conclusiones** a partir de **hechos u observaciones** a través de un **método o premisa**\n",
    "\n",
    "En el caso particular de la **inferencia estadística** podemos realizar las siguientes asociaciones\n",
    "\n",
    "- Hechos: Datos\n",
    "- Premisa: Modelo probabilístico\n",
    "- Conclusión: Una cantidad no observada que es interesante\n",
    "\n",
    "Y lo que buscamos es\n",
    "\n",
    "> Cuantificar la incerteza de la conclusión dado los datos y el modelo \n",
    "\n",
    "La inferencia estadística puede dividirse en los siguientes tres niveles\n",
    "\n",
    "1. Ajustar un modelo a nuestros datos\n",
    "1. Verificar que el modelo sea confiable\n",
    "1. Responder una pregunta usando el modelo\n",
    "\n",
    "En esta lección estudiaremos las herramientas más utilizadas asociadas a cada uno de estos niveles\n",
    "\n",
    "1. **Estimador de máxima verosimilitud**\n",
    "1. **Bondad de ajuste** e **Intervalos de confianza**\n",
    "1. **Test de hipótesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de modelos: Estimación de máxima verosimilitud\n",
    "\n",
    "En este nivel de inferencia se busca **ajustar** un modelo teórico sobre nuestros datos. En esta lección nos enfocaremos en **modelos de tipo parámetrico**. Un modelo parámetrico es aquel donde **se explicita una distribución de probabilidad**.  \n",
    "\n",
    "Recordemos que una distribución tiene **parámetros**. Por ejemplo la distribución Gaussiana (univariada) se describe por su media $\\mu$ y su varianza $\\sigma^2$. Luego ajustar una distribución Gaussiana corresponde a encontrar el valor de $\\mu$ y $\\sigma$ que hace que el modelo se parezca lo más posible a la distribución empírica de los datos.\n",
    "\n",
    "A continuación veremos los pasos necesarios para ajustar una distribución a nuestros datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué distribución ajustar?\n",
    "\n",
    "Antes de ajustar debemos realizar un supuesto sobre la distribución para nuestro modelo. En general podemos ajustar cualquier distribución pero un mal supuesto podría invalidar nuestra inferencia\n",
    "\n",
    "Podemos usar las herramientas de **estadística descriptiva** para estudiar nuestros datos y tomar esta decisión de manera informada\n",
    "\n",
    "En el siguiente ejemplo, un histograma de los datos revela que un modelo gaussiano no es una buena decisión \n",
    "\n",
    "<img src=\"../img/stats6.png\">\n",
    "\n",
    "¿Por qué? La distribución empírica es claramente asimétrica, su cola derecha es más pesada que su cola izquierda. La distribución Gaussiana es simétrica por lo tanto no es apropiada en este caso ¿Qué distribución podría ser más apropiada?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo ajustar mi modelo? Estimación de máxima verosimilitud\n",
    "\n",
    "A continuación describiremos un procedimiento para ajustar modelos paramétricos llamado *maximum likelihood estimation* (MLE)\n",
    "\n",
    "Sea un conjunto de datos $\\{x_1, x_2, \\ldots, x_N\\}$\n",
    "\n",
    "**Supuesto 1** Los datos siguen el modelo $f(x;\\theta)$ donde $f(\\cdot)$ es una distribución y $\\theta$ son sus parámetros\n",
    "\n",
    "$$\n",
    "f(x_1, x_2, \\ldots, x_N |\\theta)\n",
    "$$\n",
    "\n",
    "**Supuesto 2** Las observaciones son independientes e idénticamente distribuidas (iid)\n",
    "\n",
    "- Si dos variables son independientes se cumple que $P(x, y) = P(x)P(y)$\n",
    "- Si son además idénticamente distribuidas entonces tienen **la misma distribución y parámetros**\n",
    "\n",
    "Usando esto podemos escribir\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x_1, x_2, \\ldots, x_N |\\theta) &= f(x_1|\\theta) f(x_2|\\theta) \\ldots f(x_N|\\theta) \\nonumber \\\\\n",
    "& = \\prod_{i=1}^N f(x_i|\\theta) \\nonumber \\\\\n",
    "& = \\mathcal{L}(\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde $\\mathcal{L}(\\theta)$ se conoce como la verosimilitud o probabilidad inversa de $\\theta$ \n",
    "\n",
    "Si consideramos que los datos son fijos podemos buscar el valor de $\\theta$ de máxima verosimilitud\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg} \\max_\\theta \\mathcal{L}(\\theta) \\nonumber \\\\\n",
    "&= \\text{arg} \\max_\\theta \\log \\mathcal{L}(\\theta) \\nonumber \\\\\n",
    "&= \\text{arg} \\max_\\theta \\sum_{i=1}^N \\log f(x_i|\\theta) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "El segundo paso es valido por que el máximo de $g(x)$ y $\\log(g(x))$ es el mismo. El logaritmo es monoticamente creciente. Además aplicar el logaritmo es muy conveniente ya que convierte la multiplicatoria en una sumatoria. \n",
    "\n",
    "Ahora sólo falta encontrar el máximo. Podemos hacerlo\n",
    "\n",
    "- Analíticamente, derivando con respecto a $\\theta$ e igualando a cero\n",
    "- Usando técnicas de optimización iterativas como gradiente descedente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** La pesa defectuosa\n",
    "\n",
    "<img src=\"../img/garfield.png\" width=\"250\">\n",
    "\n",
    "Su profesor quiere medir su peso pero sospecha que su pesa está defectuosa. Para comprobarlo mide su peso $N$ veces obteniendo un conjunto de observaciones $\\{x_i\\}$. ¿Es posible obtener un estimador del peso real $\\hat x$ a partir de estas observaciones?\n",
    "\n",
    "Modelaremos las observaciones como\n",
    "\n",
    "$$\n",
    "x_i = \\hat x + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "donde $\\varepsilon_i$ corresponde al ruido o error del instrumento y asumiremos que $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$, es decir que el ruido es **independiente** y **Gaussiano** con media cero y **varianza** $\\sigma_\\varepsilon^2$ **conocida**\n",
    "\n",
    "Entonces la distribución de $x_i$ es\n",
    "\n",
    "$$\n",
    "f(x_i|\\hat x) = \\mathcal{N}(\\hat x, \\sigma_\\varepsilon^2)\n",
    "$$\n",
    "\n",
    "Para encontrar $\\hat x$, primero escribimos el logaritmo de la **verosimilitud**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log \\mathcal{L}(\\hat x) &=  \\sum_{i=1}^N \\log f(x_i|\\hat x) \\nonumber \\\\\n",
    "&= \\sum_{i=1}^N  \\log \\frac{1}{\\sqrt{2\\pi\\sigma_\\varepsilon^2}}  \\exp \\left ( - \\frac{1}{2\\sigma_\\varepsilon^2} (x_i - \\hat x)^2 \\right)  \\nonumber \\\\\n",
    "&= -\\frac{N}{2}\\log(2\\pi\\sigma_\\varepsilon^2)  - \\frac{1}{2\\sigma_\\varepsilon^2}  \\sum_{i=1}^N  (x_i - \\hat x)^2  \\nonumber\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego debemos resolver\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg} \\max_\\theta \\log \\mathcal{L}(\\theta) \\nonumber \\\\\n",
    "&= \\text{arg} \\max_\\theta - \\frac{1}{2\\sigma_\\varepsilon^2}  \\sum_{i=1}^N  (x_i - \\hat x)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde podemos ignorar el primer término de la verosimilitud ya que no depende de $\\theta$. Para encontrar el máximo derivamos la expresión anterior e igualamos a cero \n",
    "\n",
    "$$\n",
    "-\\frac{1}{2\\sigma_\\varepsilon^2} \\sum_{i=1}^N 2(x_i - \\hat x ) = 0.\n",
    "$$\n",
    "\n",
    "Finalmente si despejamos llegamos a que\n",
    "\n",
    "$$\n",
    "\\hat x = \\frac{1}{N} \\sum_{i=1}^N x_i,\n",
    "$$\n",
    "\n",
    "que se conoce como el estimador de máxima verosimilitud **para la media de una Gaussiana**\n",
    "\n",
    "Recordemos que podemos comprobar que es un máximo utilizando la segunda derivada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación MLE con `scipy`\n",
    "\n",
    "Como vimos en la lección anterior el módulo [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html) provee de un gran número de distribuciones teóricas organizadas como \n",
    "\n",
    "- continuas de una variable\n",
    "- discretas de una variable\n",
    "- multivariadas\n",
    "\n",
    "Las distribuciones comparten muchos de sus métodos, a continuación revisaremos los más importantes. A modo de ejemplo consideremos la distribución Gaussiana (Normal)\n",
    "\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "dist = norm() # Esto crea una Gaussiana con media 0 y desviación estándar (std) 1\n",
    "dist = norm(loc=2, scale=2) # Esto crea una Gaussiana con media 2 y std 2\n",
    "```\n",
    "\n",
    "**Crear una muestra aleatoria con `rvs`**\n",
    "\n",
    "Luego de crear un objeto distribución podemos obtener una muestra aleatoria usando el método el atributo `rvs` \n",
    "\n",
    "```python\n",
    "dist = norm(loc=2, scale=2)\n",
    "dist.rvs(size=10, # Cantidad de números aleatorios generados\n",
    "         random_state=None #Semilla aleatoria\n",
    "        )\n",
    "```\n",
    "\n",
    "Esto retorna un arreglo de 10 números generados aleatoriamente a partir de `dist`\n",
    "\n",
    "**Evaluar la función de densidad de probabilidad** \n",
    "\n",
    "La función de densidad de la Gaussiana es\n",
    "\n",
    "$$\n",
    "f(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left( -\\frac{1}{2\\sigma^2} (x-\\mu)^2 \\right) \n",
    "$$\n",
    "\n",
    "La densidad de un objeto distribución continuo puede obtenerse con el método `pdf` el cual es función de `x`\n",
    "\n",
    "\n",
    "```python\n",
    "dist = norm(loc=2, scale=2)\n",
    "p = dist.pdf(x # Un ndrray que representa x en la ecuación superior\n",
    "            )\n",
    "plt.plot(x, p) # Luego podemos graficar la fdp\n",
    "```\n",
    "\n",
    "De forma equivalente, si deseamos la función de densidad acumulada usamos el método `cdf`\n",
    "\n",
    "Para objetos distribución discretos debemos usar el atributo `pmf` \n",
    "\n",
    "\n",
    "**Ajustar los parámetros con MLE**\n",
    "\n",
    "Para hacer el ajuste se usa el método `fit`\n",
    "\n",
    "```python \n",
    "params = norm.fit(data # Un ndarray con los datos\n",
    "                 ) \n",
    "```\n",
    "\n",
    "En el caso de la Gaussiana el vector `params` tiene dos componentes `loc` y `scale`. La cantidad de parámetros depende de la distribución que estemos ajustando. También es importante notar que para ajustar se usa `norm` (clase abstracta) y no `norm()` (instancia)\n",
    "\n",
    "Una vez que tenemos los parámetros ajustados podemos usarlos con\n",
    "\n",
    "```python\n",
    "dist = norm(loc=params[0], scale=params[1])\n",
    "```\n",
    "\n",
    "Para distribuciones que tienen más de dos parámetros podemos usar\n",
    "\n",
    "```python\n",
    "dist = norm(*params[:-2], loc=params[-2], scale=params[-1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "Observe la siguiente distribución y reflexione ¿Qué características resaltan de la misma? ¿Qué distribución sería apropiado ajustar en este caso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T18:29:11.936106Z",
     "start_time": "2020-07-23T18:29:11.836900Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cancer.csv', index_col=0)\n",
    "df = df[[\"diagnosis\", \"radius1\", \"texture1\"]]\n",
    "x = df[\"radius1\"].values\n",
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "ax.hist(x, bins=20, density=True)\n",
    "ax.set_xlabel('Radio del nucleo');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seleccione una distribución de `scipy.stats`  ajustela a los datos\n",
    "- Grafique la pdf teórica sobre el histograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T18:29:11.941145Z",
     "start_time": "2020-07-23T18:29:11.938187Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificación de modelos: Tests de bondad de ajuste\n",
    "\n",
    "Una vez que hemos ajustado un modelo es buena práctica verificar que tan confiable es este ajuste. Las herramientas más típicas para medir que tan bien se ajusta nuestra distribución teórica son\n",
    "\n",
    "- el [test de Akaike](https://en.wikipedia.org/wiki/Akaike_information_criterion)\n",
    "- los [gráficos cuantil-cuantil](https://es.wikipedia.org/wiki/Gr%C3%A1fico_Q-Q) (QQ plot)\n",
    "- el test no-paramétrico de Kolmogorov-Smirnov (KS)\n",
    "\n",
    "A continuación revisaremos el test de KS para bondad de ajuste\n",
    "\n",
    "**El test de Kolmogorov-Smirnov**\n",
    "\n",
    "Es un test no-paramétrico que compara una muestra de datos estandarizados (distribución empírica) con una distribución de densidad acumulada (CDF) teórica. Este test busca refutar la siguiente hipótesis\n",
    "\n",
    "> **Hipótesis nula:** Las distribuciones son idénticas\n",
    "\n",
    "Para aplicar el test primero debemos **estandarizar** los datos. Estandarizar se refiere a la transformación\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu_x}{\\sigma_x}\n",
    "$$\n",
    "\n",
    "es decir los datos estándarizados tienen media cero y desviación estándar uno\n",
    "\n",
    "Esto puede hacerse fácilmente con NumPy usando\n",
    "\n",
    "```python\n",
    "z = (x - np.mean(x))/np.std(x)\n",
    "```\n",
    "\n",
    "### Test de KS con `scipy`\n",
    "\n",
    "Podemos realizar el test de KS con la función [`scipy.stats.kstest`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html) donde\n",
    "\n",
    "```python\n",
    "scipy.stats.kstest(rvs, # Una muestra de observaciones estandarizadas\n",
    "                   cdf, # Una distribución acumulada teórica, por ejemplo scipy.stats.norm.cdf\n",
    "                   ...\n",
    "                  )\n",
    "```\n",
    "\n",
    "Esta función retorna el valor del estadístico de KS y su *p-value* asociado. Mientras más cerca de cero sea el estadístico de KS mejor es el ajuste. \n",
    "\n",
    "Más adelante haremos un repaso de tests de hipótesis en detalle. De momento recordemos que si el *p-value* es menor que una confianza $\\alpha=0.05$ entonces rechazamos la hipótesis nula con confianza $1-\\alpha = 0.95$ o $95\\%$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio \n",
    "\n",
    "Considere la muestra de datos anterior\n",
    "- Seleccione un conjunto de distribuciones teóricas \n",
    "- Encuentra la que tiene mejor ajuste usando `kstest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T18:29:12.019596Z",
     "start_time": "2020-07-23T18:29:11.943159Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responder preguntas con nuestro modelo: Test de hipótesis\n",
    "\n",
    "Se aplica un tratamiento nuevo a una muestra de la población \n",
    "\n",
    "- ¿Es el tratamiento efectivo?\n",
    "- ¿Existe una diferencia entre los que tomaron el tratamiento y los que no?\n",
    "\n",
    "El test de hipótesis es un procedimiento estadístico para comprobar si el resultado de un experimento es significativo en la población\n",
    "\n",
    "Para esto formulamos dos escenarios cada uno con una hipótesis asociada\n",
    "\n",
    "- Hipótesis nula ($H_0$): Por ejemplo\n",
    "    - \"El experimento no produjo diferencia\"\n",
    "    - \"El experimento no tuvo efecto\"\n",
    "    - \"Las observaciones son producto del azar\"\n",
    "- Hipótesis alternativa ($H_A$): Usualmente el complemento de $H_0$\n",
    "\n",
    "> El test de hipótesis se diseña para medir que tan fuerte es la evidencia **en contra** de la hipótesis nula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo general de un test de hipótesis\n",
    "\n",
    "El siguiente es el algoritmo general de un test de hipótesis paramétrico\n",
    "\n",
    "1. Definimos $H_0$ y $H_A$\n",
    "1. Definimos un estadístico $T$\n",
    "1. Asumimos una distribución para $T$ dado que $H_0$ es cierto\n",
    "1. Seleccionamos un nivel de significancia $\\alpha$ \n",
    "1. Calculamos el $T$ para nuestros datos $T_{data}$\n",
    "1. Calculamos el **p-value**\n",
    "    - Si nuestro test es de una cola:\n",
    "        - Superior: $p = P(T>T_{data})$\n",
    "        - Inferior: $p = P(T<T_{data})$\n",
    "    - Si nuestro test es dos colas: $p = P(T>T_{data}) + P(T<T_{data})$\n",
    "\n",
    "Finalmente:\n",
    "\n",
    "`Si`  $p < \\alpha$\n",
    "    \n",
    "> Rechazamos la hipótesis nula con confianza (1-$\\alpha$)\n",
    "\n",
    "`De lo contrario`\n",
    "    \n",
    "> No hay suficiente evidencia para rechazar la hipótesis nula\n",
    "\n",
    "El valor de $\\alpha$ nos permite controlar el **[Error tipo I](https://es.wikipedia.org/wiki/Errores_de_tipo_I_y_de_tipo_II)**, es decir el error que cometemos si rechazamos $H_0$ cuando en realidad era cierta (falso positivo)\n",
    "\n",
    "Tipicamente se usa $\\alpha=0.05$ o $\\alpha=0.01$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Errores de interpretación comunes**\n",
    "\n",
    "Muchas veces se asume que el p-value es la probabilidad de que $H_0$ sea cierta dado nuestras observaciones\n",
    "\n",
    "$$\n",
    "p = P(H_0 | T> T_{data})\n",
    "$$\n",
    "\n",
    "Esto es un **grave error**. Formálmente el **p-value** es la probabilidad de observar un valor de $T$ más extremo que el observado, es decir \n",
    "\n",
    "$$\n",
    "p = P(T> T_{data} | H_0) \n",
    "$$\n",
    "\n",
    "Otro error común es creer que no ser capaz de rechazar $H_0$ es lo mismo que aceptar $H_0$\n",
    "\n",
    "No tener suficiente evidencia para rechazar no es lo mismo que aceptar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un primer test de hipótesis: El t-test de una muestra \n",
    "\n",
    "Sea un conjunto de $N$ observaciones iid $X = {x_1, x_2, \\ldots, x_N}$ con media muestral $\\bar x = \\sum_{i=1}^N x_i$ \n",
    "\n",
    "El t-test de una muestra es un test de hipótesis que busca verificar si $\\bar x$ es significativamente distinta de la **media poblacional** $\\mu$, en el caso de que **no conocemos la varianza poblacional** $\\sigma^2$\n",
    "\n",
    "Las hipótesis son\n",
    "\n",
    "- $H_0:$ $\\bar x = \\mu$\n",
    "- $H_A:$ $\\bar x \\neq \\mu$ (dos colas)\n",
    "\n",
    "El estadístico de prueba es \n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar x - \\mu}{\\hat \\sigma /\\sqrt{N-1}}\n",
    "$$\n",
    "\n",
    "donde $\\hat \\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i - \\bar x)^2}$ es la desviación estándar muestral (sesgada)\n",
    "\n",
    "Si asumimos que $\\bar x$ se distribuye $\\mathcal{N}(\\mu, \\frac{\\sigma^2}{N})$ entonces\n",
    "$t$ se distribuye [t-student](https://en.wikipedia.org/wiki/Student%27s_t-distribution) con $N-1$ grados de libertad\n",
    "\n",
    "- Para muestras iid y $N$ grande el supuesto se cumple por teorema central del límite\n",
    "- Si $N$ es pequeño debemos verificar la normalidad de los datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación de t-test para probar que la regresión es significativa\n",
    "\n",
    "En un modelo de regresión lineal donde tenemos $N$ ejemplos\n",
    "\n",
    "$$\n",
    "y_i = x_i \\theta_1 + \\theta_0, ~ i=1, 2, \\ldots, N\n",
    "$$\n",
    "\n",
    "Podemos probar que la correlación entre $x$ es $y$ es significativa con un test sobre $\\theta_1$\n",
    "\n",
    "Por ejemplo podemos plantear las siguientes hipótesis\n",
    "\n",
    "- $H_0:$ La pendiente es nula $\\theta_1= 0$ \n",
    "- $H_A:$ La pendiente no es nula: $\\theta_1\\neq 0$ (dos colas)\n",
    "\n",
    "Y asumiremos que $\\theta_1$ es normal pero que desconocemos su varianza. Bajo este supuesto se puede formular el siguiente estadístico de prueba \n",
    "\n",
    "$$\n",
    "t = \\frac{(\\theta_1-\\theta^*) }{\\text{SE}_{\\theta_1}/\\sqrt{N-2}} = \\frac{ r\\sqrt{N-2}}{\\sqrt{1-r^2}},\n",
    "$$\n",
    "\n",
    "donde $r$ es el coeficiente de correlación de Pearson (detalles más adelante) y la última expresión se obtiene reemplazando  $\\theta^*=0$ y $\\text{SE}_{\\theta_1} = \\sqrt{ \\frac{\\frac{1}{N} \\sum_i (y_i - \\hat y_i)^2}{\\text{Var}(x)}}$. \n",
    "\n",
    "El estadístico tiene distribución t-student con dos grados de libertad (modelo de dos parámetros) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio formativo: Regresión lineal \n",
    "\n",
    "En lecciones anteriores estudiamos el modelo de regresión lineal el cual nos permite estudiar si existe correlación entre variables continuas. También vimos como ajustar los parámetros del modelo usando el método de mínimos cuadrados. En este ejercicio formativo veremos como verificar si el modelo de regresión ajustado es correcto\n",
    "\n",
    "Luego de revisar este ejercicio usted habrá aprendido\n",
    "\n",
    "- La interpretación probabilística de la regresión lineal y la relación entre mínimos cuadrados ordinarios y la estimación por máxima verosimilitud\n",
    "- El estadístico $r$ para medir la fuerza de la correlación entre dos variables\n",
    "- Un test de hipótesis para verificar que la correlación encontrada es estadística significativa\n",
    "\n",
    "Usaremos el siguiente dataset de consumo de helados. Referencia: [A handbook of small datasets](https://www.routledge.com/A-Handbook-of-Small-Data-Sets/Hand-Daly-McConway-Lunn-Ostrowski/p/book/9780367449667), estudio realizado en los años 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T18:29:12.104916Z",
     "start_time": "2020-07-23T18:29:12.021606Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/helados.csv', header=0, index_col=0)\n",
    "df.columns = ['consumo', 'ingreso', 'precio', 'temperatura']\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset tiene la temperatura promedio del día (grados Fahrenheit), el precio promedio de los helados comprados (dolares), el ingreso promedio familiar semanal de las personas que compraron helado (dolares) y el consumo ([pintas](https://en.wikipedia.org/wiki/Pint) per capita).\n",
    "\n",
    "A continuación se muestra un gráfico de dispersión del consumo en función de las demás variables. ¿Cree usted que existe correlación en este caso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True, sharey=True)\n",
    "for i, col in enumerate(df.columns[1:]):\n",
    "    ax[i].scatter(df[col], df[\"consumo\"], s=10)\n",
    "    ax[i].set_xlabel(col)\n",
    "ax[0].set_ylabel(df.columns[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación probabilística y MLE de la regresión lineal\n",
    "\n",
    "Sea $y$ el consumo y $x$ la temperatura.\n",
    "\n",
    "Asumiremos errores gaussianos iid\n",
    "\n",
    "$$\n",
    "y_i = \\hat y_i + \\epsilon_i, \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2),\n",
    "$$\n",
    "\n",
    "y un modelo lineal de **dos parámetros** (linea recta)\n",
    "\n",
    "$$\n",
    "\\hat y_i = \\theta_0 + \\theta_1 x_i\n",
    "$$\n",
    "\n",
    "Bajo estos supuestos el estimador de máxima verosimilitud es \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg}\\max_\\theta \\log \\mathcal{L}(\\theta) \\nonumber \\\\\n",
    "&=\\text{arg}\\max_\\theta  - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - \\theta_0 - \\theta_1 x_i)^2 \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Es decir que el estimador de máxima verosimilitud es equivalente al de mínimos cuadrados ordanrios $\\hat \\theta= (X^T X)^{-1} X^T y$ que vimos anteriormente\n",
    "\n",
    "**Importante:** Cuando utilizamos la solución de mínimos cuadrados estamos asumiendo implicitamente que las observaciones son iid y que la verosimilitud es Gaussiana\n",
    "\n",
    "\n",
    "Derivando con respecto a los parámetros e igualado a cero tenemos que\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_i y_i  - N\\theta_0 - \\theta_1  \\sum_i x_i &= 0 \\nonumber \\\\\n",
    "\\sum_i y_i x_i - \\theta_0 \\sum_i x_i - \\theta_1 \\sum_i x_i^2 &= 0 \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finalmente podemos despejar\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_0 &= \\bar y - \\theta_1 \\bar x \\nonumber \\\\\n",
    "\\theta_1 &= \\frac{\\sum_i x_i y_i - N \\bar x \\bar y}{\\sum_i x_i^2 - M \\bar x^2}  \\nonumber \\\\\n",
    "&= \\frac{ \\sum_i (y_i - \\bar y)(x_i - \\bar x)}{\\sum_i (x_i - \\bar x)^2}  = \\frac{\\text{COV}(x, y)}{\\text{Var}(x)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "de donde reconocemos las expresiones para la covarianza entre $x$ e $y$ y la varianza de $x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coeficiente de correlación de Pearson\n",
    "\n",
    "La fuerza de la correlación se suele medir usando\n",
    "\n",
    "$$\n",
    "r^2 = 1 - \\frac{\\sum_i ( y_i - \\hat y_i)^2}{\\sum_i ( y_i - \\bar y)^2} = 1 - \\frac{\\frac{1}{M} \\sum_i (y_i - \\hat y_i)^2}{\\text{Var}(y)} = \\frac{\\text{COV}^2(x, y)}{\\text{Var}(x) \\text{Var}(y)}\n",
    "$$\n",
    "\n",
    "donde $r = \\frac{\\text{COV}(x, y)}{\\sqrt{\\text{Var}(x) \\text{Var}(y)}} \\in [-1, 1]$ se conoce como [coeficiente de correlación de Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n",
    "\n",
    "donde\n",
    "\n",
    "- si $r=1$ existe una correlación lineal perfecta\n",
    "- si $r=-1$ existe una anticorrelación lineal perfecta\n",
    "- si $r=0$ no hay correlación lineal entre las variables\n",
    "\n",
    "En general un $r>0.5$ se considera una correlación importante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculando $r$ con y los parámetros de la regresión lineal**\n",
    "\n",
    "Podemos usar el atributo de dataframe\n",
    "\n",
    "```python\n",
    "df.corr()\n",
    "```\n",
    "\n",
    "Que retorna la matriz de correlaciones lineales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos también el valor de los parámetros podemos usar la función de scipy \n",
    "\n",
    "```python\n",
    "scipy.stats.linregress(x, # Variable independiente unidimensional\n",
    "                       y # Variable dependiente unidimensional\n",
    "                      )\n",
    "```\n",
    "\n",
    "Esta función retorna una tupla con\n",
    "\n",
    "- Valor de la pendiente: $\\theta_1$\n",
    "- Valor de la intercepta: $\\theta_0$\n",
    "- Coeficiente de correlación $r$\n",
    "- p-value\n",
    "- Error estándar del ajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T18:29:12.415952Z",
     "start_time": "2020-07-23T18:29:12.255458Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True, sharey=True)\n",
    "ax[0].set_ylabel(df.columns[0]);\n",
    "\n",
    "\n",
    "for i, col in enumerate(df.columns[1:]):\n",
    "    res = scipy.stats.linregress(df[col], df[\"consumo\"])\n",
    "    x_plot = np.linspace(np.amin(df[col]), np. amax(df[col]), num=100)\n",
    "    ax[i].scatter(df[col], df[\"consumo\"], label='datos', s=10)    \n",
    "    ax[i].plot(x_plot, res.slope*x_plot + res.intercept, lw=2, c='r', label='modelo');\n",
    "    ax[i].set_xlabel(col)\n",
    "    ax[i].set_title(f\"$r$: {res.rvalue:0.5f}\")\n",
    "    ax[i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir que visualmente parece existir\n",
    "\n",
    "- una correlación positiva alta entre consumo y temperatura\n",
    "- una correlación negativa moderada entre consumo y precio\n",
    "- una correlación cercana a cero entre consumo e ingreso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test de hipótesis y conclusiones\n",
    "\n",
    "La función `linregress` implementa el t-test sobre $\\theta_1$ que vimos anteriormente. Usemos estos resultados para verificar si las correlaciones son estadísticamente significativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "for i, col in enumerate(df.columns[1:]):\n",
    "    res = scipy.stats.linregress(df[col], df[\"consumo\"])\n",
    "    print(f\"{col}: \\t p-value:{res.pvalue:0.4f} \\t ¿Menor que {alpha}?: {res.pvalue < alpha}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como complemento visualizemos \n",
    "\n",
    "- las distribuciones bajo la hipótesis nula: linea azul\n",
    "- los límites dados por $\\alpha$: linea punteada negra\n",
    "- El valor del observado para cada una de las variables: linea roja "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T18:29:12.537649Z",
     "start_time": "2020-07-23T18:29:12.417765Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 2), tight_layout=True, sharey=True)\n",
    "ax[0].set_ylabel(df.columns[0]);\n",
    "\n",
    "N = df.shape[0]\n",
    "t = np.linspace(-7, 7, num=1000)\n",
    "dist = scipy.stats.t(loc=0, scale=1, df=N-2) # dos grados de libertad\n",
    "\n",
    "\n",
    "for i, col in enumerate(df.columns[1:]):\n",
    "    res = scipy.stats.linregress(df[col], df[\"consumo\"])\n",
    "    t_data = res.rvalue*np.sqrt(N-2)/np.sqrt(1.-res.rvalue**2)\n",
    "    ax[i].plot(t, dist.pdf(t))\n",
    "    ax[i].plot([dist.ppf(alpha/2)]*2, [0, np.amax(dist.pdf(t))], 'k--')\n",
    "    ax[i].plot([dist.ppf(1-alpha/2)]*2, [0, np.amax(dist.pdf(t))], 'k--')\n",
    "    ax[i].plot([t_data]*2, [0, np.amax(dist.pdf(t))], 'r-')\n",
    "    ax[i].set_xlabel(col)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusión**\n",
    "\n",
    "Basado en los p-values y considerando $\\alpha=0.05$\n",
    "\n",
    "¿Qué podemos decir de las correlaciones con el consumo de helados?\n",
    "\n",
    "> Rechazamos la hipótesis nula de que no existe correlación entre temperatura y consumo con un 95% de confianza\n",
    "\n",
    "Para las variables ingreso y precio no existe suficiente evidencia para rechazar $H_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflexión final\n",
    "\n",
    "En el ejercicio anterior usamos t-test para una regresión lineal entre dos variables ¿Qué prueba puedo usar si quiero hacer regresión lineal multivariada? \n",
    "\n",
    "> Se puede usar [ANOVA](https://pythonfordatascience.org/anova-python/)\n",
    "\n",
    "¿Qué pasa si...\n",
    "\n",
    "- mis datos tienen una relación que no es lineal? \n",
    "- $\\theta_1$ no es Gaussiano/normal? \n",
    "- si el ruido no es Gaussiano? \n",
    "- si el ruido es Gaussiano pero su varianza cambia en el tiempo? \n",
    "\n",
    "> En estos casos no se cumplen los supuestos del modelo o del test, por ende el resultado no es confiable\n",
    "\n",
    "Si mis supuestos no se cumplen con ninguna prueba parámetrica, la opión es utilizar pruebas no-paramétricas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba no-paramétrica: *Bootstrap*\n",
    "\n",
    "Podemos estimar la incerteza de un estimador de forma no-paramétrica usando **muestreo tipo *bootstrap***\n",
    "\n",
    "Esto consiste en tomar nuestro conjunto de datos de tamaño $N$ y crear $T$ nuevos conjuntos que \"se le parezcan\". Luego se calcula el valor del estimador que estamos buscando en los $T$ conjuntos. Con esto obtenemos una distribución para el estimador como muestra el siguiente diagrama\n",
    "\n",
    "\n",
    "<img src=\"https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2016/10/bootstrap-sample.png\">\n",
    "\n",
    "\n",
    "Para crear los subconjuntos podríamos suponer independencia y utilizar **muestreo con reemplazo**. Esto consiste en tomar $N$ muestras al azar permitiendo repeticiones, como muestra el siguiente diagrama\n",
    "\n",
    "<img src=\"../img/stats7.png\" width=\"700\">\n",
    "\n",
    "Si no es posible suponer indepdencia se puede realizar bootstrap basado en residuos y bootstrap dependiente. Puedes consultar más detalles sobre [*bootstrap*](https://www.stat.cmu.edu/~cshalizi/402/lectures/08-bootstrap/lecture-08.pdf) [aquí](http://homepage.divms.uiowa.edu/~rdecook/stat3200/notes/bootstrap_4pp.pdf) y [acá](https://www.sagepub.com/sites/default/files/upm-binaries/21122_Chapter_21.pdf). A continuación nos enfocaremos en el clásico muestreo con reemplazo y como implementarlo en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación con Numpy y Scipy\n",
    "\n",
    "La función `numpy.random.choice` permite remuestrear un conjunto de datos\n",
    "\n",
    "Por ejemplo para la regresión lineal debemos remuestrar las parejas/tuplas $(x_i, y_i)$\n",
    "\n",
    "Luego calculamos y guardamos los parámetros del modelo para cada remuestreo. En este ejemplo haremos $1000$ repeticiones del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T18:29:13.021567Z",
     "start_time": "2020-07-23T18:29:12.539446Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/helados.csv', header=0, index_col=0)\n",
    "df.columns = ['consumo', 'ingreso', 'precio', 'temperatura']\n",
    "\n",
    "x, y = df[\"temperatura\"].values, df[\"consumo\"].values\n",
    "params = scipy.stats.linregress(x, y)\n",
    "\n",
    "def muestreo_con_reemplazo(x, y):\n",
    "    N = len(x)\n",
    "    idx = np.random.choice(N, size=N, replace=True)\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "def boostrap_linregress(x, y, T=100):\n",
    "    # Parámetros: t0, t1 y r\n",
    "    params = np.zeros(shape=(T, 3)) \n",
    "    for t in range(T):\n",
    "        res = scipy.stats.linregress(*muestreo_con_reemplazo(x, y))\n",
    "        params[t, :] = [res.intercept, res.slope, res.rvalue]\n",
    "    return params\n",
    "\n",
    "boostrap_params = boostrap_linregress(x, y, T=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intervalos de confianza empíricos\n",
    "\n",
    "Veamos la distribución empírica de $r$ obtenida usando bootstrap\n",
    "\n",
    "En la figura de abajo tenemos\n",
    "\n",
    "- Histograma azul: Distribución bootstrap de $r$\n",
    "- Linea roja: $r$ de los datos\n",
    "- Lineas punteadas negras: Intervalo de confianza empírico al 95%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T18:29:13.079035Z",
     "start_time": "2020-07-23T18:29:13.023395Z"
    }
   },
   "outputs": [],
   "source": [
    "r_bootstrap = boostrap_params[:, 2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3), tight_layout=True)\n",
    "hist_val, hist_lim, _ = ax.hist(r_bootstrap, bins=20, density=True)\n",
    "\n",
    "ax.plot([params.rvalue]*2, [0, np.max(hist_val)], 'r-', lw=2)\n",
    "IC = np.percentile(r_bootstrap, [2.5, 97.5])\n",
    "ax.plot([IC[0]]*2, [0, np.max(hist_val)], 'k--', lw=2)\n",
    "ax.plot([IC[1]]*2, [0, np.max(hist_val)], 'k--', lw=2)\n",
    "\n",
    "print(f\"Intervalo de confianza al 95% de r: {IC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la figura podemos notar que el 95% de la distribución empírica esta sobre $r=0.5$\n",
    "\n",
    "También podemos notar que la distribución empírica de $r$ no es simétrica, por lo que aplicar un t-test parámetrico sobre $r$ no hubiera sido correcto "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando la incerteza del modelo\n",
    "\n",
    "Usando la distribución empírica de los parámetros $\\theta_0$ y $\\theta_1$ podemos visualizar la incerteza de nuestro modelo de regresión lineal\n",
    "\n",
    "En la figura de abajo tenemos\n",
    "- Puntos azules: Datos\n",
    "- Linea roja: Modelo de regresión lineal en los datos\n",
    "- Sombra rojo claro: $\\pm 2$ desviaciones estándar del modelo en base a la distribución empírica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T18:29:13.157915Z",
     "start_time": "2020-07-23T18:29:13.080675Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 3), tight_layout=True)\n",
    "ax.set_ylabel('Consumo')\n",
    "ax.set_xlabel('Temperatura')\n",
    "ax.scatter(x, y, zorder=100, s=10, label='datos')\n",
    "\n",
    "def model(theta0, theta1, x):\n",
    "    return x*theta1 + theta0\n",
    "\n",
    "ax.plot(x_plot, model(params.intercept, params.slope, x_plot),\n",
    "        c='r', lw=2, label='mejor ajuste')\n",
    "\n",
    "dist_lines = model(boostrap_params[:, 0], boostrap_params[:, 1], x_plot.reshape(-1, 1)).T\n",
    "mean_lines, std_lines = np.mean(dist_lines, axis=0), np.std(dist_lines, axis=0)\n",
    "ax.fill_between(x_plot, \n",
    "                mean_lines - 2*std_lines,\n",
    "                mean_lines + 2*std_lines, \n",
    "                color='r', alpha=0.25, label='incerteza')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
