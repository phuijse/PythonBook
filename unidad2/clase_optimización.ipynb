{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from matplotlib import animation\n",
    "import ipywidgets as widgets\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización\n",
    "\n",
    "Es el proceso para:\n",
    "- **(Prosa)** encontrar la mejor solución de un problema\n",
    "- **(Matematicamente)** encontrar un valor extremo de una función\n",
    "\n",
    "### Definiciones\n",
    "\n",
    "- Función objetivo: Función continua que queremos optimizar, $f : \\mathbb{R}^D \\to \\mathbb{R}$\n",
    "    - Puede ser diferenciable\n",
    "    - Puede ser convexa\n",
    "- Valor extremo: Mínimo o máximo de la función objetivo, $ \\max f(\\vec x) \\equiv \\min - f(\\vec x)$\n",
    "    - Es suficiente hablar de minimización\n",
    "- Un mínimo $x^*$ es tal que $f(x) > f(x^*)$ para $x \\in \\mathbb{S}$\n",
    "    - Si $\\mathbb{S}$ es igual al dominio de $f(x)$ entonces es un mínimo global\n",
    "    - De lo contrario hablamos de un mínimo local\n",
    "    - Una función convexa tiene sólo un mínimo\n",
    "- La solución que buscamos podría estar sujeta a restricciones\n",
    "\n",
    "\n",
    "### Problema general de optimización\n",
    "\n",
    "Para una función $f : \\mathbb{R}^D \\to \\mathbb{R}$\n",
    "$$\n",
    "\\min_x f(x) ~ \\text{s.a.} ~g(x) = 0, h(x) \\leq 0,\n",
    "$$\n",
    "donde $g : \\mathbb{R}^D \\to \\mathbb{R}^G$ y $h : \\mathbb{R}^D \\to \\mathbb{R}^H$\n",
    "\n",
    "#### Clasificación de problemas de optimización\n",
    "\n",
    "- Una variable versus multi-variable\n",
    "- Ecuaciones lineales o no-lineales (convexo o no convexo)\n",
    "- Sin/con restricciones (sin/con desigualdades)\n",
    "\n",
    "Mínimos cuadrados: Multi-variable, lineal, sin restricciones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo encontrar el mínimo de una función continua sin restricciones?\n",
    "\n",
    "Encontrar las raices (ceros) de la derivada/gradiente de $f$\n",
    "\n",
    "$$\n",
    "\\nabla f (\\theta^*) = \\begin{pmatrix} \\frac{\\partial f}{\\partial \\theta_1}, \\frac{\\partial f}{\\partial \\theta_2}, \\ldots, \\frac{\\partial f}{\\partial \\theta_D} \\end{pmatrix} = \\vec 0\n",
    "$$\n",
    "\n",
    "Las soluciones se conocen como puntos estacionarios de $f$\n",
    "\n",
    "Luego si las segunda derivada/Hessiano de $f$\n",
    "\n",
    "$$\n",
    "H_{ij}^f (\\theta^*)  = \\frac{\\partial^2 f}{\\partial \\theta_i \\partial \\theta_j} (\\theta^*)\n",
    "$$\n",
    "\n",
    "es positiva/semi-definida positiva  entonces $\\theta^*$ es un **mínimo local**\n",
    "\n",
    "\n",
    "Receta\n",
    "1. Obtener $\\theta^*$ tal que $\\nabla f (\\theta^*)=0$\n",
    "1. Probar que es un mínimo el Hessiano\n",
    "\n",
    "##### Problema: Sólo sirve si podemos obtener una expresión análitica de $\\theta$ a partir de $\\nabla f (\\theta^*)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo: Regresión con elemento no-lineal\n",
    "\n",
    "- Sea un conjunto de $M=100$ datos que corresponden a dos categorías, $y_i \\in \\{0, 1\\}$\n",
    "- Los datos son bidimensionales, $x_i \\in \\mathbb{R}^2$\n",
    "- Para mapear los datos a la categoría pasar un **hiperplano** a través de  una **función no-lineal**\n",
    "\n",
    "$$\n",
    "\\sigma \\left ( \\theta_0 + \\theta_1 x_{i1} + \\theta_2 x_{i2} \\right) \\approx y_i ~ \\forall i=1,\\ldots,100\n",
    "$$\n",
    "por ejemplo\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-x}} \\in [0, 1],\n",
    "$$\n",
    "que se conoce como función sigmoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "z = np.linspace(-6, 6, num=100)\n",
    "ax.plot(z, 1/(1+np.exp(-z)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego podemos intentar ajustar \n",
    "\n",
    "$$\n",
    "\\min_\\theta \\sum_{i=1}^M \\left(y_i - \\sigma \\left ( \\theta_0 + \\theta_1 x_{i1} + \\theta_2 x_{i2} \\right)\\right)^2\n",
    "$$\n",
    "\n",
    "Pero: \n",
    "- Ya no es lineal en sus parámetros\n",
    "- No podremos despejar en función de $\\theta$\n",
    "- No hay solución análitica como en mínimos cuadrados lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "X = np.concatenate((np.random.randn(N//2, 2), 1.5+np.random.randn(N//2, 2)), axis=0)\n",
    "Y = np.concatenate((np.zeros(shape=(N//2, 1)), np.ones(shape=(N//2, 1))), axis=0)[:, 0]\n",
    "fig, ax = plt.subplots(figsize=(7, 4), tight_layout=True)\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], c='r', label='gatos')\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], c='b', label='perros')\n",
    "ax.set_ylabel('Lealtad normalizada')\n",
    "ax.set_xlabel('Tamaño normalizado')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Búsqueda exhaustiva de la mejor solución\n",
    "\n",
    "También conocido como \"método de fuerza bruta\"\n",
    "1. Definimos una grilla para nuestro espacio de parámetros (dominio y resolución)\n",
    "1. Para cada elemento de la grilla calculamos la función de costo\n",
    "1. Buscamos el elemento con menor función de costo\n",
    "\n",
    "Ventaja: Si la resolución es lo suficientemente fina podemos encontrar el mínimo global del dominio\n",
    "\n",
    "Desventaja: Costo computacional, explosión combinatorial\n",
    "\n",
    "\n",
    "Sea un modelo de $10$ parámetros con una resolución de $1000$ puntos cada uno: $1000^{10}$ evaluaciones de $f()$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def f(theta0, theta1, theta2):\n",
    "    model = theta0 + theta1*X[:, 0] + theta2*X[:, 1]\n",
    "    sig = 1./(1. + np.exp(-model))\n",
    "    return np.mean((Y-sig)**2)\n",
    "\n",
    "theta0 = np.linspace(-10, 10, num=10)\n",
    "theta1 = np.linspace(-10, 10, num=10)\n",
    "theta2 = np.linspace(-10, 10, num=10)\n",
    "\n",
    "mse_plot = np.zeros(shape=(len(theta0), len(theta1), len(theta2)))\n",
    "for k, t0_ in enumerate(theta0):\n",
    "    for i, t1_ in enumerate(theta1):\n",
    "        for j, t2_ in enumerate(theta2):\n",
    "            mse_plot[k, i, j] = f(t0_, t1_, t2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "T1, T2 = np.meshgrid(theta1, theta2)\n",
    "idx_all = np.unravel_index(np.argmin(mse_plot), mse_plot.shape)\n",
    "\n",
    "def update_plot(idx):\n",
    "    ax.cla()\n",
    "    ax.set_title(r\"$\\theta_0$ = {0:0.4f}\".format(theta0[idx]))\n",
    "    cf = ax.pcolormesh(T1, T2, mse_plot[idx, :, :], cmap=plt.cm.Reds,\n",
    "                     vmin=np.amin(mse_plot), vmax=np.amax(mse_plot))\n",
    "    idx = np.unravel_index(np.argmin(mse_plot[idx, :, :]), mse_plot[0, :, :].shape)\n",
    "    ax.scatter(theta1[idx[1]], theta2[idx[0]], s=100, c='k', marker='x');\n",
    "    ax.scatter(theta1[idx_all[2]], theta2[idx_all[1]], s=100, c='k');\n",
    "    ax.set_xlim((theta1[0], theta1[-1]))\n",
    "    ax.set_ylim((theta2[0], theta2[-1]))\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update_plot, frames=len(theta0), \n",
    "                               interval=5000/len(theta0), repeat=False, blit=True)\n",
    "\n",
    "display(theta0[idx_all[0]], theta1[idx_all[1]], theta2[idx_all[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método iterativos: Newton\n",
    "\n",
    "\n",
    "- Acercarse a la mejor solución paso a paso\n",
    "- Encontrar en cada instante la mejor dirección\n",
    "\n",
    "Sea el valor actual del vector de parámetros $\\theta_t$\n",
    "\n",
    "Queremos encontrar el mejor \"próximo valor\" según nuestra función objetivo\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t + \\Delta \\theta\n",
    "$$\n",
    "Consideremos la aproximación de Taylor de segundo orden de $f$\n",
    "$$\n",
    "f(\\theta_{t} + \\Delta \\theta) \\approx f(\\theta_t) + \\nabla f (\\theta_t) \\Delta \\theta + \\frac{1}{2} \\Delta \\theta^T H_f (\\theta_t) \\Delta \\theta \n",
    "$$\n",
    "Derivando en función de $\\Delta \\theta$ e igualando a cero tenemos\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla f (\\theta_t)  +  H_f (\\theta_t) \\Delta \\theta &= 0 \\nonumber \\\\\n",
    "\\Delta \\theta &= - [H_f (\\theta_t)]^{-1}\\nabla f (\\theta_t)  \\nonumber \\\\\n",
    "\\theta_{t+1} &= \\theta_{t} - [H_f (\\theta_t)]^{-1}\\nabla f (\\theta_t)  \\nonumber \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Se obtiene una regla iterativa en función del **Gradiente** y del **Hessiano**\n",
    "- La solución depende de $\\theta_0$\n",
    "- \"Asumimos\" que la aproximación de segundo orden es \"buena\"\n",
    "- Si nuestro modelo tiene $N$ parámetros el Hessiano es de $N\\times N$, ¿Qué pasa si $N$ es grande?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradiente descendente\n",
    "\n",
    "Si el Hessiano es prohibitivo podemos usar una aproximación de primer orden\n",
    "\n",
    "El método más clásico es el **gradiente descendente**\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\eta \\nabla f (\\theta_t)\n",
    "$$\n",
    "\n",
    "donde hemos reemplazado el Hessiano por una constante $\\eta$ llamado \"paso\" o \"tasa de aprendizaje\"\n",
    "\n",
    "¿Cómo cambia la optimización con distintos $\\eta$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(2, figsize=(7, 4), tight_layout=True, sharex=True)\n",
    "x = np.linspace(-4, 6, num=100)\n",
    "f = lambda theta : (theta-1.)**2 #+ 10*np.sin(theta)\n",
    "df = lambda theta : 2*(theta -1.) #+ 10*np.cos(theta)\n",
    "df2 = lambda theta : 2 #- 10*np.cos(theta)\n",
    "\n",
    "t = 10*np.random.rand(10) - 4.\n",
    "ax[0].plot(x, f(x))\n",
    "sc = ax[0].scatter(t, f(t), s=100)\n",
    "\n",
    "ax[1].set_xlabel(r'$\\theta$')\n",
    "ax[0].set_ylabel(r'$f(\\theta)$')\n",
    "ax[1].plot(x, -df(x))\n",
    "ax[1].set_ylabel(r'$-\\nabla f(\\theta)$')\n",
    "eta = 0.01\n",
    "\n",
    "def update(n):\n",
    "    t = sc.get_offsets()[:, 0]\n",
    "    t -= eta*df(t)\n",
    "    #t -= df(t)/(df2(t)+10)\n",
    "    sc.set_offsets(np.c_[t, f(t)])\n",
    "    \n",
    "anim = animation.FuncAnimation(fig, update, frames=100, interval=200, repeat=False, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión con elemento no lineal usando gradiente descendente\n",
    "\n",
    "- Calcule el gradiente de la función de costo en función de los parámetros\n",
    "- Pruebe distintos valores de $\\eta$ y estudie lo que ocurre con la solución\n",
    "- ¿Cómo cambia la solución con la inicialización?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# Dividir los datos en entrenamiento y validación\n",
    "idx = np.split(np.random.permutation(len(X)), 4)\n",
    "train_idx, valid_idx = np.hstack(idx[:-1]), idx[-1]\n",
    "# Inicializar parámetros\n",
    "theta0, theta1, theta2 = np.random.randn(3).T\n",
    "sigmoid = lambda z: 1./(1+np.exp(-z))\n",
    "nepochs, eta = 10, 1e-2\n",
    "metrics = np.zeros(shape=(nepochs, 5))\n",
    "# Gradiente descendente (batch)\n",
    "for t in range(nepochs):\n",
    "    model_v = theta0 + theta1*X[valid_idx, 0] + theta2*X[valid_idx, 1]\n",
    "    model_t = theta0 + theta1*X[train_idx, 0] + theta2*X[train_idx, 1]\n",
    "    tmp = -2*(Y[train_idx] - sigmoid(model_t))*(1 - sigmoid(model_t))\n",
    "    theta0 -= ?\n",
    "    theta1 -= ?\n",
    "    theta2 -= ?\n",
    "    metrics[t, 0] = np.mean((Y[train_idx] - sigmoid(model_t))**2)\n",
    "    metrics[t, 1] = np.mean((Y[valid_idx] - sigmoid(model_v))**2)\n",
    "    metrics[t, 2:] = np.array([theta0, theta1, theta2])\n",
    "    if t % (nepochs/10) == 0:\n",
    "        print(t, metrics[t, :])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, figsize=(7, 5), tight_layout=True)\n",
    "ax[1].scatter(X[Y==0, 0], X[Y==0, 1], c='k', marker='x', label='perros')\n",
    "ax[1].scatter(X[Y==1, 0], X[Y==1, 1], c='k', marker='o', label='gatos')\n",
    "x_lim, y_lim = ax[1].get_xlim(), ax[1].get_ylim()\n",
    "x = np.linspace(x_lim[0], x_lim[1], num=10)\n",
    "y = np.linspace(y_lim[0], y_lim[1], num=10)\n",
    "X_plot, Y_plot = np.meshgrid(x, y)\n",
    "C = sigmoid(theta0 + theta1*X_plot + theta2*Y_plot)\n",
    "cf = ax[1].contourf(X_plot, Y_plot, C, zorder=-100, cmap=plt.cm.RdBu_r)\n",
    "plt.colorbar(cf, ax=ax[1])\n",
    "plt.legend();\n",
    "ax[0].plot(metrics[:, 0], 'k-');\n",
    "ax[0].plot(metrics[:, 1], 'k--');\n",
    "#ax[0].set_xscale('log')\n",
    "ax2 = ax[0].twinx()\n",
    "ax2.plot(metrics[:, 2:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modulo [`scipy.optimize`](https://docs.scipy.org/doc/scipy-1.3.0/reference/tutorial/optimize.html)\n",
    "\n",
    "La función [`minimize`](https://docs.scipy.org/doc/scipy-1.3.0/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) engloba una batería de optimizadores \n",
    "\n",
    "- Optimización sin restricciones \n",
    "    - usando gradientes\n",
    "        - CG: Gradiente conjugado. GD con paso adaptivo \n",
    "        - Newton-CG: Método de Newton con Hessiano\n",
    "        - BFGS (default): Método [Quasi-Newton](https://en.wikipedia.org/wiki/Quasi-Newton_method) con Hessiano inverso aproximado a cada paso\n",
    "    -  sin gradientes (para funciones no derivables o muy ruidosas)\n",
    "        - [Nelder-Mead](https://www.youtube.com/watch?v=HUqLxHfxWqU): Heurística basada en evaluaciones de la función objetivo\n",
    "        - [Powell](https://www.youtube.com/watch?v=4TYJGihyuDg): Búsqueda de linea en múltiples direcciones\n",
    "- Optimización con restricciones\n",
    "    - SLSQP: Sequential Least Squares Programming\n",
    "    - COBYLA: Constrained Optimization BY Linear Approximation \n",
    "- Optimización global: [`brute`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brute.html#scipy.optimize.brute)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo: \n",
    "\n",
    "Sea la función del manto sagrado. Encuentre el mínimo usando `optimize`\n",
    "\n",
    "¿Cúantos mínimos globales existen? ¿Qué ocurre si inicializa en cero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "a = np.linspace(-2, 2, num=100)\n",
    "b = np.linspace(-1, 1, num=100)\n",
    "A, B = np.meshgrid(x, y)\n",
    "C = (4 -2.1*A**2+A**4/3)*A**2 +A*B + (4*B**2 -4)*B**2\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(A, B, C, cmap=plt.cm.cividis);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "def f(x):\n",
    "    return 0.0\n",
    "\n",
    "x0 = 0.0\n",
    "res = scipy.optimize.minimize(f, x0, options={'disp':True})\n",
    "\n",
    "ax.scatter(res.x[0], res.x[1], f(res.x), s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 2: \n",
    "\n",
    "Regresión con elemento no lineal usando optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda z: 1/(1+np.exp(-z))\n",
    "\n",
    "# Hay que proporcionar una función de Python que evalue la función de costo\n",
    "def f(theta):\n",
    "    model = theta[0] + theta[1]*X[train_idx, 0] + theta[2]*X[train_idx, 1]\n",
    "    return np.mean((Y[train_idx] - sigmoid(model))**2)\n",
    "\n",
    "# Opcionalmente se puede proporcionar el gradiente (jac) y el Hessiano (hess)\n",
    "# de lo contrario se estiman numericamente\n",
    "def df(theta):\n",
    "    model = theta[0] + theta[1]*X[train_idx, 0] + theta[2]*X[train_idx, 1]\n",
    "    #print(sigmoid(model))\n",
    "    tmp = -2*(Y[train_idx] - sigmoid(model))*(1. - sigmoid(model))/len(train_idx)\n",
    "    return np.array([np.sum(tmp), np.sum(tmp*X[train_idx, 0]), np.sum(tmp*X[train_idx, 1])])\n",
    "\n",
    "# Valor inicial para los parámetros\n",
    "np.random.seed(0)\n",
    "theta0 = np.random.randn(3)\n",
    "res = scipy.optimize.minimize(f, theta0,\n",
    "                              method='CG', tol=1e-3, \n",
    "                              options={'disp': True, 'gtol': 1e-2})\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = res.x\n",
    "fig, ax = plt.subplots(1, figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], c='k', marker='x', label='perros')\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], c='k', marker='o', label='gatos')\n",
    "x_lim, y_lim = ax.get_xlim(), ax.get_ylim()\n",
    "x = np.linspace(x_lim[0], x_lim[1], num=20)\n",
    "y = np.linspace(y_lim[0], y_lim[1], num=20)\n",
    "X_plot, Y_plot = np.meshgrid(x, y)\n",
    "C = sigmoid(theta[0] + theta[1]*X_plot + theta[2]*Y_plot)\n",
    "cf = ax.contourf(X_plot, Y_plot, C, zorder=-100, cmap=plt.cm.RdBu_r)\n",
    "plt.colorbar(cf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización con restricciones usando SLSQP\n",
    "\n",
    "Sea la siguiente función de costo\n",
    "$$\n",
    "\\min f(x, y) = -(2xy+2x-x^2-2y^2) \n",
    "$$\n",
    "sujeta a \n",
    "$$\n",
    "x^3 - y = 0 ~\\wedge~y-(x-1)^4-2 \\geq 0 \n",
    "$$\n",
    "donde\n",
    "$$\n",
    "0.5\\leq x \\leq 1.5 ~\\wedge~ 1.5 \\leq y \\leq 2.5\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return -(2*x[0]*x[1] + 2*x[0] - x[0]**2 - 2*x[1]**2)\n",
    "\n",
    "cons = ({'type': 'eq',\n",
    "         'fun' : lambda x: np.array([x[0]**3 - x[1]]),\n",
    "         'jac' : lambda x: np.array([3.0*(x[0]**2.0), -1.0])},\n",
    "        {'type': 'ineq',\n",
    "         'fun' : lambda x: np.array([x[1] - (x[0]-1)**4 - 2])})\n",
    "\n",
    "bnds = ((0.5, 1.5), (1.5, 2.5))\n",
    "x0 = np.array([0, 1])\n",
    "res1 = scipy.optimize.minimize(f, x0, method='BFGS', \n",
    "                              options={'disp':True})\n",
    "res2 = scipy.optimize.minimize(f, x0, method='L-BFGS-B', bounds=bnds,\n",
    "                              options={'disp':True})\n",
    "res3 = scipy.optimize.minimize(f, x0, method='SLSQP', \n",
    "                              constraints=cons, bounds=bnds,\n",
    "                              options={'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 3, 100)\n",
    "y = np.linspace(0, 3, 100)\n",
    "A, B = np.meshgrid(x, y)\n",
    "C = f(np.vstack([A.ravel(), B.ravel()])).reshape((100,100))\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.contourf(A, B, C, );\n",
    "ax.plot(x, x**3, 'k--', lw=2)\n",
    "ax.plot(x, 2+(x-1)**4, 'k:', lw=2)\n",
    "ax.fill([0.5, 0.5, 1.5, 1.5], [2.5, 1.5, 1.5, 2.5], alpha=0.3)\n",
    "ax.axis([0, 3, 0, 3])\n",
    "ax.scatter(res1.x[0], res1.x[1], s=20, c='b');\n",
    "ax.scatter(res2.x[0], res2.x[1], s=20, c='g');\n",
    "ax.scatter(res3.x[0], res3.x[1], s=20, c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo \n",
    "\n",
    "Nuevamente el aire sucio italiano\n",
    "\n",
    "¿Podemos ajustar la frecuencia del modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -c https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip\n",
    "unzip AirQualityUCI.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"AirQualityUCI.csv\", sep=';', decimal=',')\n",
    "df = df[df.columns[:-2]] \n",
    "df.dropna(inplace=True)\n",
    "df[\"Date_time\"] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format=\"%d/%m/%Y %H.%M.%S\")\n",
    "df.drop([\"Date\", \"Time\"], axis=1, inplace=True)\n",
    "time, data = df.loc[(df[\"Date_time\"] < pd.to_datetime(\"03/24/2004 18:00:00\")) & (df[\"CO(GT)\"] > -200)][[\"Date_time\", \"CO(GT)\"]].values.T\n",
    "data = data.astype('float32')\n",
    "time_float = np.array([t.timestamp()/(24*3600) - time[0].timestamp()/(24*3600) for t in time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "mask = time_float <= 7\n",
    "ax.plot(time_float, data)\n",
    "\n",
    "def model(time, theta):\n",
    "    return theta[0] + theta[2]*np.cos(2.0*np.pi*time*theta[1]) + theta[3]*np.sin(2.0*np.pi*time*theta[1]) +\\\n",
    "+ theta[4]*np.cos(2.0*np.pi*time*theta[1]*2) + theta[5]*np.sin(2.0*np.pi*time*theta[1]*2) \n",
    "\n",
    "def residual(theta, *args):\n",
    "    time, data = args\n",
    "    return np.sum((data - model(time, theta))**2)\n",
    "\n",
    "theta0 = np.random.rand(6)\n",
    "res = scipy.optimize.minimize(residual, theta0, method='Powell',\n",
    "                              args=(time_float[mask], data[mask]))\n",
    "ax.plot(time_float, model(time_float, res.x))\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consideraciones prácticas\n",
    "- Escoger apropiadamente nuestro optimizador\n",
    "- Ser cuidadoso con las soluciones iniciales\n",
    "- Revisar la convergencia de los algoritmos\n",
    "\n",
    "### Apéndices\n",
    "\n",
    "- [Raices de una función](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.root.html#scipy.optimize.root)\n",
    "- [Mínimos cuadrados no lineal y con cotas](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares) \n",
    "- [Programación lineal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html#scipy.optimize.linprog)\n",
    "- [Comparativa detallada entre los distintos métodos de optimización](https://scipy-lectures.org/advanced/mathematical_optimization/index.html) [2](http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
