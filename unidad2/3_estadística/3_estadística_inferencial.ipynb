{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo, Markdown, SVG\n",
    "from functools import partial\n",
    "YouTubeVideo_formato = partial(YouTubeVideo, modestbranding=1, disablekb=0,\n",
    "                               width=640, height=360, autoplay=0, rel=0, showinfo=0)\n",
    "\n",
    "display(Markdown(filename='../../preamble.md'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "from scipy.special import erf\n",
    "gaussian_pdf = lambda x, mu=0, s=1: np.exp(-0.5*(x-mu)**2/s**2)/(s*np.sqrt(2*np.pi))\n",
    "gaussian_cdf = lambda x, mu=0, s=1: 0.5 + 0.5*erf((x-mu)/(s*np.sqrt(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de aplicar las herramientas de estadísticas descriptiva para entender mejor nuestros datos podemos enfocarnos en\n",
    "\n",
    "# Inferencia estadística\n",
    "\n",
    "La inferencia busca\n",
    "\n",
    "> Extraer conclusiones a partir de hechos a través de un método o premisa\n",
    "\n",
    "En el caso particular de la inferencia estadística tenemos que\n",
    "\n",
    "- Hechos: Datos\n",
    "- Premisa: Modelo probabilístico\n",
    "- Conclusión: Una cantidad no observada que es interesante\n",
    "\n",
    "Y lo que buscamos es\n",
    "\n",
    "> Cuantificar la incerteza de la conclusión dado los datos y el modelo \n",
    "\n",
    "La inferencia estadística puede dividirse en los siguientes tres niveles\n",
    "1. Ajustar un modelo a nuestros datos: **Máxima verosimilitud**\n",
    "1. Verificar que el modelo sea confiable: **Intervalo de confianza**\n",
    "1. Responder una pregunta usando el modelo: **Test de hipótesis**\n",
    "\n",
    "A continuación estudiaremos cada una de estas herramientas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de modelos paramétricos\n",
    "\n",
    "Se refiere a aquellos modelos que explicitan una distribución de probabilidad\n",
    "\n",
    "### Ejemplo: La pesa defectuosa\n",
    "\n",
    "- Mi pesa está defectuosa\n",
    "- Luego de comer mido $M$ veces mi peso obteniendo un conjunto de observaciones $\\{x_i\\}$\n",
    "- El objetivo es encontrar mi peso real $\\hat x$. \n",
    "\n",
    "Puedo modelar mis observaciones como\n",
    "$$\n",
    "x_i = \\hat x + \\varepsilon_i\n",
    "$$\n",
    "donde $\\varepsilon_i$ corresponde al ruido del instrumento\n",
    "\n",
    "Asumamos que $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$\n",
    "\n",
    "es decir que el ruido es\n",
    "- Independiente\n",
    "- Gaussiano\n",
    "- Con media cero y varianza $\\sigma_\\varepsilon^2$\n",
    "\n",
    "Con esto podemos escribir la probabilidad de observar $x_i$ dado un cierto valor $\\hat x$ como\n",
    "\n",
    "$$\n",
    "p(x_i|\\hat x) = \\mathcal{N}(\\hat x, \\sigma_\\varepsilon^2)\n",
    "$$\n",
    "\n",
    "> Mi modelo tiene dos parámetros: $\\hat x$ y $\\sigma_\\varepsilon^2$\n",
    "\n",
    "Si **asumimos** que las observaciones son independientes e identicamente distribuidas (iid), podemos escribir la probabilidad de observar el conjunto completo como \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x_1, x_2, \\ldots, x_M| \\hat x) &= \\prod_{i=1}^M p(x_i|\\hat x) \\nonumber \\\\\n",
    "&= \\prod_{i=1}^M  \\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}  \\exp \\left ( - \\frac{1}{2\\sigma_\\varepsilon^2} (x_i - \\hat x)^2 \\right) = \\mathcal{L}(\\hat x)  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Esto se conoce como la **verosimilitud** de los parámetros dado el conjunto de observaciones\n",
    "\n",
    "Podemos buscar los parámetros que hacen mi modelo más \"creible\" **maximizando la verosimilitud**\n",
    "\n",
    "\n",
    "$$\n",
    "\\max_{\\hat x}  \\mathcal{L}(\\hat x) = \\max_{\\hat x}  \\log \\mathcal{L}(\\hat x) = -\\frac{1}{2} \\log(2\\pi\\sigma_\\varepsilon^2)M - \\frac{1}{2\\sigma_\\varepsilon^2} \\sum_{i=1}^M (x_i - \\hat x )^2\n",
    "$$\n",
    "\n",
    "Como el logaritmo es monotónico conviene maximizar el log de la verosimilitud\n",
    "\n",
    "Si derivamos e igualamos a cero\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\frac{1}{2\\sigma_\\varepsilon^2} \\sum_{i=1}^M 2(x_i - \\hat x ) &= 0 \\nonumber \\\\\n",
    "\\hat x &= \\frac{1}{M} \\sum_{i=1}^M x_i \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> El estimador de máxima verosimilitud para la media de una Gaussiana es el clásico promedio muestral\n",
    "\n",
    "Si hicieramos lo mismo para la varianza encontraríamos\n",
    "\n",
    "$$\n",
    "\\sigma_\\epsilon^2 = \\frac{1}{M} \\sum_{i=1}^M (x_i - \\hat x)^2\n",
    "$$\n",
    "\n",
    "> Que es la varianza muestreal (sesgada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimación de máxima verosimilitud\n",
    "\n",
    "El procedimiento que acabamos de ver se llama *maximum likelihood estimation* (MLE)\n",
    "\n",
    "Procedimiento\n",
    "\n",
    "1. Definir los supuestos del problema, el modelo (distribución) y sus parámetros $\\theta$\n",
    "1. Escribir el **logaritmo de la verosimilud** de los parámetros $\\log \\mathcal{L}(\\theta)$\n",
    "1. Encontrar $\\theta$ que maximiza \n",
    "$$\n",
    "\\hat \\theta = \\text{arg}\\max_\\theta \\log \\mathcal{L}(\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "Las distribuciones de  `scipy.stats` tienen los métodos\n",
    "- `fit (data)` Ajusta una distribución continua a datos usando MLE\n",
    "- `expect (func)` Valor esperado de una función c/r a la distribución\n",
    "- `interval (alpha)` Cotas para el intervalo que contiene un porcentaje $\\alpha$ de la distribución\n",
    "\n",
    "Para distribuciones complicadas sin solución analítica se pueden usar métodos iterativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Ajustando y comparando distintas distribuciones\n",
    "\n",
    "Observemos la siguiente distribución, ¿Qué características resaltan? ¿Qué distribución sería apropiado ajustar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "data_set = datasets.load_breast_cancer()\n",
    "x, y = data_set['data'][:, 0], data_set['target']\n",
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "ax.hist(x, bins=20, density=True)\n",
    "ax.set_xlabel('Tamaño del nodulo');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos probar varias distribuciones y observar el resultado\n",
    "\n",
    "¿Cómo medir la bondad del ajuste?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(np.amin(x), np.amax(x), num=500)\n",
    "dist = scipy.stats.uniform\n",
    "params = dist.fit(x)\n",
    "print(dist.name)\n",
    "print(params)\n",
    "p_plot = dist(*params[:-2], loc=params[-2], scale=params[-1]).pdf(x_plot)\n",
    "ax.plot(x_plot, p_plot, label=dist.name)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bondad de ajuste\n",
    "\n",
    "Podemos usar el test chi-cuadrado, el [test de Akaike](https://en.wikipedia.org/wiki/Akaike_information_criterion), el test no-paramétrico de Kolmogorov-Smirnov (KS) o gráficos QQ para medir que tan bien se ajusta nuestra distribución teórica a los datos\n",
    "\n",
    "El test KS nos permite comparar que tan distinta es una distribución continua empírica de una teórica comparando sus CDFs: [`scipy.stats.kstest`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html)\n",
    "\n",
    "- Require que los datos estén estandarizados \n",
    "\n",
    "$$\n",
    "Z = \\frac{X - \\mu_X}{\\sigma_X}\n",
    "$$\n",
    "\n",
    "- Retorna un estadístico: Mientras más cerca a cero, mejor es el ajuste\n",
    "- También retorna un p-value: Si es menor que $\\alpha$ entonces rechazo la hipótesis nula de que las distribuciones son iguales con un $1-\\alpha$ de confianza. Veremos test de hipótesis más adelante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_std = (x-np.mean(x))/np.std(x)\n",
    "ks_res = []\n",
    "for dist in [scipy.stats.norm, scipy.stats.lognorm, scipy.stats.beta, scipy.stats.gamma, scipy.stats.genextreme]:    \n",
    "    params = dist.fit(x_std)\n",
    "    fitted_dist = dist(*params[:-2], loc=params[-2], scale=params[-1])\n",
    "    ks_res.append(scipy.stats.kstest(rvs=x_std, cdf=fitted_dist.cdf))\n",
    "    print(dist.name)\n",
    "    print(ks_res[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respondiendo preguntas usando nuestro modelo\n",
    "\n",
    "1. ¿Cuál es la media de la distribución? \n",
    "1. ¿Cúal es la probabilidad de que el \"tamaño del nódulo\" sea mayor o igual que 20?\n",
    "1. ¿Cúal es el tamaño de nódulo que acumula el 90% de la distribución?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "ax.hist(x, bins=20, density=True)\n",
    "ax.set_xlabel('Tamaño del nodulo')\n",
    "dist = scipy.stats.genextreme\n",
    "params = dist.fit(x)\n",
    "fitted_dist = dist(*params[:-2], loc=params[-2], scale=params[-1])\n",
    "p_plot = fitted_dist.pdf(x_plot)\n",
    "ax.plot(x_plot, p_plot, c='k', lw=2);\n",
    "ax.fill_between(x_plot[x_plot>=20], np.zeros_like(x_plot[x_plot>=20]), p_plot[x_plot>=20], \n",
    "                alpha=0.5, color='k', zorder=20)\n",
    "\n",
    "# P1: Usando el atributo mean() \n",
    "display(fitted_dist.mean())\n",
    "# P2: Usando el atributo CDF\n",
    "display(1. - fitted_dist.cdf(20))\n",
    "# P3: Usando el atributo PPF\n",
    "display(fitted_dist.ppf(0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-values: https://ipython-books.github.io/72-getting-started-with-statistical-hypothesis-testing-a-simple-z-test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "ax.hist(x[y==0], bins=30, range=(5, 30), density=False, alpha=0.75, label='maligno')\n",
    "ax.hist(x[y==1], bins=30, range=(5, 30), density=False, alpha=0.75, label='benigno')\n",
    "ax.set_xlabel('Tamaño del nodulo');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Ajustando una recta\n",
    "\n",
    "Sea el siguiente dataset de [consumo de helados](https://forge.scilab.org/index.php/p/rdataset/source/tree/master/csv/Ecdat/Icecream.csv) en USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc -c https://forge.scilab.org/index.php/p/rdataset/source/file/master/csv/Ecdat/Icecream.csv\n",
    "df = pd.read_csv('Icecream.csv', header=0, index_col=0)\n",
    "df.columns = ['consumo', 'ingreso', 'precio', 'temperatura']\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True, sharey=True)\n",
    "for i, col in enumerate(df.columns[1:]):\n",
    "    ax[i].scatter(df[col], df[\"consumo\"])\n",
    "    ax[i].set_xlabel(col)\n",
    "ax[0].set_ylabel(df.columns[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $y$ el consumo y $x$ la temperatura.\n",
    "\n",
    "Asumiendo errores gaussianos\n",
    "\n",
    "$$\n",
    "y_i = \\hat y_i + \\epsilon_i, \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2),\n",
    "$$\n",
    "\n",
    "un modelo lineal de dos parámetros,\n",
    "\n",
    "$$\n",
    "\\hat y_i = \\theta_0 + \\theta_1 x_i\n",
    "$$\n",
    "\n",
    "y observaciones iid, podemos estimar $\\theta$ buscando la máxima verosimilitud del modelo\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^M (y_i - \\theta_0 - \\theta_1 x_i)^2\n",
    "$$\n",
    "\n",
    "donde \n",
    "$$\n",
    "\\sum_i y_i  - M\\theta_0 - \\theta_1  \\sum_i x_i = 0 \\rightarrow \\theta_0 = \\bar y - \\theta_1 \\bar x\n",
    "$$\n",
    "$$\n",
    "\\sum_i y_i x_i - \\theta_0 \\sum_i x_i - \\theta_1 \\sum_i x_i^2 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_1 = \\frac{\\sum_i x_i y_i - M \\bar x \\bar y}{\\sum_i x_i^2 - M \\bar x^2} = \\frac{ \\sum_i (y_i - \\bar y)(x_i - \\bar x)}{\\sum_i (x_i - \\bar x)^2} = \\frac{\\text{COV}(x, y)}{\\text{Var}(x)}\n",
    "$$\n",
    "\n",
    "la fuerza de la correlación se mide con \n",
    "\n",
    "$$\n",
    "r^2 = 1 - \\frac{\\sum_i ( y_i - \\hat y_i)^2}{\\sum_i ( y_i - \\bar y)^2} = 1 - \\frac{\\frac{1}{M} \\sum_i (y_i - \\hat y_i)^2}{\\text{Var}(y)} = \\frac{\\text{COV}^2(x, y)}{\\text{Var}(x) \\text{Var}(y)}\n",
    "$$\n",
    "\n",
    "donde $r = \\frac{\\text{COV}(x, y)}{\\sqrt{\\text{Var}(x) \\text{Var}(y)}} \\in [-1, 1]$ se conoce como [coeficiente de correlación de Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n",
    "\n",
    "Podemos usar pandas para calcular $r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir que existe \n",
    "- una correlación positiva alta entre consumo y temperatura\n",
    "- una correlación negativa moderada entre consumo y precio\n",
    "- una correlación cercana a cero entre consumo e ingreso\n",
    "\n",
    "Podemos usar la función [`scipy.stats.linregress`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html) para recuperar $\\theta_0$, $\\theta_1$ y $r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True, sharey=True)\n",
    "ax[0].set_ylabel(df.columns[0]);\n",
    "\n",
    "for i, col in enumerate(df.columns[1:]):\n",
    "    ax[i].scatter(df[col], df[\"consumo\"])    \n",
    "    res = scipy.stats.linregress(df[col], df[\"consumo\"])\n",
    "    ax[i].set_title(\"r: {0:0.5f}\".format(res.rvalue))\n",
    "    x_plot = np.linspace(np.amin(df[col]), np. amax(df[col]), num=100)\n",
    "    ax[i].plot(x_plot, res.slope*x_plot + res.intercept, lw=4, alpha=0.5, c='k');\n",
    "    ax[i].set_xlabel(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respondiendo preguntas con nuestro modelo\n",
    "\n",
    "1. ¿Qué tan confiables son los valores de $\\theta_0$, $\\theta_1$ y $r$?\n",
    "1. ¿Son las correlaciones encontradas significativas?\n",
    "\n",
    "\n",
    "Dos caminos para responder\n",
    "- Pruebas paramétricas\n",
    "- Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de hipótesis\n",
    "\n",
    "Se aplica un tratamiento nuevo a una muestra de la población \n",
    "- ¿Es el tratamiento efectivo?\n",
    "- ¿Existe una diferencia entre los que tomaron el tratamiento y los que no?\n",
    "\n",
    "El test de hipótesis es un procedimiento estadístico para comprobar si el resultado de un experimento es significativo en la población\n",
    "\n",
    "Para esto formulamos dos escenarios cada uno con una hipótesis asociada\n",
    "- Hipótesis nula ($H_0$): El experimento no produjo diferencia. El experimento no tuvo efecto. Las observaciones son producto del azar\n",
    "- Hipótesis alternativa ($H_A$): Usualmente el complemento de $H_0$\n",
    "\n",
    "El test de hipótesis se diseña para medir que tan fuerte es la evidencia **en contra** de la hipótesis nula\n",
    "\n",
    "El algoritmo general es de un test de hipótesis:\n",
    "1. Definimos $H_0$ y $H_A$\n",
    "1. Definimos un estadístico $T$\n",
    "1. Asumimos una distribución para $T$ dado que $H_0$ es cierto\n",
    "1. Seleccionamos un nivel de significancia $\\alpha$ \n",
    "1. Calculamos el $T$ para nuestros datos $T_{data}$\n",
    "1. Calculamos el **p-value**: Probabilidad de observar un valor de $T$ más extremo que el observado \n",
    "    - Si nuestro test es de una cola:\n",
    "        - Superior: $p = P(T>T_{data})$\n",
    "        - Inferior: $p = P(T<T_{data})$\n",
    "    - Si nuestro test es dos colas: $p = P(T>T_{data}) + P(T<T_{data})$\n",
    "\n",
    "Finalmente:\n",
    "\n",
    "`Si`  $p < \\alpha$\n",
    "    \n",
    "    Rechazamos la hipótesis nula con confianza (1-\\alpha)\n",
    "De lo contrario:\n",
    "    \n",
    "    No hay suficiente evidencia para rechazar la hipótesis nula\n",
    "    \n",
    "**Escenarios:**\n",
    "- Rechazamos $H_0$ cuando en realidad era cierta (falso positivo): **Error tipo I**\n",
    "    - Ocurre con probabilidad $\\alpha$ (es controlable)\n",
    "- No rechazamos $H_0$ cuando en realidad era falsa (falso negativo): **Error tipo II**\n",
    "    - Ocurre con probabilidad $\\beta$\n",
    "\n",
    "\n",
    "**Errores de interpretación comunes:**\n",
    "\n",
    "- El p-value **no es** la probabilidad de que $H_0$ sea cierta\n",
    "\n",
    "    $p = P(T> T_{data} | H_0) \\neq P(H_0 | T> T_{data})$\n",
    "    \n",
    "- No rechazar $H_0$ no es lo mismo que aceptarla\n",
    "- Rechazar $H_0$ no es lo mismo que aceptar $H_A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: t-test de una muestra \n",
    "\n",
    "Para un conjunto de $M$ observaciones iid $X = {x_1, x_2, \\ldots, x_M}$ con media muestral $\\bar x = \\sum_{i=1}^M x_i$ \n",
    "\n",
    "El t-test de una muestra es un test de hipótesis que busca probar si $\\bar x$ es significativamente distinta de la **media poblacional** $\\mu$, en el caso de que **no conocemos la varianza poblacional** $\\sigma^2$\n",
    "\n",
    "Las hipótesis son\n",
    "\n",
    "- $H_0:$ $\\bar x = \\mu$\n",
    "- $H_A:$ $\\bar x \\neq \\mu$ (dos colas)\n",
    "\n",
    "El estadístico de prueba es \n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar x - \\mu}{\\hat \\sigma /\\sqrt{M-1}}\n",
    "$$\n",
    "\n",
    "donde $\\hat \\sigma = \\sqrt{ \\frac{1}{M} \\sum_{i=1}^M (x_i - \\bar x)^2}$ es la desviación estándar muestral (sesgada)\n",
    "\n",
    "Si asumimos que $\\bar x$ se distribuye $\\mathcal{N}(\\mu, \\frac{\\sigma^2}{M})$ entonces\n",
    "$t$ que se distribuye [t-student](https://en.wikipedia.org/wiki/Student%27s_t-distribution) con $M-1$ grados de libertad\n",
    "\n",
    "- Para muestras iid y $M$ grande el supuesto se cumple por teorema central del límite\n",
    "- Si $M$ es pequeño debemos verificar la normalidad de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-test para probar que la regresión es significativa\n",
    "\n",
    "En este caso queremos hacer el siguiente test\n",
    "- $H_0:$ La pendiente es nula $\\theta_1= 0$ \n",
    "- $H_A:$ La pendiente no es nula: $\\theta_1\\neq 0$ (dos colas)\n",
    "\n",
    "Asumimos que $\\theta_1$ es normal pero que desconocemos su varianza \n",
    "\n",
    "Entonces se puede formular el siguiente estadístico de prueba \n",
    "\n",
    "$$\n",
    "t = \\frac{(\\theta_1-\\theta^*) }{\\text{SE}_{\\theta_1}/\\sqrt{M-2}} = \\frac{ r\\sqrt{M-2}}{\\sqrt{1-r^2}},\n",
    "$$\n",
    "\n",
    "donde en este caso particular se usa $\\theta^*=0$ y $\\text{SE}_{\\theta_1} = \\sqrt{ \\frac{\\frac{1}{M} \\sum_i (y_i - \\hat y_i)^2}{\\text{Var}(x)}}$\n",
    "\n",
    "$t$ se [t-student](https://en.wikipedia.org/wiki/Student%27s_t-distribution) con dos grados de libertad (modelo de dos parámetros) \n",
    "\n",
    "La función `scipy.stats.linregress` implementa este test y retorna los p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True, sharey=True)\n",
    "t = np.linspace(-7, 7, num=1000)\n",
    "M = df.shape[0]\n",
    "ax[0].set_ylabel(df.columns[0]);\n",
    "\n",
    "for i, col in enumerate(df.columns[1:]):\n",
    "    res = scipy.stats.linregress(df[col], df[\"consumo\"])\n",
    "    t_data = res.rvalue*np.sqrt(M-2)/np.sqrt(1.-res.rvalue**2)\n",
    "    display(res)\n",
    "    ax[i].set_title(\"t_data: {0:0.5f}\".format(t_data))\n",
    "    ax[i].set_xlabel(col)\n",
    "    dist = scipy.stats.t(loc=0, scale=1, df=M-2)\n",
    "    p = dist.pdf(t)\n",
    "    ax[i].plot(t, p)\n",
    "    ax[i].plot([dist.ppf(0.025)]*2, [0, np.amax(p)], 'k--')\n",
    "    ax[i].plot([dist.ppf(0.975)]*2, [0, np.amax(p)], 'k--')\n",
    "    ax[i].plot([t_data]*2, [0, np.amax(p)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué podemos decir de las correlaciones con el consumo de helados?\n",
    "\n",
    "Para temperatura y considerando $\\alpha = 0.05$:\n",
    "\n",
    "> $\\text{p-value} = 4.7892e-07 < \\alpha$\n",
    "\n",
    "Por ende rechazamos $H_0$ con 95% de confianza\n",
    "\n",
    "En cambio para ingreso y precio no podemos rechazar $H_0$\n",
    "\n",
    "### Reflexión\n",
    "- ¿Cómo se escogen el estadístico y la distribución de prueba? Depende del problema \n",
    "- ¿Qué prueba puedo usar si quiero hacer regresión lineal multivariada? [ANOVA](https://pythonfordatascience.org/anova-python/)\n",
    "- ¿Qué pasa si mis datos tienen una relación que no es lineal? La prueba no es confiable\n",
    "- ¿Qué pasa si $\\theta_1$ no es normal? La prueba no es confiable\n",
    "- ¿Qué pasa si el ruido no es Gaussiano? La prueba no es confiable\n",
    "- ¿Qué pasa si el ruido es Gaussiano pero su varianza cambia en el tiempo? La prueba no es confiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba no-paramétrica: *Bootstrap*\n",
    "\n",
    "Podemos estimar la incerteza de un estimador de forma no-paramétrica usando **muestreo tipo *bootstrap***\n",
    "\n",
    "Tomamos nuestros conjunto de datos de tamaño $M$ y creamos $T$ nuevos conjuntos que \"se le parezcan\" \n",
    "\n",
    "Luego se calcula el valor del estimador en esos nuevos conjuntos\n",
    "\n",
    "*Bootstrap* está basado en la Ley de los grandes números y Teorema central del límite \n",
    "\n",
    "<img src=\"https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2016/10/bootstrap-sample.png\">\n",
    "\n",
    "- En este caso supondremos independencia: **Muestreo con reemplazo de tamaño $M$**\n",
    "- También existe bootstrap basado en los residuos y bootstrap dependiente\n",
    "\n",
    "Podemos usar la función [`numpy.random.choice`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html) para generar los nuevos índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(2, 2, figsize=(6, 5), tight_layout=True)\n",
    "x, y = df[\"temperatura\"].values, df[\"consumo\"].values\n",
    "x_plot = np.linspace(np.amin(x), np. amax(x), num=100)\n",
    "res_all = scipy.stats.linregress(x, y)\n",
    "M = len(x)\n",
    "\n",
    "def update_plot(T):\n",
    "    [ax_.cla() for ax_ in ax.ravel()]\n",
    "    ax[0, 0].scatter(x, y, zorder=100, s=10)\n",
    "    np.random.seed(0)\n",
    "    param = np.zeros(shape=(T, 3))\n",
    "    lines = []\n",
    "    for t in range(T):\n",
    "        bootstrap_idx = np.random.choice(np.arange(len(x)), size=len(x), replace=True)\n",
    "        res = scipy.stats.linregress(x[bootstrap_idx], y[bootstrap_idx])\n",
    "        lines.append(x_plot*res.slope + res.intercept)\n",
    "        #ax[0, 0].plot(x_plot, x_plot*res.slope + res.intercept, alpha=0.05, c='k')\n",
    "        param[t, :] = [res.intercept, res.slope, res.rvalue]\n",
    "    ax[0, 0].plot(x_plot, x_plot*res_all.slope + res_all.intercept, alpha=1, c='r')\n",
    "    lines = np.stack(lines)\n",
    "    ax[0, 0].fill_between(x_plot, np.mean(lines, axis=0)-3*np.std(lines, axis=0), \n",
    "                          np.mean(lines, axis=0)+3*np.std(lines, axis=0), color='r', alpha=0.25)\n",
    "    hist_val, hist_lim, _ = ax[0, 1].hist(param[:, 2], bins=15, density=True)\n",
    "    ax[0, 1].plot([res_all.rvalue]*2, [0, np.max(hist_val)], c='r')\n",
    "    ax[0, 1].set_xlabel('r')\n",
    "    display(\"Intervalo de confianza al 95% de r {}\".format(np.percentile(param[:, 2], [2.5, 97.5])))\n",
    "    hist_val, hist_lim, _ = ax[1, 0].hist(param[:, 0], bins=15, density=True)\n",
    "    ax[1, 0].plot([res_all.intercept]*2, [0, np.max(hist_val)], c='r')\n",
    "    ax[1, 0].set_xlabel(r'$\\theta_0$')\n",
    "    display(\"Intervalo de confianza al 95% de theta_0 {}\".format(np.percentile(param[:, 0], [2.5, 97.5])))\n",
    "    hist_val, hist_lim, _ = ax[1, 1].hist(param[:, 1], bins=15, density=True)\n",
    "    ax[1, 1].plot([res_all.slope]*2, [0, np.max(hist_val)], c='r')\n",
    "    ax[1, 1].set_xlabel(r'$\\theta_1$')\n",
    "    display(\"Intervalo de confianza al 95% de theta_1 {}\".format(np.percentile(param[:, 1], [2.5, 97.5])))\n",
    "\n",
    "widgets.interact(update_plot, T=SelSlider_nice(options=[10, 20, 50, 100, 200, 500], value=50));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\theta_1=0$ no está en el intervalo de confianza por lo que podemos rechazar $H_0$\n",
    "- Notamos que $\\theta_0$ y $\\theta_1$ son asintóticamente normales. El supuesto del t-test estaba bien!\n",
    "- Si hubieramos hecho un t-test sobre $r$ no sería valido (no normalidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más sobre [*bootstrap* y regresión lineal](https://www.stat.cmu.edu/~cshalizi/402/lectures/08-bootstrap/lecture-08.pdf) [aquí](http://homepage.divms.uiowa.edu/~rdecook/stat3200/notes/bootstrap_4pp.pdf) y [acá](https://www.sagepub.com/sites/default/files/upm-binaries/21122_Chapter_21.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm Icecream.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Regresión lineal multivariada y mínimos cuadrados\n",
    "\n",
    "Queremos ajustar un modelo lineal de $N$ parámetros a un conjunto de $M$ observaciones ruidosas\n",
    "\n",
    "Podemos modelar \n",
    "\n",
    "$$\n",
    "y_i = \\Phi(x_i) \\theta + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "donde $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$\n",
    "\n",
    "Si asumimos que los datos son iid podemos escribir el logaritmo de la verosimilitud como\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(y_1, y_2, \\ldots, y_M| \\theta) &= \\log \\prod_{i=1}^M p(y_i|\\theta) \\nonumber \\\\\n",
    "&=  -\\frac{1}{2} \\log(2\\pi\\sigma_\\varepsilon^2) M - \\frac{1}{2\\sigma_\\varepsilon^2} \\sum_{i=1}^M (y_i - \\Phi(x_i) \\theta)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "el primer término no depende de $\\theta$ y el segundo es negativo\n",
    "\n",
    "El resultado de maximizar la log verosimilitud es equivalente a\n",
    "$$\n",
    "\\min_\\theta \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^M (y_i - \\Phi(x_i) \\theta)^2\n",
    "$$\n",
    "\n",
    "Que corresponde al problema de **mínimos cuadrados**\n",
    "\n",
    "#### Reflexionemos\n",
    "¿Qué estamos asumiendo cuando usamos mínimos cuadrados?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para leer y reflexionar: \n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S0167715218300737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
