{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "from scipy.special import erf\n",
    "gaussian_pdf = lambda x, mu=0, s=1: np.exp(-0.5*(x-mu)**2/s**2)/(s*np.sqrt(2*np.pi))\n",
    "gaussian_cdf = lambda x, mu=0, s=1: 0.5 + 0.5*erf((x-mu)/(s*np.sqrt(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mundo de datos\n",
    "\n",
    "Los avances tecnológicos recientes nos permiten **medir, almacenar y enviar** datos de toda índole\n",
    "\n",
    "- Operaciones industriales\n",
    "- Comercio\n",
    "- Entretenimiento \n",
    "- Datos públicos y gubernamentales\n",
    "- Datos médicos\n",
    "- Ciencia: Genómica, Astronomía, Simulaciones, etc\n",
    "- Vehículos autónomos\n",
    "- Internet de las cosas\n",
    "\n",
    "> Los datos crudos tienen poco valor, necesitamos extraer información a partir de los datos\n",
    "\n",
    "- ¿Cómo se comportan mis datos? ¿Cúales son las observaciones más cómunes? ¿Qué datos son más relevantes?\n",
    "- ¿Cúal es la diferencia entre dos muestras? ¿Son las diferencias que observo reales o productos del ruido?\n",
    "- ¿Qué tan probable es que ocurra el suceso $X$? \n",
    "- ¿Qué tan arriesgado es tomar la decisión $Y$?\n",
    "\n",
    "> La **estadística** nos da herramientas para entender los procesos y tomar decisiones\n",
    "\n",
    "\n",
    "<img src=\"https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F1*ufWDxL-5ogd22Rg_37rakw.png&f=1\">\n",
    "\n",
    "\n",
    "En esta clase aprenderemos a usar [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html) y [`numpy.random`](https://docs.scipy.org/doc/numpy/reference/routines.random.html) para resolver problemas de **inferencia estadística**\n",
    "\n",
    "Pero antes, algunos fundamentos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentos: Incerteza\n",
    "\n",
    "Fuentes de incerteza:\n",
    "1. Estocasticidad inherente: Sistemas con [dinámicas aleatorias](https://en.wikipedia.org/wiki/Uncertainty_principle)\n",
    "1. Observación incompleta: Sistema determinista que parece estocástico [cuando no se observa completamente](https://en.wikipedia.org/wiki/Monty_Hall_problem)\n",
    "1. Modelamiento incompleto: Los [supuestos y aproximaciones del sistema](https://en.wikipedia.org/wiki/Discretization) introducen incerteza \n",
    "\n",
    "¿Cómo representamos la incerteza?\n",
    "\n",
    "> Teoría de probabilidades\n",
    "\n",
    "Nos proporciona reglas formales para determinar la verosimilitud de una proposición versus otras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentos: Variables aleatorias \n",
    "\n",
    "- **V.A.** Es una variable que, al ser observada, puede tomar diferentes valores\n",
    "\n",
    "    - La denotamos como $X$\n",
    "    \n",
    "    - Sus observaciones (realizaciones) son $x\\sim X$\n",
    "    \n",
    "    - Sus realizaciones tienen un dominio $x \\in \\mathcal{X}$\n",
    "    \n",
    "    - La probabilidad de observar $x$ es $P(X=x)$\n",
    "    \n",
    "    - Puede ser discreta o continua\n",
    "    \n",
    "\n",
    "    \n",
    "- El comportamiento de $X$ está dictado por \n",
    "    - Función de masa de probabilidad (para $X$ discreta)\n",
    "    \n",
    "    $P(X=x) \\in [0, 1]$\n",
    "    \n",
    "    $\\sum_{x\\in\\mathcal{X}} P(X=x) = 1$    \n",
    "    \n",
    "    - Función de densidad de probabilidad (para $X$ continua)\n",
    "    \n",
    "    $f(x) \\geq 0$\n",
    "    \n",
    "    $\\int_{x\\in\\mathcal{X}} f(x) \\,dx = 1$, \n",
    "    \n",
    "    $P(a\\leq X \\leq b) = F(b) - F(a) = \\int_{a}^{b} f(x) \\,dx$\n",
    "    \n",
    "        - Función de densidad acumulada: \n",
    "        \n",
    "        $F(a)  = \\int_{-\\infty}^{a} f(x) \\,dx$\n",
    "        \n",
    "        \n",
    "> **Importante:** Sólo en el caso discreto la distribución puede interpretarse como probabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "dt=1e-4; x = np.arange(-5, 5, step=dt)\n",
    "\n",
    "def update_plot(x_r):\n",
    "    xi, xf = x_r\n",
    "    for axis in ax:\n",
    "        axis.cla(); \n",
    "        axis.set_xlim([-5, 5]);\n",
    "    ax[0].plot(x, gaussian_pdf(x)); ax[1].plot(x, gaussian_cdf(x));\n",
    "    xrange = np.arange(xi, xf, step=dt)\n",
    "    ax[0].fill_between(xrange, 0, gaussian_pdf(xrange), alpha=0.5)\n",
    "    ax[1].scatter([xi, xf], [gaussian_cdf(xi), gaussian_cdf(xf)], s=100, c='k', zorder=100)\n",
    "    ax[1].text(xi+0.5, gaussian_cdf(xi), \"Init\"); ax[1].text(xf+0.5, gaussian_cdf(xf), \"End\")\n",
    "    ax[0].set_title(\"$\\int_{x_i}^{x_f} f(x) dx$ = %0.4f\" %(np.sum(gaussian_pdf(xrange))*dt))\n",
    "    area = gaussian_cdf(xf) - gaussian_cdf(xi)\n",
    "    ax[1].set_title(\"$F(x_f) - F(x_i)$ = %0.4f\" %(area if area >= 0 else 0))\n",
    "\n",
    "widgets.interact(update_plot, \n",
    "         x_r=widgets.FloatRangeSlider(description=r\"$x_i, x_f$\", min=-5, max=5, value=[-1, 1]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilidad conjunta, marginal y condicional\n",
    "\n",
    "La probabilidad de dos eventos $X=x$ e $Y=y$ se caracteriza con la distribución conjunta $P(X, Y)$\n",
    "\n",
    "A partir de la conjunta se pueden obtener la probabilidad marginal de $X$ (o de $Y$)\n",
    "\n",
    "$$\n",
    "P(X=x) = \\sum_{y \\in \\mathcal{Y}} P(X=x, Y=y) \n",
    "$$\n",
    "\n",
    "Usando la conjunta y las marginales podemos obtener las probabilidades condicionales\n",
    "\n",
    "$$\n",
    "P(Y=y|X=x) = \\frac{P(X=x, Y=y)}{P(X=x)}\n",
    "$$\n",
    "\n",
    "(ssi $P(X=x) \\neq 0$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "x = np.arange(-4, 5, 1)\n",
    "y = np.arange(-4, 5, 1)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "Z[-3, 2:-2] = 1\n",
    "Z[2, 2:-2] = 1\n",
    "Z[2:-2, 4] = 1\n",
    "Z = Z/np.sum(Z)\n",
    "\n",
    "ax.bar(x, np.sum(Z, axis=1), zdir='x', zs=-4)\n",
    "ax.bar(y, np.sum(Z, axis=0), zdir='y', zs=5)\n",
    "ax.bar3d(X.ravel(), Y.ravel(), np.zeros_like(Z.ravel()), 1, 1, Z.ravel())\n",
    "ax.set_xlim([-4, 5]); ax.set_xlabel('X')\n",
    "ax.set_ylim([-4, 5]); ax.set_ylabel('Y')\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "def update_plot(idx):\n",
    "    ax2.cla()\n",
    "    ax2.bar(y, Z[:, idx]/np.sum(Z[:, idx]))\n",
    "    ax2.set_title(\"P(Y|X={0})\".format(x[idx]))\n",
    "    ax2.set_ylim([0, 0.55])\n",
    "    ax2.set_xlim([-4, 4])\n",
    "    \n",
    "widgets.interact(update_plot, idx=IntSlider_nice(min=0, max=len(x), value=4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independencia\n",
    "\n",
    "Si dos V.A. son independientes podemos escribir\n",
    "$$\n",
    "\\begin{align}\n",
    "P(x, y)  &= P(x)P(y|x)\\nonumber \\\\\n",
    "&= P(x)P(y) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Saber que ocurrió $x$ no me sirve da nada para saber si ocurrió $y$\n",
    " \n",
    "\n",
    "Dos V.A. son condicionalmente independientes si\n",
    "\n",
    "$$\n",
    "P(x, y|z)  = P(x|z)P(y|z)\n",
    "$$\n",
    "\n",
    "\n",
    "## Regla de la cadena\n",
    "\n",
    "Podemos descomponer una probabilidad conjunta como\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(x_1, x_2, x_3) &= P(x_3|x_2, x_1) P(x_1, x_2) \\nonumber \\\\\n",
    "&= P(x_3|x_2, x_1) P(x_2|x_1) P(x_1) \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Ley de probabilidad total\n",
    "\n",
    "Si el espacio de probabilidad está particionado en $N$ pedazos y se conocen las probabilidades condicionales $P(X=x|Y=y_i)$ podemos calcular la probabilidad del evento $x$ usando\n",
    "\n",
    "$$\n",
    "P(X=x) = \\sum_{i=1}^N P(X=x|Y=y_i) P(Y=y_i)\n",
    "$$\n",
    "\n",
    "## Teorema de Bayes\n",
    "\n",
    "Podemos escribir \n",
    "\n",
    "- la probabilidad de un evento $y$ dado condiciones $x$: $p(y|x)$ \n",
    "- en función de nuestro conocimiento *a priori* sobre $y$: $p(y$)\n",
    "- y de la verosimilitud de que se observen dichas condiciones si $y$ ocurriese: $p(x|y)$\n",
    "\n",
    "como\n",
    "\n",
    "$$\n",
    "P(y | x) = \\frac{P(x|y) P(y)}{P(x)} = \\frac{P(x|y) P(y)}{\\sum_{y\\in\\mathcal{Y}} P(x|y) P(y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptores de las distribuciones\n",
    "\n",
    "Podemos describir una variable aleatoria $X$ con $x \\sim f(x)$ usando\n",
    "\n",
    "### Valor esperado\n",
    "\n",
    "El valor medio de $X$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x\\sim f(x)} [ X ] = \\int_{x\\in \\mathcal{X}} x f(x)  \\,dx\n",
    "$$\n",
    "\n",
    "### Varianza \n",
    "\n",
    "La dispersión de $X$ en torno a su valor medio\n",
    "\n",
    "$$\n",
    "\\text{Var}[X]  = \\mathbb{E}_{x\\sim f(x)} \\left[\\left(X - \\mathbb{E}[X] \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "la relación lineal entre $X$ e $Y$\n",
    "\n",
    "$$\n",
    "\\text{Cov}[X, Y]  = \\mathbb{E}_{x\\sim f_X(x), y\\sim f_Y(y)} \\left[\\left(X - \\mathbb{E}[X] \\right) \\left(Y - \\mathbb{E}[Y] \\right)^T \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "### Momentos estadísticos\n",
    "\n",
    "$$\n",
    "m_k [X] = \\mathbb{E}_{x\\sim f(x)} [ X^k ]\n",
    "$$\n",
    "\n",
    "- Tercer momento: Simetría \n",
    "- Cuarto momento: Cúrtosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize=(7, 3), tight_layout=True)\n",
    "x = np.linspace(-4, 4, num=1000)\n",
    "\n",
    "def update_plot(loc, scale, skew, shape):\n",
    "    distribution1 = scipy.stats.skewnorm(skew, loc=loc, scale=scale)\n",
    "    distribution2 = scipy.stats.gennorm(shape, loc=loc, scale=scale*np.sqrt(2))\n",
    "    [ax_.cla() for ax_ in ax]\n",
    "    ax[0].plot(x, distribution1.pdf(x))\n",
    "    ax[1].plot(x, distribution2.pdf(x))\n",
    "    \n",
    "widgets.interact(update_plot, loc=FloatSlider_nice(min=-3, max=3), \n",
    "                 scale=FloatSlider_nice(min=0.01, max=2., value=1), \n",
    "                 skew=FloatSlider_nice(min=-5, max=5, value=0),\n",
    "                 shape=FloatSlider_nice(min=0.1, max=10, value=2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algunas distribuciones de probabilidad\n",
    "\n",
    "| Distribución | Fenomeno que representa | Ejemplo |\n",
    "| --- | --- | --- |\n",
    "| **Bernoulli** | Evento binario  | Lanzamiento de una moneda |\n",
    "| **Binomial** | Multiples eventos binarios independientes | |\n",
    "| **Categórica** | Evento con $k$ valores posibles | Lanzamiento de un dado, Ruleta |\n",
    "| **Poisson** | Conteo de eventos ocurridos en un período de tiempo | Cantidad de alumnos que llegan entre 9:50 y 10:00| \n",
    "| **Exponencial** | Valor continuo positivo | Tiempo de espera entre eventos|\n",
    "| **Gamma** | Valor continuo positivo | Tiempos de espera hasta que ocurren $n$ eventos|\n",
    "| **Beta** | Valor continuo en $[0, 1]$ |  Tiempo para completar una tarea, proporciones|\n",
    "| **Normal/Gaussiana** | Valor continuo ubicado en la vecindad de un valor central| [Demasiados](https://galtonboard.com/probabilityexamplesinlife)|\n",
    "| **Uniforme** | Valor discreto/continuo acotado a un rango, todos con igual probabilidad de ocurrencia| |\n",
    "\n",
    "<img src=\"https://thumbs.gfycat.com/AggressiveAromaticBuckeyebutterfly-size_restricted.gif\">\n",
    "\n",
    "***\n",
    "\n",
    "Podemos usar `np.random` para generar números aleatorios con distintas propiedades\n",
    "- `randn` : Números reales con distribución normal estándar\n",
    "- `rand`: Números reales con distribución uniforme en $[0, 1]$\n",
    "- `randint(low=1, high=10)`: Números enteros con distribución uniforme entre $[0, 10]$\n",
    "\n",
    "Podemos usar `scipy.stats` para generar datos de una distribución específica\n",
    "- [continua](https://docs.scipy.org/doc/scipy/reference/stats.html#continuous-distributions)\n",
    "- [multivariada](https://docs.scipy.org/doc/scipy/reference/stats.html#multivariate-distributions)\n",
    "- [discreta](https://docs.scipy.org/doc/scipy/reference/stats.html#discrete-distributions)\n",
    "\n",
    "Las distribuciones tienen un constructor específico y métodos\n",
    "- `pdf`/`pmf(x)` Retorna la distribución de probabilidad evaluada en $x$\n",
    "- `cdf(x)` Distribución acumulada evaluada en $x$\n",
    "- `ppf(p)` Inverso de la distribución acumulada\n",
    "- `rvs(size=100)` Retorna $100$ muestras a partir de la distribución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En detalle: Gaussiana/Normal Multivariada\n",
    "\n",
    "- Dominio $x\\in \\mathbb{R}^d$\n",
    "- Parámetros: \n",
    "    - Media $\\mathbb{E}[X] = \\mu \\in \\mathbb{R}^d$ \n",
    "    - Covarianza $\\mathbb{E}[(X-\\mu)(X-\\mu)^T] = \\Sigma \\in \\mathbb{R}^{d\\times d}$ (semidefinida positiva)\n",
    "- Función de densidad de probabilidad\n",
    "$$\n",
    "p(x| \\mu, \\Sigma) = \\frac{1}{ \\sqrt{(2 \\pi)^d} |\\Sigma |} \\exp \\left ( -\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right)\n",
    "$$\n",
    "\n",
    "- Casos especiales\n",
    "    - Covarianza diagonal: $\\Sigma = [\\sigma_1^2, \\sigma_2^2, \\ldots, \\sigma_d^2] I$\n",
    "    - Covarianza isotrópica (esférica): $\\Sigma = \\sigma^2 I$\n",
    "    - Normal estándar: $\\mu = [0, 0, \\ldots, 0]$, $\\Sigma = I$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5, 4))\n",
    "\n",
    "def update_plot(mu1, mu2, s1, s2, rho, seed):\n",
    "    ax.cla()\n",
    "    np.random.seed(seed)\n",
    "    mu = np.array([mu1, mu2]); s = np.diag(np.array([s1, s2]))\n",
    "    rot_mat = [[np.cos(rho), -np.sin(rho)], [np.sin(rho), np.cos(rho)]]\n",
    "    L = np.dot(rot_mat, s)\n",
    "    #x = np.random.multivariate_normal(mean=mu, cov=np.dot(L, L.T), size=5000)\n",
    "    dist = scipy.stats.multivariate_normal(mean=mu, cov=np.dot(L, L.T))\n",
    "    x = np.linspace(-3, 3)\n",
    "    X, Y = np.meshgrid(x, x)\n",
    "    Z = dist.pdf(np.dstack((X, Y)))\n",
    "    ax.contour(X, Y, Z)\n",
    "    xhat = dist.rvs(size=5000)\n",
    "    ax.scatter(xhat[:, 0], xhat[:, 1], s=5, alpha=0.5)\n",
    "    ax.set_xlim([-3, 3]); ax.set_ylim([-3, 3])\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "widgets.interact(update_plot, mu1=FloatSlider_nice(min=-2, max=2), mu2=FloatSlider_nice(min=-2, max=2),\n",
    "                 s1=FloatSlider_nice(value=1, min=0.1, max=2), s2=FloatSlider_nice(value=1, min=0.1, max=2),\n",
    "                 rho=FloatSlider_nice(value=0, min=-np.pi/2, max=np.pi/2), seed=IntSlider_nice());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ley de los grandes números\n",
    "\n",
    "Si $X_1, X_2, \\ldots, X_N$ son V.A independientes e idénticamente distribuidas (iid) con media $\\mu$  entonces su promedio\n",
    "\n",
    "$$\n",
    "\\bar X = \\frac{1}{N} (X_1 + X_2 + \\ldots + X_N)\n",
    "$$\n",
    "\n",
    "tiende a $\\bar X \\to \\mu$ cuando $N \\to \\infty$\n",
    "\n",
    "## Teorema central del límite\n",
    "\n",
    "Si $X_1, X_2, \\ldots, X_N$ son V.A iid, entonces su promedio \n",
    "\n",
    "$$\n",
    "\\bar X \\sim \\mathcal{N}(\\mu, \\sigma^2/N)\n",
    "$$\n",
    "\n",
    "> Sin importar su distribución original, si sumo muchas VA iid entonces la suma se distribuye normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En detalle: Distribución multinomial\n",
    "\n",
    "- Dominio $x \\in \\{0, 1, \\ldots n\\}^k$\n",
    "- Parámetros: \n",
    "    - $n>0$: Número de experimentos \n",
    "    - ${p_1, p_2, \\ldots, p_k}$: Probabilidad de las categorías donde $\\sum_i p_i = 1$\n",
    "- Función de masa de probabilidad\n",
    "$$\n",
    "p(x| m, \\{p\\}) = \\frac{n!}{x_1! x_2! \\cdots x_k!} \\prod_{i=1}^k {p_i}^{x_i}\n",
    "$$\n",
    "\n",
    "- Casos especiales\n",
    "    - $k = 2$: Distribución binomial\n",
    "    - $k = 2$ y $n = 1$: Distribución Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = scipy.stats.multinomial(n=2, p=[1/6]*6)\n",
    "dist.rvs(size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "\n",
    "def update_plot(k):\n",
    "    ax.cla()\n",
    "    ax.set_title(\"Promedio de {0} lanzamiento/s de dado\".format(k+1))\n",
    "    dist = scipy.stats.multinomial(n=k+1, p=[1/6]*6)\n",
    "    repeats = dist.rvs(size=1000)/(k+1)\n",
    "    average_dice = np.sum(repeats*range(1, 7), axis=1)\n",
    "    ax.hist(average_dice, bins=12, density=True)\n",
    "    ax.set_xlim([1, 6])\n",
    "update_plot(0)\n",
    "#anim = animation.FuncAnimation(fig, update_plot, frames=20, interval=300, \n",
    "#                               repeat=True, blit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estadística\n",
    "\n",
    "La estadística busca:\n",
    "\n",
    "> Describir fenómenos complejos a partir de observaciones parciales \n",
    "\n",
    "> Inferir propiedades de una población basándonos en una muestra\n",
    "\n",
    "> Usar datos para responder preguntas y tomar decisiones\n",
    "\n",
    "La estadística es:\n",
    "\n",
    "> Disciplina científica dedicada al desarrollo y estudio de métodos para recopilar, analizar y extraer información de los datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estadística descriptiva\n",
    "\n",
    "Sea una muestra de datos. Para entenderla mejor podemos empezar describiendola:\n",
    "- ¿Discreto o continuo? ¿No-negativo?\n",
    "- ¿Dónde está localizada? **Media**\n",
    "- ¿Cuál es su disperción? **Varianza**\n",
    "- ¿Son las colas iguales o distintas? **Simetría** (*Skewness*)\n",
    "- ¿Son las colas ligeras o pesadas? **Curtosis** (*Kurtosis*)\n",
    "- ¿Tiene una moda o múltiples modas?\n",
    "- ¿Cuantiles? ¿Percentiles? \n",
    "- ¿Existen *outliers*?\n",
    "\n",
    "Podemos responder estas preguntas usando [estadísticos de resumen](https://docs.scipy.org/doc/scipy/reference/stats.html#summary-statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-11, 10, num=1000)\n",
    "px = 0.7*gaussian_pdf(x, mu=-4, s=2) + 0.3*gaussian_pdf(x, mu=3, s=2)\n",
    "N = 1000; \n",
    "np.random.seed(0)\n",
    "hatx = np.concatenate((-4 + 2*np.random.randn(int(0.7*N)), \n",
    "                       (3 + 2*np.random.randn(int(0.3*N)))))\n",
    "\n",
    "scipy.stats.describe(hatx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ciertos casos podemos responder estas preguntas graficamente usando:\n",
    "\n",
    "### Histograma\n",
    "\n",
    "- Es una representación numérica de una distribución\n",
    "- Nos permite visualizar las características de la distribución\n",
    "- Se construye dividiendo el dominio en **bines** y contando los datos que caen en cada **bin**\n",
    "- Está definido por la posición y tamaño de los bines\n",
    "- Método no-paramétrico para representar distribuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "def update_plot(nbins): \n",
    "    ax.cla()\n",
    "    ax.plot(x, px, 'k-', linewidth=4, alpha=0.8)\n",
    "    hist, bin_edges = np.histogram(hatx, bins=nbins, density=True)\n",
    "    ax.bar(bin_edges[:-1], hist, width=bin_edges[1:] - bin_edges[:-1], \n",
    "           edgecolor='k', align='edge', alpha=0.8)\n",
    "    ax.set_xlabel('x')\n",
    "    \n",
    "widgets.interact(update_plot, nbins=SelSlider_nice(options=[2, 5, 10, 15, 20, 50], value=5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel density estimation (KDE)\n",
    "\n",
    "Mismo objetivo que el histograma pero\n",
    "1. Cada dato es \"su propio bin\"\n",
    "1. Los bines se pueden traslapar\n",
    "1. No se escoge la posición o fronteras de bines, solo su ancho\n",
    "\n",
    "Para un set de observaciones unidimensionales $\\{x_i\\}_{i=1,\\ldots, N}$ con dominio continuo\n",
    "\n",
    "$$\n",
    "\\hat f_h(x) = \\frac{1}{Nh} \\sum_{i=1}^N \\kappa \\left ( \\frac{x - x_i}{h} \\right)\n",
    "$$\n",
    "\n",
    "donde \n",
    "- $h$ es el **ancho de banda del kernel** o **tamaño de kernel**\n",
    "- $\\kappa(u)$ es una **función de kernel** que debe ser positiva, con media cero e integrar a la unidad\n",
    "\n",
    "Por ejemplo  el famoso kernel Gaussiano\n",
    "\n",
    "$$\n",
    "\\kappa(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp \\left ( - \\frac{u^2}{2} \\right),\n",
    "$$\n",
    "\n",
    "Ojo: \n",
    "- Usar un kernel gaussiano no es lo mismo que asumir que los datos se distribuyen gaussianos\n",
    "- Se puede usar un kernel gaussiano en datos que no se distribuyen gaussianos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity\n",
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.plot(x, px, 'k-', linewidth=4, alpha=0.8)\n",
    "ax.scatter(hatx, np.zeros_like(hatx), marker='+', c='k', s=20, alpha=0.1)\n",
    "ax.set_xlabel('x')\n",
    "line_kde = ax.plot(x, np.zeros_like(x))\n",
    "#hs = 0.9*np.std(hatx)*N**(-1/5)\n",
    "def update(k): \n",
    "    kde = scipy.stats.gaussian_kde(hatx, bw_method=lambda kde: k*kde.silverman_factor() )\n",
    "    line_kde[0].set_ydata(kde.pdf(x))\n",
    "    \n",
    "widgets.interact(update, k=SelSlider_nice(description=\"$k=h/h_s$\", options=[1/8, 1/4, 1/2, 1, 2, 4], value=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más opciones de kernels en [`sklearn.neighbors.KernelDensity`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia estadística\n",
    "\n",
    "> Extraer conclusiones a partir de hechos a través de una premisa científica\n",
    "\n",
    "- Hechos: Datos\n",
    "- Premisa: Modelo probabilístico\n",
    "- Conclusión: Una cantidad no observada que es interesante\n",
    "\n",
    "> Cuantificar la incerteza de la conclusión dado los datos y el modelo \n",
    "\n",
    "Tareas inferenciales\n",
    "1. Ajustar un modelo: **Máxima verosimilitud**\n",
    "1. Verificar el modelo: **Intervalo de confianza**\n",
    "1. Responder una pregunta usando el modelo: **Test de hipótesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de modelos paramétricos\n",
    "\n",
    "Se refiere a aquellos modelos que explicitan una distribución de probabilidad\n",
    "\n",
    "### Ejemplo: La pesa defectuosa\n",
    "\n",
    "- Mi pesa está defectuosa\n",
    "- Luego de comer mido $M$ veces mi peso obteniendo un conjunto de observaciones $\\{x_i\\}$\n",
    "- El objetivo es encontrar mi peso real $\\hat x$. \n",
    "\n",
    "Puedo modelar mis observaciones como\n",
    "$$\n",
    "x_i = \\hat x + \\varepsilon_i\n",
    "$$\n",
    "donde $\\varepsilon_i$ corresponde al ruido del instrumento\n",
    "\n",
    "Asumamos que $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$\n",
    "\n",
    "es decir que el ruido es\n",
    "- Independiente\n",
    "- Gaussiano\n",
    "- Con media cero y varianza $\\sigma_\\varepsilon^2$\n",
    "\n",
    "Con esto podemos escribir la probabilidad de observar $x_i$ dado un cierto valor $\\hat x$ como\n",
    "\n",
    "$$\n",
    "p(x_i|\\hat x) = \\mathcal{N}(\\hat x, \\sigma_\\varepsilon^2)\n",
    "$$\n",
    "\n",
    "> Mi modelo tiene dos parámetros: $\\hat x$ y $\\sigma_\\varepsilon^2$\n",
    "\n",
    "Si **asumimos** que las observaciones son independientes e identicamente distribuidas (iid), podemos escribir la probabilidad de observar el conjunto completo como \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x_1, x_2, \\ldots, x_M| \\hat x) &= \\prod_{i=1}^M p(x_i|\\hat x) \\nonumber \\\\\n",
    "&= \\prod_{i=1}^M  \\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}  \\exp \\left ( - \\frac{1}{2\\sigma_\\varepsilon^2} (x_i - \\hat x)^2 \\right) = \\mathcal{L}(\\hat x)  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Esto se conoce como la **verosimilitud** de los parámetros dado el conjunto de observaciones\n",
    "\n",
    "Podemos buscar los parámetros que hacen mi modelo más \"creible\" **maximizando la verosimilitud**\n",
    "\n",
    "\n",
    "$$\n",
    "\\max_{\\hat x}  \\mathcal{L}(\\hat x) = \\max_{\\hat x}  \\log \\mathcal{L}(\\hat x) = -\\frac{1}{2} \\log(2\\pi\\sigma_\\varepsilon^2)M - \\frac{1}{2\\sigma_\\varepsilon^2} \\sum_{i=1}^M (x_i - \\hat x )^2\n",
    "$$\n",
    "\n",
    "Como el logaritmo es monotónico conviene maximizar el log de la verosimilitud\n",
    "\n",
    "Si derivamos e igualamos a cero\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\frac{1}{2\\sigma_\\varepsilon^2} \\sum_{i=1}^M 2(x_i - \\hat x ) &= 0 \\nonumber \\\\\n",
    "\\hat x &= \\frac{1}{M} \\sum_{i=1}^M x_i \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> El estimador de máxima verosimilitud para la media de una Gaussiana es el clásico promedio muestral\n",
    "\n",
    "Si hicieramos lo mismo para la varianza encontraríamos\n",
    "\n",
    "$$\n",
    "\\sigma_\\epsilon^2 = \\frac{1}{M} \\sum_{i=1}^M (x_i - \\hat x)^2\n",
    "$$\n",
    "\n",
    "> Que es la varianza muestreal (sesgada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimación de máxima verosimilitud\n",
    "\n",
    "El procedimiento que acabamos de ver se llama *maximum likelihood estimation* (MLE)\n",
    "\n",
    "Procedimiento\n",
    "\n",
    "1. Definir los supuestos del problema, el modelo (distribución) y sus parámetros $\\theta$\n",
    "1. Escribir el **logaritmo de la verosimilud** de los parámetros $\\log \\mathcal{L}(\\theta)$\n",
    "1. Encontrar $\\theta$ que maximiza \n",
    "$$\n",
    "\\hat \\theta = \\text{arg}\\max_\\theta \\log \\mathcal{L}(\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "Las distribuciones de  `scipy.stats` tienen los métodos\n",
    "- `fit (data)` Ajusta una distribución continua a datos usando MLE\n",
    "- `expect (func)` Valor esperado de una función c/r a la distribución\n",
    "- `interval (alpha)` Cotas para el intervalo que contiene un porcentaje $\\alpha$ de la distribución\n",
    "\n",
    "Para distribuciones complicadas sin solución analítica se pueden usar métodos iterativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Ajustando y comparando distintas distribuciones\n",
    "\n",
    "Observemos la siguiente distribución, ¿Qué características resaltan? ¿Qué distribución sería apropiado ajustar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "data_set = datasets.load_breast_cancer()\n",
    "x, y = data_set['data'][:, 0], data_set['target']\n",
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "ax.hist(x, bins=20, density=True)\n",
    "ax.set_xlabel('Tamaño del nodulo');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos probar varias distribuciones y observar el resultado\n",
    "\n",
    "¿Cómo medir la bondad del ajuste?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(np.amin(x), np.amax(x), num=500)\n",
    "dist = scipy.stats.uniform\n",
    "params = dist.fit(x)\n",
    "print(dist.name)\n",
    "print(params)\n",
    "p_plot = dist(*params[:-2], loc=params[-2], scale=params[-1]).pdf(x_plot)\n",
    "ax.plot(x_plot, p_plot, label=dist.name)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bondad de ajuste\n",
    "\n",
    "Podemos usar el test chi-cuadrado, el [test de Akaike](https://en.wikipedia.org/wiki/Akaike_information_criterion), el test no-paramétrico de Kolmogorov-Smirnov (KS) o gráficos QQ para medir que tan bien se ajusta nuestra distribución teórica a los datos\n",
    "\n",
    "El test KS nos permite comparar que tan distinta es una distribución continua empírica de una teórica comparando sus CDFs: [`scipy.stats.kstest`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html)\n",
    "\n",
    "- Require que los datos estén estandarizados \n",
    "\n",
    "$$\n",
    "Z = \\frac{X - \\mu_X}{\\sigma_X}\n",
    "$$\n",
    "\n",
    "- Retorna un estadístico: Mientras más cerca a cero, mejor es el ajuste\n",
    "- También retorna un p-value: Si es menor que $\\alpha$ entonces rechazo la hipótesis nula de que las distribuciones son iguales con un $1-\\alpha$ de confianza. Veremos test de hipótesis más adelante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_std = (x-np.mean(x))/np.std(x)\n",
    "ks_res = []\n",
    "for dist in [scipy.stats.norm, scipy.stats.lognorm, scipy.stats.beta, scipy.stats.gamma, scipy.stats.genextreme]:    \n",
    "    params = dist.fit(x_std)\n",
    "    fitted_dist = dist(*params[:-2], loc=params[-2], scale=params[-1])\n",
    "    ks_res.append(scipy.stats.kstest(rvs=x_std, cdf=fitted_dist.cdf))\n",
    "    print(dist.name)\n",
    "    print(ks_res[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respondiendo preguntas usando nuestro modelo\n",
    "\n",
    "1. ¿Cuál es la media de la distribución? \n",
    "1. ¿Cúal es la probabilidad de que el \"tamaño del nódulo\" sea mayor o igual que 20?\n",
    "1. ¿Cúal es el tamaño de nódulo que acumula el 90% de la distribución?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "ax.hist(x, bins=20, density=True)\n",
    "ax.set_xlabel('Tamaño del nodulo')\n",
    "dist = scipy.stats.genextreme\n",
    "params = dist.fit(x)\n",
    "fitted_dist = dist(*params[:-2], loc=params[-2], scale=params[-1])\n",
    "p_plot = fitted_dist.pdf(x_plot)\n",
    "ax.plot(x_plot, p_plot, c='k', lw=2);\n",
    "ax.fill_between(x_plot[x_plot>=20], np.zeros_like(x_plot[x_plot>=20]), p_plot[x_plot>=20], \n",
    "                alpha=0.5, color='k', zorder=20)\n",
    "\n",
    "# P1: Usando el atributo mean() \n",
    "display(fitted_dist.mean())\n",
    "# P2: Usando el atributo CDF\n",
    "display(1. - fitted_dist.cdf(20))\n",
    "# P3: Usando el atributo PPF\n",
    "display(fitted_dist.ppf(0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-values: https://ipython-books.github.io/72-getting-started-with-statistical-hypothesis-testing-a-simple-z-test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Clasificador Bayesiano Ingenuo\n",
    "\n",
    "Tenemos un dataset de pacientes con cancer de mama \n",
    "\n",
    "Cada paciente está representado por\n",
    "- x: radio de la muestra (continua)\n",
    "- z: textura de la muestra (continua)\n",
    "- y: etiqueta de la muestra (0:maligno o 1:benigno)\n",
    "\n",
    "El dataset tiene 569 pacientes, 212 con tumores malignos(0) y 357 tumores benignos (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data_set['data'][:, :2], data_set['target']\n",
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "ax.scatter(x[y==0, 0], x[y==0, 1], c='k', s=10, marker='o', label='Sanos', alpha=0.5)\n",
    "ax.scatter(x[y==1, 0], x[y==1, 1], c='k', s=10, marker='x', label='Cancer', alpha=0.5)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando el teorema de Bayes podemos escribir la probabilidad de que el tumor sea benigno dada sus características (*posterior*) como\n",
    "\n",
    "$$\n",
    "p(y=1|x, z) = \\frac{p(x, z|y=1) p(y=1)}{p(x, z)}\n",
    "$$\n",
    "\n",
    "y la probabilidad de que sea maligno como\n",
    "$$\n",
    "p(y=0|x, z) = \\frac{p(x, z|y=0) p(y=0)}{p(x, z)}\n",
    "$$\n",
    "\n",
    "- Nos interesa saber si $p(y=0|x, z) > p(y=1|x, z)$\n",
    "- La evidencia $p(x, z)$ es difícil de estimar \n",
    "\n",
    "Entonces\n",
    "\n",
    "$$\n",
    "\\frac{p(y=1|x, z)}{p(y=0|x, z)} = \\frac{p(x, z|y=1) p(y=1)}{p(x, z|y=0) p(y=0)}\n",
    "$$\n",
    "\n",
    "y los *priors*: $p(y=0) = \\frac{212}{569} \\approx 0.41$, $p(y=1) = \\frac{357}{569} \\approx 0.59$\n",
    "\n",
    "Lo único que falta es la verosimilitud, asumiremos que\n",
    "\n",
    "- las características son condicionalmente independientes\n",
    "$$\n",
    "p(x, z|y) = p(x|y) p(z|y)\n",
    "$$\n",
    "- las características son normales\n",
    "$$\n",
    "p(x|y) = \\mathcal{N}(\\mu_x, \\sigma_x^2)\n",
    "$$\n",
    "\n",
    "Estos supuestos son lo que hacen al clasificador \"ingenuo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilidades a priori\n",
    "from collections import Counter\n",
    "py = [Counter(y)[i]/len(y) for i in range(2)]\n",
    "# Ajuste de verosimilitudes\n",
    "dists = {}\n",
    "for y_ in [0, 1]: # Para cada clase\n",
    "    for x_ in [0, 1]: # para cada característica\n",
    "        params = scipy.stats.norm.fit(x[y==y_, x_])\n",
    "        dists[(y_, x_)] = scipy.stats.norm(loc=params[-2], scale=params[-1])\n",
    "\n",
    "def likelihoods(x, z):\n",
    "    pxzy0 = dists[(0, 0)].pdf(x)*dists[(0, 1)].pdf(z)\n",
    "    pxzy1 = dists[(1, 0)].pdf(x)*dists[(1, 1)].pdf(z)\n",
    "    return pxzy0, pxzy1, (pxzy1*py[1])/(pxzy0*py[0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "for k, (label, marker) in enumerate(zip(['Sanos', 'Cancer'], ['o', 'x'])):\n",
    "    ax.scatter(x[y==k, 0], x[y==k, 1], c='k', s=10, \n",
    "               marker=marker, label=label, alpha=0.5)\n",
    "\n",
    "x_plot = np.linspace(np.amin(x[:, 0]), np.amax(x[:, 0]), num=500)\n",
    "z_plot = np.linspace(np.amin(x[:, 1]), np.amax(x[:, 1]), num=500)\n",
    "X, Z = np.meshgrid(x_plot, z_plot)\n",
    "Y = likelihoods(X, Z)\n",
    "ax.contourf(X, Z, Y[1] - Y[0], zorder=-1, cmap=plt.cm.RdBu, \n",
    "            vmin=-2e-2, vmax=2e-2, levels=20)\n",
    "ax.set_xlim([np.amin(x_plot), np.amax(x_plot)])\n",
    "ax.set_ylim([np.amin(z_plot), np.amax(z_plot)])\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Z');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decidimos entre sano y enfermo usando el cociente entre los posterior\n",
    "\n",
    "$$\n",
    "\\frac{p(y=1|x, z)}{p(y=0|x, z)} > R\n",
    "$$\n",
    "\n",
    "Dos tipos de errores:\n",
    "1. Predecir que está enfermo $\\hat y=1$ cuando en realidad estaba sano $y=0$\n",
    "1. Predecir que está sano $\\hat y=0$ cuando en realidad estaba enfermo $y=1$\n",
    "\n",
    "¿Las decisiones tienen igual riesgo ($R$=1) ? \n",
    "\n",
    "Podemos \"calibrar\" la importancia de las decisiones usando un $R$ mayor o menor que $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.contour(X, Z, Y[2] > 1., zorder=-1, levels=[0]);\n",
    "#ax.contour(X, Z, Y[2] > 0.5, zorder=-1, levels=[0]);\n",
    "#ax.contour(X, Z, Y[2] > 0.1, zorder=-1, levels=[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resumen\n",
    "- Cada clase es representada por una distribución Gaussiana\n",
    "- Entrenamos cada Gaussiana por **máxima verosimilitud**\n",
    "- El cociente entre los posterior nos da la clase más probable\n",
    "- **Ventajas:** Fácil de entrenar, en general no se sobreajusta\n",
    "- **Desventajas:** Supuestos fuertes, características independientes y probabilidades normales\n",
    "- Podemos entrenar un clasificador para datos discretos si usamos distribuciones multinomiales\n",
    "- Implementación en [scikit-learn](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [Más sobre clasificador bayesiano](https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Ajustando una recta\n",
    "\n",
    "Sea el siguiente dataset de [consumo de helados](https://forge.scilab.org/index.php/p/rdataset/source/tree/master/csv/Ecdat/Icecream.csv) en USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc -c https://forge.scilab.org/index.php/p/rdataset/source/file/master/csv/Ecdat/Icecream.csv\n",
    "df = pd.read_csv('Icecream.csv', header=0, index_col=0)\n",
    "df.columns = ['consumo', 'ingreso', 'precio', 'temperatura']\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True, sharey=True)\n",
    "for i, col in enumerate(df.columns[1:]):\n",
    "    ax[i].scatter(df[col], df[\"consumo\"])\n",
    "    ax[i].set_xlabel(col)\n",
    "ax[0].set_ylabel(df.columns[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $y$ el consumo y $x$ la temperatura.\n",
    "\n",
    "Asumiendo errores gaussianos\n",
    "\n",
    "$$\n",
    "y_i = \\hat y_i + \\epsilon_i, \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2),\n",
    "$$\n",
    "\n",
    "un modelo lineal de dos parámetros,\n",
    "\n",
    "$$\n",
    "\\hat y_i = \\theta_0 + \\theta_1 x_i\n",
    "$$\n",
    "\n",
    "y observaciones iid, podemos estimar $\\theta$ buscando la máxima verosimilitud del modelo\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^M (y_i - \\theta_0 - \\theta_1 x_i)^2\n",
    "$$\n",
    "\n",
    "donde \n",
    "$$\n",
    "\\sum_i y_i  - M\\theta_0 - \\theta_1  \\sum_i x_i = 0 \\rightarrow \\theta_0 = \\bar y - \\theta_1 \\bar x\n",
    "$$\n",
    "$$\n",
    "\\sum_i y_i x_i - \\theta_0 \\sum_i x_i - \\theta_1 \\sum_i x_i^2 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_1 = \\frac{\\sum_i x_i y_i - M \\bar x \\bar y}{\\sum_i x_i^2 - M \\bar x^2} = \\frac{ \\sum_i (y_i - \\bar y)(x_i - \\bar x)}{\\sum_i (x_i - \\bar x)^2} = \\frac{\\text{COV}(x, y)}{\\text{Var}(x)}\n",
    "$$\n",
    "\n",
    "la fuerza de la correlación se mide con \n",
    "\n",
    "$$\n",
    "r^2 = 1 - \\frac{\\sum_i ( y_i - \\hat y_i)^2}{\\sum_i ( y_i - \\bar y)^2} = 1 - \\frac{\\frac{1}{M} \\sum_i (y_i - \\hat y_i)^2}{\\text{Var}(y)} = \\frac{\\text{COV}^2(x, y)}{\\text{Var}(x) \\text{Var}(y)}\n",
    "$$\n",
    "\n",
    "donde $r \\in [0, 1]$ se conoce como [coeficiente de correlación de Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n",
    "\n",
    "Podemos usar pandas para calcular $r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir que existe \n",
    "- una correlación positiva alta entre consumo y temperatura\n",
    "- una correlación negativa moderada entre consumo y precio\n",
    "- una correlación cercana a cero entre consumo e ingreso\n",
    "\n",
    "Podemos usar la función [`scipy.stats.linregress`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html) para recuperar $\\theta_0$, $\\theta_1$ y $r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True, sharey=True)\n",
    "ax[0].set_ylabel(df.columns[0]);\n",
    "\n",
    "for i, col in enumerate(df.columns[1:]):\n",
    "    ax[i].scatter(df[col], df[\"consumo\"])    \n",
    "    res = scipy.stats.linregress(df[col], df[\"consumo\"])\n",
    "    ax[i].set_title(\"r: {0:0.5f}\".format(res.rvalue))\n",
    "    x_plot = np.linspace(np.amin(df[col]), np. amax(df[col]), num=100)\n",
    "    ax[i].plot(x_plot, res.slope*x_plot + res.intercept, lw=4, alpha=0.5, c='k');\n",
    "    ax[i].set_xlabel(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respondiendo preguntas con nuestro modelo\n",
    "\n",
    "1. ¿Qué tan confiables son los valores de $\\theta_0$, $\\theta_1$ y $r$?\n",
    "1. ¿Son las correlaciones encontradas significativas?\n",
    "\n",
    "\n",
    "Dos caminos para responder\n",
    "- Pruebas paramétricas\n",
    "- Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de hipótesis\n",
    "\n",
    "Algoritmo general para hacer test de hipótesis\n",
    "1. Definimos una hipótesis nula y una hipótesis alternativa\n",
    "1. Definimos un estadístico $t$ que sirva para verificar la hipótesis nula \n",
    "1. Asumimos una distribución para el estadístico bajo la hipótesis nula\n",
    "1. Seleccionamos un nivel de significancia $\\alpha$ \n",
    "1. Calculamos el estadístico para nuestros datos $t_{data}$\n",
    "1. Calculamos el **p-value**: Probabilidad de observar un valor $t>t_{data}$\n",
    "1. Rechazamos la hipótesis nula con confianza $1-\\alpha$ si el p-value es menor que $\\alpha$\n",
    "\n",
    "### t-test para probar que la regresión es significativa\n",
    "\n",
    "En este caso queremos hacer el siguiente test\n",
    "- $H_0:$ La pendiente es nula $\\theta_1= 0$ \n",
    "- $H_1:$ La pendiente no es nula: $\\theta_1\\neq 0$ \n",
    "\n",
    "Asumimos que $\\theta_1$ es normal pero que desconocemos su varianza \n",
    "\n",
    "Podemos formular el siguiente estadístico de prueba\n",
    "\n",
    "$$\n",
    "t = \\frac{(\\theta_1-\\theta^*) }{\\text{SE}_{\\theta_1}/\\sqrt{M-2}} = \\frac{ r\\sqrt{M-2}}{\\sqrt{1-r^2}},\n",
    "$$\n",
    "\n",
    "donde en este caso usaremos $\\theta^*=0$ y $\\text{SE}_{\\theta_1} = \\sqrt{ \\frac{\\frac{1}{M} \\sum_i (y_i - \\hat y_i)^2}{\\text{Var}(x)}}$\n",
    "\n",
    "$t$ se distribuye [t-student](https://en.wikipedia.org/wiki/Student%27s_t-distribution) con dos grados de libertad (modelo de dos parámetros) \n",
    "\n",
    "La función `scipy.stats.linregress` implementa este test y retorna los p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True, sharey=True)\n",
    "t = np.linspace(-7, 7, num=1000)\n",
    "M = df.shape[0]\n",
    "ax[0].set_ylabel(df.columns[0]);\n",
    "\n",
    "for i, col in enumerate(df.columns[1:]):\n",
    "    res = scipy.stats.linregress(df[col], df[\"consumo\"])\n",
    "    t_data = res.rvalue*np.sqrt(M-2)/np.sqrt(1.-res.rvalue**2)\n",
    "    ax[i].set_title(\"t_data: {0:0.5f}\".format(t_data))\n",
    "    ax[i].set_xlabel(col)\n",
    "    dist = scipy.stats.t(loc=0, scale=1, df=M-2)\n",
    "    p = dist.pdf(t)\n",
    "    ax[i].plot(t, p)\n",
    "    ax[i].plot([dist.ppf(0.025)]*2, [0, np.amax(p)], 'k--')\n",
    "    ax[i].plot([dist.ppf(0.975)]*2, [0, np.amax(p)], 'k--')\n",
    "    ax[i].plot([t_data]*2, [0, np.amax(p)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué podemos decir de las correlaciones con el consumo de helados?\n",
    "\n",
    "Para temperatura y considerando $\\alpha = 0.05$:\n",
    "\n",
    "> $\\text{p-value} = 4.7892e-07 < \\alpha$\n",
    "\n",
    "Por ende rechazamos $H_0$ con 95% de confianza\n",
    "\n",
    "En cambio para ingreso y precio no podemos rechazar $H_0$\n",
    "\n",
    "### Reflexión\n",
    "- ¿Cómo se escogen el estadístico y la distribución de prueba? Depende del problema \n",
    "- ¿Qué prueba puedo usar si quiero hacer regresión lineal multivariada? [ANOVA](https://pythonfordatascience.org/anova-python/)\n",
    "- ¿Qué pasa si mis datos tienen una relación que no es lineal? La prueba no es confiable\n",
    "- ¿Qué pasa si $\\theta_1$ no es normal? La prueba no es confiable\n",
    "- ¿Qué pasa si el ruido no es Gaussiano? La prueba no es confiable\n",
    "- ¿Qué pasa si el ruido es Gaussiano pero su varianza cambia en el tiempo? La prueba no es confiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba no-paramétrica: *Bootstrap*\n",
    "\n",
    "Podemos estimar la incerteza de un estimador de forma no-paramétrica usando **muestreo tipo *bootstrap***\n",
    "\n",
    "Tomamos nuestros conjunto de datos de tamaño $M$ y creamos $T$ nuevos conjuntos que \"se le parezcan\" \n",
    "\n",
    "Luego se calcula el valor del estimador en esos nuevos conjuntos\n",
    "\n",
    "*Bootstrap* está basado en la Ley de los grandes números y Teorema central del límite \n",
    "\n",
    "<img src=\"https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2016/10/bootstrap-sample.png\">\n",
    "\n",
    "- En este caso supondremos independencia: **Muestreo con reemplazo de tamaño $M$**\n",
    "- También existe bootstrap basado en los residuos y bootstrap dependiente\n",
    "\n",
    "Podemos usar la función [`numpy.random.choice`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html) para generar los nuevos índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(2, 2, figsize=(6, 5), tight_layout=True)\n",
    "x, y = df[\"temperatura\"].values, df[\"consumo\"].values\n",
    "x_plot = np.linspace(np.amin(x), np. amax(x), num=100)\n",
    "res_all = scipy.stats.linregress(x, y)\n",
    "M = len(x)\n",
    "\n",
    "def update_plot(T):\n",
    "    [ax_.cla() for ax_ in ax.ravel()]\n",
    "    ax[0, 0].scatter(x, y, zorder=100, s=10)\n",
    "    np.random.seed(0)\n",
    "    param = np.zeros(shape=(T, 3))\n",
    "    lines = []\n",
    "    for t in range(T):\n",
    "        bootstrap_idx = np.random.choice(np.arange(len(x)), size=len(x), replace=True)\n",
    "        res = scipy.stats.linregress(x[bootstrap_idx], y[bootstrap_idx])\n",
    "        lines.append(x_plot*res.slope + res.intercept)\n",
    "        #ax[0, 0].plot(x_plot, x_plot*res.slope + res.intercept, alpha=0.05, c='k')\n",
    "        param[t, :] = [res.intercept, res.slope, res.rvalue]\n",
    "    ax[0, 0].plot(x_plot, x_plot*res_all.slope + res_all.intercept, alpha=1, c='r')\n",
    "    lines = np.stack(lines)\n",
    "    ax[0, 0].fill_between(x_plot, np.mean(lines, axis=0)-3*np.std(lines, axis=0), \n",
    "                          np.mean(lines, axis=0)+3*np.std(lines, axis=0), color='r', alpha=0.25)\n",
    "    hist_val, hist_lim, _ = ax[0, 1].hist(param[:, 2], bins=15, density=True)\n",
    "    ax[0, 1].plot([res_all.rvalue]*2, [0, np.max(hist_val)], c='r')\n",
    "    ax[0, 1].set_xlabel('r')\n",
    "    display(\"Intervalo de confianza al 95% de r {}\".format(np.percentile(param[:, 2], [2.5, 97.5])))\n",
    "    hist_val, hist_lim, _ = ax[1, 0].hist(param[:, 0], bins=15, density=True)\n",
    "    ax[1, 0].plot([res_all.intercept]*2, [0, np.max(hist_val)], c='r')\n",
    "    ax[1, 0].set_xlabel(r'$\\theta_0$')\n",
    "    display(\"Intervalo de confianza al 95% de theta_0 {}\".format(np.percentile(param[:, 0], [2.5, 97.5])))\n",
    "    hist_val, hist_lim, _ = ax[1, 1].hist(param[:, 1], bins=15, density=True)\n",
    "    ax[1, 1].plot([res_all.slope]*2, [0, np.max(hist_val)], c='r')\n",
    "    ax[1, 1].set_xlabel(r'$\\theta_1$')\n",
    "    display(\"Intervalo de confianza al 95% de theta_1 {}\".format(np.percentile(param[:, 1], [2.5, 97.5])))\n",
    "\n",
    "widgets.interact(update_plot, T=SelSlider_nice(options=[10, 20, 50, 100, 200, 500], value=50));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\theta_1=0$ no está en el intervalo de confianza por lo que podemos rechazar $H_0$\n",
    "- Notamos que $\\theta_0$ y $\\theta_1$ son asintóticamente normales. El supuesto del t-test estaba bien!\n",
    "- Si hubieramos hecho un t-test sobre $r$ no sería valido (no normalidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más sobre [*bootstrap* y regresión lineal](https://www.stat.cmu.edu/~cshalizi/402/lectures/08-bootstrap/lecture-08.pdf) [aquí](http://homepage.divms.uiowa.edu/~rdecook/stat3200/notes/bootstrap_4pp.pdf) y [acá](https://www.sagepub.com/sites/default/files/upm-binaries/21122_Chapter_21.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm Icecream.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Regresión lineal multivariada y mínimos cuadrados\n",
    "\n",
    "Queremos ajustar un modelo lineal de $N$ parámetros a un conjunto de $M$ observaciones ruidosas\n",
    "\n",
    "Podemos modelar \n",
    "\n",
    "$$\n",
    "y_i = \\Phi(x_i) \\theta + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "donde $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$\n",
    "\n",
    "Si asumimos que los datos son iid podemos escribir el logaritmo de la verosimilitud como\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(y_1, y_2, \\ldots, y_M| \\theta) &= \\log \\prod_{i=1}^M p(y_i|\\theta) \\nonumber \\\\\n",
    "&=  -\\frac{1}{2} \\log(2\\pi\\sigma_\\varepsilon^2) M - \\frac{1}{2\\sigma_\\varepsilon^2} \\sum_{i=1}^M (y_i - \\Phi(x_i) \\theta)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "el primer término no depende de $\\theta$ y el segundo es negativo\n",
    "\n",
    "El resultado de maximizar la log verosimilitud es equivalente a\n",
    "$$\n",
    "\\min_\\theta \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^M (y_i - \\Phi(x_i) \\theta)^2\n",
    "$$\n",
    "\n",
    "Que corresponde al problema de **mínimos cuadrados**\n",
    "\n",
    "#### Reflexionemos\n",
    "¿Qué estamos asumiendo cuando usamos mínimos cuadrados?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum a posteriori (MAP)\n",
    "\n",
    "- Hasta ahora nos hemos enfocado en la verosimilitud $p(\\mathcal{D}|\\theta)$\n",
    "- En realidad lo que más nos interesa es el *posterior* $p(\\theta|\\mathcal{D})$\n",
    "\n",
    "Bayes nos dice que\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta)p(\\theta)}{p(\\mathcal{D})}\n",
    "$$\n",
    "\n",
    "El criterio MAP busca encontrar $\\theta$ que maximiza el posterior. \n",
    "\n",
    "Es decir la moda del posterior\n",
    "\n",
    "Usando el logaritmo para escribir\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg}\\max_\\theta \\log p(\\theta|\\mathcal{D}) \\nonumber \\\\\n",
    "&= \\text{arg}\\max_\\theta \\log p(\\mathcal{D}|\\theta) + \\log p(\\theta) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "donde omitimos la evidencia $p(\\mathcal{D})$ ya que no depende de $\\theta$\n",
    "\n",
    "- El criterio MAP consiste en maximizar la **verosimilitud** más el **prior**\n",
    "- Usamos el **prior** para agregar información sobre $\\theta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Regresión  con MAP\n",
    "\n",
    "Quiero modelar una regresión de $N$ parámetros\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i = & \\Phi(x_i) \\theta + \\varepsilon_i \\nonumber \\\\\n",
    "&\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) \\nonumber \\\\\n",
    "&y_i|\\theta \\sim \\mathcal{N}(\\Phi(x_i) \\theta, \\sigma_\\epsilon^2) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "es decir **verosimilitud** Gaussiana tal como antes\n",
    "\n",
    "Adicionalmente asumiremos que $p(\\theta) = \\mathcal{N}(0, \\sigma_\\theta^2)$, es decir **prior** Gaussiano\n",
    "\n",
    "El estimador MAP es \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg} \\max_\\theta -\\frac{1}{2} \\log(2\\pi\\sigma_\\varepsilon^2) M - \\frac{1}{2\\sigma_\\varepsilon^2} \\sum_{i=1}^M (y_i - \\Phi(x_i) \\theta)^2 -\\frac{1}{2} \\log(2\\pi \\sigma_\\theta^2) N - \\frac{1}{2\\sigma_\\theta^2} \\sum_{j=1}^N \\theta_j^2 \\nonumber \\\\\n",
    "&= \\text{arg} \\min_\\theta  \\sum_{i=1}^M (y_i - \\Phi(x_i) \\theta)^2 + \\frac{\\sigma_\\varepsilon^2}{\\sigma_\\theta^2} \\sum_{j=1}^N \\theta_j^2 \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Reflexión\n",
    "- Si hay mucho ruido ($\\sigma_\\varepsilon^2 \\to \\infty$) entonces ignoro la verosimilitud y le creo al prior\n",
    "- Si uso un prior que no me da información ($\\sigma_\\theta^2 \\to \\infty$) entonces lo ignoro y me enfoco en la verosimilitud\n",
    "\n",
    "#### Relación con lo visto sobre regularización\n",
    "\n",
    "- Si decimos $\\lambda = \\frac{\\sigma_\\varepsilon^2}{\\sigma_\\theta^2}$ entonces la formulación es equivalente a Mínimos cuadrados regularizado, Ridge Regressión, Tikhonov!\n",
    "- Si hubieramos asumido una distribución Laplaciana para $p(\\theta)$ hubieramos obtenido el [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia Bayesiana\n",
    "\n",
    "- Asumimos que $\\theta$ es una variable aleatoria y que tiene cierta distribución\n",
    "- Buscamos estimar el posterior completo $p(\\theta|\\mathcal{D})$\n",
    "- El problema es que $p(\\mathcal{D})$ es generalmente incalculable\n",
    "- Para proseguir tenemos tres opciones\n",
    "    1. Usar distribuciones que sean conjugadas\n",
    "    1. Usar una aproximación (inferencia variacional)\n",
    "    1. Aprender un sistema que se comporte como el posterior (Markov Chain Monte Carlo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "\n",
    "temperatura, consumo = df[\"temperatura\"].values, df[\"consumo\"].values\n",
    "with pm.Model() as helados:\n",
    "    sigma = pm.HalfNormal('sigma', sd=10, shape=1)\n",
    "    theta0 = pm.Normal('intercept', mu=0, sd=10, shape=1)\n",
    "    theta1 = pm.Normal('slope', mu=0, sd=10, shape=1)\n",
    "    mu = temperatura*theta1 + theta0\n",
    "    x_observed = pm.Normal('x_obs', mu=mu, sd=sigma, observed=consumo)\n",
    "    trace = pm.sample(draws=500, tune=200, init='advi', n_init=30000, \n",
    "                      cores=4, chains=2, live_plot=False)\n",
    "display(pm.summary(trace))\n",
    "pm.traceplot(trace, figsize=(9, 6), combined=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más sobre estos temas en el [repositorio de INFO337](https://github.com/magister-informatica-uach/INFO337/blob/master/MCMC/lecture.ipynb) del magíster de informática y en [Ipython](https://ipython-books.github.io/73-getting-started-with-bayesian-methods/) [cookbook](https://ipython-books.github.io/77-fitting-a-bayesian-model-by-sampling-from-a-posterior-distribution-with-a-markov-chain-monte-carlo-method/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para leer y reflexionar: \n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S0167715218300737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
